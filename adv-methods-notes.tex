% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{Statistical Modeling: A Tools Approach}
\author{Carlisle Rainey}
\date{2022-09-08}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Statistical Modeling: A Tools Approach},
  pdfauthor={Carlisle Rainey},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=2cm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\begin{document}
\frontmatter
\maketitle

\mainmatter
\hypertarget{week-1-maximum-likelihood}{%
\chapter{Week 1: Maximum Likelihood}\label{week-1-maximum-likelihood}}

\hypertarget{class-agenda}{%
\section{Class agenda}\label{class-agenda}}

\textbf{Goal of the class} Make you competent users and consumers
(applied and methods papers) of methods beyond least-squares. I'm
deliberately avoiding causal-inference methods (matching, DID, etc)
because we have a class that covers those specifically that we're
offering regularly. I want you to learn a lot about specific tools, but
also develop the skills to go and learn more on your own.

We can deviate into any particular topic you'd find helpful.

\textbf{Structure of the class}

We have three sources of information that we'll learn from:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{My lectures} I have a set of tools that I want to introduce you
  to throughout the semester. I think of the lecture as offering ``an
  overview'' as well as ``my take'' on the tool. I will not supply all
  the details--we don't have enough time and a lecture isn't the ideal
  medium for deep and subtle ideas. In the past, I have supplied all of
  my lecture notes to students. However, the research seems clear that
  student note-taking boosts learning.
\item
  \emph{Required readings} For each topic, I have a few readings
  selected to supply further details or offer a different perspective. I
  want you to carefully read the required readings, even if they seem
  familiar.
\item
  \emph{Suggested and other readings} I encourage you to engage readings
  beyond the required set. These might be ``easier'' readings (e.g.,
  FPP) or more difficult readings (e.g., Greene). In this category, I
  want you to use judgement. If the required readings are easy, then I
  recommend moving on \emph{after} seriously engaging the required
  readings. If the required readings are too difficult, then seek out
  gentler introductions. You should NOT pursue the suggested or other
  readings at the expense of the required readings.
\end{enumerate}

\textbf{Assessments}

This semester, we have a large set of tools that you must demonstrate
that you (1) understand and (2) can implement.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exams: We will have regular exams that require you to implement and
  explain particular tools. I'm open to suggestions on frequency, but I
  suggest a \emph{weekly}, open-book, take-home exam with about a one
  hour time limit. I will grade these as pass/fail. You can re-take a
  (slightly modified) exam up to three times if you fail.
\item
  Journal: I want to you to journal throughout the semester. I want you
  to spend \emph{at least} three hours (hopefully more most weeks)
  outside of class working on your journal. This journal should have
  several parts:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Class Notes
  \item
    Review Exercises
  \item
    Notes from the required readings, including summaries, reactions,
    and (especially) questions or flags for ideas you didn't understand.
    This latter is very important--it will make us all better.
  \item
    Notes from other readings. I want to give you a bit of space to
    explore things on your own. You could do a deeper dive on ideas
    covered carefully in the lectures or readings. Or you could pursue a
    tangential topic (but keep it somewhat related to class). Again,
    summaries, reactions, and questions are appropriate. I suggest
    engaging with reading from substantive course with this class in
    mind, and record your thoughts in your journal.
  \item
    Connections throughout the semester.
  \item
    Explorations of ideas for future projects.
  \end{enumerate}
\end{enumerate}

As I see it, ``regression modeling'' in political science is a
several-step process:

You begin with a substantive understanding of the way the world works.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a regression model. I introduce many.
\item
  Fit a regression model. Maximum likelihood and Markov chain Monte
  Carlo methods are powerful and general.
\item
  Evaluate the fit. What are the properties of the procedure? How well
  does the model match the data?
\item
  Interpret the model. I emphasize quantities of interest and confidence
  intervals, but also discuss hypothesis tests.
\end{enumerate}

You then update your understanding of the world.

This week, I introduce our first ``engine'': maximum likelihood. As a
starting point, we use ML to estimate the parameters of Bernoulli,
Poisson, and beta distributions (without covariates). I introduce the
parametric bootstrap as a tool to obtain confidence intervals. I
introduce the invariance property and show how we can use the invariance
property to transform the estimated parameters into other quantities of
interest. To evaluate the models, we use the predictive distribution.

\hypertarget{maximum-likelihood}{%
\section{Maximum Likelihood}\label{maximum-likelihood}}

Suppose we have a random sample from a distribution \(f(x; \theta)\). We
find the maximum likelihood (ML) estimator \(\hat{\theta}\) of
\(\theta\) by maximizing the likelihood of the observed data with
respect to \(\theta\).

In short, we take the likelihood of the data (given the model and a
particular \(\theta\)) and find the parameter \(\theta\) that maximizes
it.

In practice, to make the math and/or computation a bit easier, we
manipulate the likelihood function in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relabel the likelihood function \(f(x; \theta) = L(\theta)\), since
  it's weird to maximize with respect to a ``conditioning
  variable''fixed'' variable. (The notation \(f(x; \theta)\) suggests
  \(x\) varies for a particular \(\theta\).)
\item
  Work with \(\log L(\theta)\) rather than \(L(\theta)\). Because
  \(\log()\) is a monotonically increasing function, the \(\theta\) that
  maximizes \(L(\theta)\) also maximizes \(\log L(\theta)\).
\end{enumerate}

Suppose we have samples \(x_1, x_2, ..., x_N\) from \(f(x; \theta)\).
Then the joint density/probability is
\(f(x; \theta) = \prod_{n = 1}^N f(x_n; \theta)\) and
\(\log L(\theta) = \sum_{n = 1}^N \log \left[ f(x_n; \theta) \right]\).
The ML estimator \(\hat{\theta}\) of \(\theta\) is
\(\arg \max \log L(\theta)\).

In applied problems, we might be able to simplify \(\log L\)
substantially. Occasionally, we can find a nice analytical maximum. In
many cases, we have a computer find the parameter that maximizes
\(\log L\).

\hypertarget{example-bernoulli-distribution}{%
\subsection{Example: Bernoulli
Distribution}\label{example-bernoulli-distribution}}

As a running example, we use the \textbf{toothpaste cap problem}:

\begin{quote}
We have a toothpaste cap--one with a wide bottom and a narrow top. We're
going to toss the toothpaste cap. It can either end up lying on its
side, its (wide) bottom, or its (narrow) top.
\end{quote}

\begin{quote}
We want to estimate the probability of the toothpaste cap landing on its
top.
\end{quote}

\begin{quote}
We can model each toss as a Bernoulli trial, thinking of each toss as a
random variable \(X\) where \(X \sim \text{Bernoulli}(\pi)\). If the cap
lands on its top, we think of the outcome as 1. If not, as 0.
\end{quote}

Suppose we toss the cap \(N\) times and observe \(k\) tops. What is the
ML estimate \(\hat{\pi}\) of \(\pi\)?

According to the model
\(f(x_i; \pi) = \pi^{x_i} (1 - \pi)^{(1 - x_i)}\). Because the samples
are iid, we can find the \emph{joint} distribution
\(f(x) = f(x_1) \times ... \times f(x_N) = \prod_{i = 1}^N f(x_i)\).
We're just multiplying \(k\) \(\pi\)s (i.e., each of the \(k\) ones has
probability \(\pi\)) and \((N - k)\) \((1 - \pi)\)s (i.e., each of the
\(N - k\) zeros has probability \(1 - \pi\)), so that the
\(f(x; \pi) = \pi^{k} (1 - \pi)^{(N - k)}\). \[
\text{the likelihood:  } f(x; \pi) =  \pi^{k} (1 - \pi)^{(N - k)}, \text{where } k = \sum_{n = 1}^N x_n \\
\] Then, we relabel. \[
\text{the likelihood:  } L(\pi) = \pi^{k} (1 - \pi)^{(N - k)}\\
\] Then, we take the log and simplify. \[
\text{the log-likelihood:  } \log L(\pi) = k \log (\pi) + (N - k) \log(1 - \pi)\\
\] To find the ML estimator, we find \(\hat{\pi}\) that maximizes
\(\log L\).

The code below plots the log-likelihood function using the 8/150 data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pi }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{1000}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{pi =}\NormalTok{ pi) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_lik =} \DecValTok{18}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(pi) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{150} \SpecialCharTok{{-}} \DecValTok{8}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi))}
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pi, }\AttributeTok{y =}\NormalTok{ log\_lik)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{8}\SpecialCharTok{/}\DecValTok{150}\NormalTok{, }\AttributeTok{color =} \StringTok{"green"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-2-1.pdf}

In this case, the analytical optimum is easy.

\[
\begin{aligned}
\frac{d \log L}{d\hat{\pi}} = k \left( \frac{1}{\hat{\pi}}\right) + (N - k) \left( \frac{1}{1 - \hat{\pi}}\right)(-1) &= 0\\
\frac{k}{\hat{\pi}} - \frac{N - y}{1 - \hat{\pi}} &= 0 \\
\frac{k}{\hat{\pi}} &= \frac{N - y}{1 - \hat{\pi}} \\
k(1 - \hat{\pi}) &= (N - y)\hat{\pi} \\
k - y\hat{\pi} &= N\hat{\pi} - y\hat{\pi} \\
k  &= N\hat{\pi} \\
\hat{\pi} &= \frac{k}{N} = \text{avg}(x)\\
\end{aligned}
\] The ML estimator of \(\pi\) is the average of the \(N\) Bernoulli
trials, or, equivalently, the fraction of successes.

The collected data consist of 150 trials and 8 successes, so the ML
estimate of \(\pi\) is \(\frac{8}{150} \approx 0.053\).

\hypertarget{example-poisson-distribution}{%
\subsection{Example: Poisson
Distribution}\label{example-poisson-distribution}}

Suppose we collect \(N\) random samples \(x = \{x_1, x_2, ..., x_N\}\)
and model each draw as a random variable
\(X \sim \text{Poisson}(\lambda)\). Find the ML estimator of
\(\lambda\).

\[
\begin{aligned}
\text{Poisson likelihood: } f(x; \lambda) &= \prod_{n = 1}^N \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \\
L(\lambda) &= \prod_{n = 1}^N \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \\
\log L(\lambda) &= \sum_{n = 1}^N \log \left[ \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \right]\\
&= \sum_{n = 1}^N \left[ x_n \log \lambda + (-\lambda) \log e - \log x_n! \right]\\
&= \log \lambda \left[ \sum_{n = 1}^N x_n \right]  -N\lambda + \sum_{n = 1}^N \log (x_n!) \\
\end{aligned}
\]

To find the ML estimator, we find \(\hat{\lambda}\) that maximizes
\(\log L\). In this case, the analytical optimum is easy.

\[
\begin{aligned}
\frac{d \log L}{d\hat{\lambda}} = \frac{1}{\hat{\lambda}} \left[ \sum_{n = 1}^N x_n \right] - N &= 0\\
\frac{1}{\hat{\lambda}} \left[ \sum_{n = 1}^N x_n \right] &= N \\
\left[ \sum_{n = 1}^N x_n \right] &= N \hat{\lambda} \\
\hat{\lambda} &= \frac{ \sum_{n = 1}^N x_n }{N} = \text{avg}(x)  \\
\end{aligned}
\] The ML estimator for the Poisson distribution is just the average of
the samples.

\hypertarget{remarks}{%
\subsection{Remarks}\label{remarks}}

The ML estimator is extremely common in political science because they
are general, fast, and work extremely well. Lots of models that you've
heard of, such as logistic regression, are estimated with ML.

We can even obtain ML estimates for the linear regression model. We
assume that the observed data are samples from a normal distribution
with mean \(\mu_n = \alpha + \beta x_n\) and variance \(\sigma^2\). For
this model, the least-squares estimate that we learned earlier is also
the ML estimate.

\hypertarget{example-beta-distribution}{%
\subsection{Example: Beta
Distribution}\label{example-beta-distribution}}

Questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the \textit{support} of the beta distribution? \([0, 1]\)
\item
  Is \(y\) a discrete random variable or a continuous random variable?
  Continuous.
\item
  What is the pdf/pmf?
  \(f(y_i; \alpha, \beta) = \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}\),
  where
  \(B(\alpha, \beta) = \displaystyle \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1}dt\).
\end{enumerate}

With the beta distribution, we add two complications that typically
occur when using ML.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  multiple parameters
\item
  an intractable log-likelihood
\end{enumerate}

Start with the probability model \(Y_i \sim f(y_i; \theta)\). In the
case of the beta model, we have
\(Y_i \sim \text{beta}(y_i; \alpha, \beta)\). The \(\alpha\) and
\(\beta\) here don't have a convenient interpretation. They are
``shape'' parameters. You can think of \(\alpha\) as pushing the
distribution to the right and \(\beta\) as pushing the distribution to
the left.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alphas }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{25}\NormalTok{)}
\NormalTok{betas }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{25}\NormalTok{)}

\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}

\NormalTok{pdfs }\OtherTok{\textless{}{-}} \FunctionTok{crossing}\NormalTok{(}\AttributeTok{alpha =}\NormalTok{ alphas, }
                 \AttributeTok{beta =}\NormalTok{ betas, }
                 \AttributeTok{x =}\NormalTok{ x) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pdf =} \FunctionTok{dbeta}\NormalTok{(x, alpha, beta)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{alpha\_lbl =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"alpha == "}\NormalTok{, alpha),}
         \AttributeTok{beta\_lbl =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"beta == "}\NormalTok{, beta)) }

\FunctionTok{ggplot}\NormalTok{(pdfs, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ pdf)) }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(}\AttributeTok{rows =} \FunctionTok{vars}\NormalTok{(beta\_lbl), }\AttributeTok{cols =} \FunctionTok{vars}\NormalTok{(alpha\_lbl), }
             \AttributeTok{labeller =} \StringTok{"label\_parsed"}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-3-1.pdf}

We now have two parameters to estimate and we're going to assume that we
have multiple observations, so that \(y = [y_1, y_2, ,..., y_n]\).

In general, this is how we do ML:

\textbf{Step 1} Write down the likelihood function. Recall that we can
obtain the joint density of \(y_1\) AND \(y_2\) AND \ldots{} AND \(y_n\)
by multiplying the probabilities of each (assuming independence). \[
\begin{aligned}
L(\alpha, \beta) = \displaystyle\prod_{i = 1}^n \overbrace{f(y_i;\alpha, \beta)}^{\text{density}} = \displaystyle\prod_{i = 1}^n \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}
\end{aligned}
\] We see again, as will be usual, that we have this complicated product
that will make our lives difficult.

\textbf{Step 2} Take the log and simplify. \[
\begin{aligned}
L(\alpha, \beta) &= \displaystyle\prod_{i = 1}^n \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}\\
\log L(\alpha, \beta) &= \displaystyle\sum_{i = 1}^n \log \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}\\
&= \displaystyle\sum_{i = 1}^n \left[ \log y_i^{\alpha - 1} + \log (1 - y_i)^{\beta - 1} - \log B(\alpha, \beta)\right]\\
&= \displaystyle\sum_{i = 1}^n \left[ (\alpha - 1)\log y_i + (\beta - 1)\log (1 - y_i) - \log B(\alpha, \beta)\right]\\
&= \displaystyle\sum_{i = 1}^n \left[ (\alpha - 1)\log y_i + (\beta - 1)\log (1 - y_i)\right] - n \log B(\alpha, \beta)\\
\log L(\alpha, \beta) &= (\alpha - 1) \sum_{i = 1}^n \log y_i + (\beta - 1) \sum_{i = 1}^n \log (1 - y_i) - n \log B(\alpha, \beta)
\end{aligned}
\] \textbf{Step 3} Maximize

If we wanted, we could work on this one analytically.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the derivative w.r.t. \(\alpha\).
\item
  Take the derivative w.r.t. \(\beta\).
\item
  Set both equal to zero and solve. (Two equations and two unknowns.)
\end{enumerate}

But the last term
\(B(\alpha, \beta) = \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1}dt\) is
tricky! So let's do it numerically.

To perform the optimization, we need a data set. For now, let's simulate
a fake data set with known parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{shape1 =} \DecValTok{10}\NormalTok{, }\AttributeTok{shape2 =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's plot the log-likelihood function to see what we're dealing with.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotly)}

\NormalTok{alpha }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{beta  }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{crossing}\NormalTok{(alpha, beta) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_lik =}\NormalTok{ alpha}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(y)) }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)) }\SpecialCharTok{{-}} 
           \FunctionTok{length}\NormalTok{(y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\FunctionTok{beta}\NormalTok{(alpha, beta)))}

\FunctionTok{plot\_ly}\NormalTok{(}\AttributeTok{x =} \SpecialCharTok{\textasciitilde{}}\NormalTok{alpha, }\AttributeTok{y =} \SpecialCharTok{\textasciitilde{}}\NormalTok{beta, }\AttributeTok{z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{log\_lik, }\AttributeTok{data =}\NormalTok{ data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_mesh}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"alpha"}\NormalTok{, }\StringTok{"beta"}\NormalTok{, }\StringTok{"log{-}likelihood"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-5-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ alpha, }\AttributeTok{y =}\NormalTok{ beta, }\AttributeTok{z =}\NormalTok{ log\_lik)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_contour}\NormalTok{(}\AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-6-1.pdf}

Now let's program the log-likelihood function in R to handle the
optimization numerically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ll\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, y) \{}
\NormalTok{  alpha }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# optim() requires a single parameter vector}
\NormalTok{  beta }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  ll }\OtherTok{\textless{}{-}}\NormalTok{ alpha}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(y)) }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)) }\SpecialCharTok{{-}} 
           \FunctionTok{length}\NormalTok{(y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\FunctionTok{beta}\NormalTok{(alpha, beta))}
  \FunctionTok{return}\NormalTok{(ll)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now let's use \texttt{optim()} to do the maximization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ ll\_fn, }\AttributeTok{y =}\NormalTok{ y,}
               \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
               \AttributeTok{method =} \StringTok{"Nelder{-}Mead"}\NormalTok{)}

\FunctionTok{print}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{par, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11.0 10.8
\end{verbatim}

We can also wrap the \texttt{optim()} in a function, to make obtaining
the estimates a little bit easier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est\_beta }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y) \{}
\NormalTok{  est }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ ll\_fn, }\AttributeTok{y =}\NormalTok{ y,}
               \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
               \AttributeTok{method =} \StringTok{"Nelder{-}Mead"}\NormalTok{) }\CommentTok{\# for \textgreater{}1d problems}
  \ControlFlowTok{if}\NormalTok{ (est}\SpecialCharTok{$}\NormalTok{convergence }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{) }\FunctionTok{print}\NormalTok{(}\StringTok{"Model did not converge!"}\NormalTok{)}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{est =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{par)}
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}

\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{est\_beta}\NormalTok{(y)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $est
## [1] 11.0 10.8
\end{verbatim}

\hypertarget{the-invariance-property}{%
\section{The Invariance Property}\label{the-invariance-property}}

The parameter \(\pi\) has a nice interpretation--it's a probability or
the expected fraction of 1s in the long-run. However, the model
parameters might not always have nice interpretation. (See the ``shape''
parameters of the beta distribution.) Fortunately, it's easy to
transform the ML estimates of the model parameters into ML estimates of
a quantity of interest.

Suppose we obtain an ML estimate \(\hat{\theta}\) of a parameter
\(\theta\). But we also (or instead) want to estimate a transformation
\(\tau(\theta)\). The we can estimate \(\tau(\theta)\) by applying the
transformation \(\tau\) to the ML estimate \(\hat{\theta}\), so that
\(\widehat{\tau(\theta)} = \hat{\tau} = \tau(\hat{\theta})\).

\hypertarget{example-bernoulli-odds}{%
\subsection{Example: Bernoulli Odds}\label{example-bernoulli-odds}}

Suppose that we want an ML estimator of the \emph{odds} of getting a top
for the toothpaste cap problem. We already used ML to estimate the
\emph{probability} \(\pi\) of getting a top and came up with
\(\frac{8}{150} \approx 0.053\). We can directly transform a probability
into odds using \(\text{odds} = \frac{\pi}{1 - \pi}\). This has a nice
interpretation: odds = 2 means that a top is twice as likely as not;
odds = 0.5 means that a top is half as likely as not.

In our case, we can plug our ML estimate of \(\pi\) into the
transformation to obtain the ML estimate of the odds. \[
\begin{aligned}
\widehat{\text{odds}} &= \frac{\hat{\pi}}{1 - \hat{\pi}} \\
& = \frac{\frac{8}{150}}{1 - \frac{8}{150}} \\
& = \frac{\frac{8}{150}}{\frac{150}{150} - \frac{8}{150}} \\
& = \frac{\frac{8}{150}}{\frac{142}{150}} \\
& = \frac{8}{142} \\
& \approx 0.056
\end{aligned}
\] This means that tops are about 0.06 times as likelihood as not-tops.
Inverted, you're about \(\frac{142}{8} \approx 18\) times more likely to
not get a top than get a top.

\hypertarget{example-poisson-sd}{%
\subsection{Example: Poisson SD}\label{example-poisson-sd}}

In this example, we use real data from Hultman, Kathman, and Shannon
(2013). They are interested in civilian casualties during civil wars.
They write:

\begin{quote}
To gauge the effectiveness of peacekeeping, we explore all intrastate
armed conflicts in sub-Saharan Africa from 1991 to 2008 with monthly
observations. Conflicts are identified using the Uppsala Conflict Data
Program/Peace Research Institute, Oslo (UCDP/PRIO) Armed Conflict
Dataset v.4--2010 (Gleditsch et al.~2002; Harbom and Wallensteen 2009),
which employs a threshold of 25 battle deaths per year. The dataset
covers 36 conflicts, 12 of which have a PKO present at some time.
Consistent with previous research, we add two years of observations to
the end of each conflict episode, as the theoretical processes
associated with victimization may continue after the cessation of
hostilities (Cunningham, Gleditsch, and Salehyan 2009).
\end{quote}

Below are a random sample of 250 observations from their 3,972 monthly
observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{civilian\_casualties }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{61}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{147}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{934}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{42}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{145844}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{7971}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{444}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{109}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{84}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{104}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{104}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{576}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{94}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{ ) }
\end{Highlighting}
\end{Shaded}

We can estimate a single-parameter Poisson model to estimate a mean
\(\lambda\) and a rate \(\frac{1}{\lambda}\). In the case of the Poisson
model, the ML estimate \(\hat{lambda}\) of \(\lambda\) is
\(\text{avg}(y)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(civilian\_casualties)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 630
\end{verbatim}

The mean is a nice, interpretable parameter, but we might want also want
the SD. For the Poisson distribution, the variance equals the mean, so
\(\text{Var}(y) = \text{E}(y) = \lambda\). Therefore, the SD is
\(\sqrt{\lambda}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ML estimate of SD}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{630}\NormalTok{)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25
\end{verbatim}

This is the ML estimate of the SD of the data, and it carries all the
properties of ML estimators. We're using the invariance property to move
from the mean to the SD by a simple transformation.

\hypertarget{example-beta-mean-and-variance}{%
\subsection{Example: Beta Mean and
Variance}\label{example-beta-mean-and-variance}}

Now let's see an example of the beta distribution
\(Y \sim \text{beta}(\alpha, \beta)\). The beta distribution does not
have parameters that are easily interpretable in terms of mean and
variance. Instead, it has two ``shape'' parameters \(\alpha\) and
\(\beta\) that are in tension---one pulls the distribution to the left
and the other pulls the distribution to the right.

For this example, I use opinion data from the 50 states from Barrilleaux
and Rainey (2014). You can find the data here:
\url{https://github.com/carlislerainey/aca-opinion/blob/master/Data/mrp_est.csv}

To make these data suitable for the beta distribution, I rescaled the
observations from a percent to a proportion that ranges from 0 to 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{br }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tribble}\NormalTok{(}
  \SpecialCharTok{\textasciitilde{}}\NormalTok{state\_abbr, }\SpecialCharTok{\textasciitilde{}}\NormalTok{prop\_favorable\_aca,}
         \StringTok{"AL"}\NormalTok{,   }\FloatTok{0.382711108911823}\NormalTok{,}
         \StringTok{"AK"}\NormalTok{,   }\FloatTok{0.374428493677838}\NormalTok{,}
         \StringTok{"AZ"}\NormalTok{,   }\FloatTok{0.396721609154912}\NormalTok{,}
         \StringTok{"AR"}\NormalTok{,   }\FloatTok{0.361623814680961}\NormalTok{,}
         \StringTok{"CA"}\NormalTok{,   }\FloatTok{0.560999240847165}\NormalTok{,}
         \StringTok{"CO"}\NormalTok{,   }\FloatTok{0.450011650633043}\NormalTok{,}
         \StringTok{"CT"}\NormalTok{,   }\FloatTok{0.522239143634457}\NormalTok{,}
         \StringTok{"DE"}\NormalTok{,   }\FloatTok{0.524637037667977}\NormalTok{,}
         \StringTok{"DC"}\NormalTok{,   }\FloatTok{0.853595690161985}\NormalTok{,}
         \StringTok{"FL"}\NormalTok{,    }\FloatTok{0.47022917052716}\NormalTok{,}
         \StringTok{"GA"}\NormalTok{,   }\FloatTok{0.460216990024346}\NormalTok{,}
         \StringTok{"HI"}\NormalTok{,    }\FloatTok{0.61965456264517}\NormalTok{,}
         \StringTok{"ID"}\NormalTok{,   }\FloatTok{0.282992730179373}\NormalTok{,}
         \StringTok{"IL"}\NormalTok{,   }\FloatTok{0.550517975187469}\NormalTok{,}
         \StringTok{"IN"}\NormalTok{,   }\FloatTok{0.421854785281297}\NormalTok{,}
         \StringTok{"IA"}\NormalTok{,   }\FloatTok{0.454007062646206}\NormalTok{,}
         \StringTok{"KS"}\NormalTok{,   }\FloatTok{0.394817640911206}\NormalTok{,}
         \StringTok{"KY"}\NormalTok{,   }\FloatTok{0.336156662764729}\NormalTok{,}
         \StringTok{"LA"}\NormalTok{,   }\FloatTok{0.425588396620569}\NormalTok{,}
         \StringTok{"ME"}\NormalTok{,   }\FloatTok{0.472319257331465}\NormalTok{,}
         \StringTok{"MD"}\NormalTok{,   }\FloatTok{0.583719023711148}\NormalTok{,}
         \StringTok{"MA"}\NormalTok{,   }\FloatTok{0.531871146279692}\NormalTok{,}
         \StringTok{"MI"}\NormalTok{,   }\FloatTok{0.509096426714406}\NormalTok{,}
         \StringTok{"MN"}\NormalTok{,   }\FloatTok{0.497981331879903}\NormalTok{,}
         \StringTok{"MS"}\NormalTok{,   }\FloatTok{0.468038078521612}\NormalTok{,}
         \StringTok{"MO"}\NormalTok{,   }\FloatTok{0.420161837905426}\NormalTok{,}
         \StringTok{"MT"}\NormalTok{,   }\FloatTok{0.351773944902139}\NormalTok{,}
         \StringTok{"NE"}\NormalTok{,   }\FloatTok{0.365225584190989}\NormalTok{,}
         \StringTok{"NV"}\NormalTok{,   }\FloatTok{0.459026605256376}\NormalTok{,}
         \StringTok{"NH"}\NormalTok{,    }\FloatTok{0.43886275738451}\NormalTok{,}
         \StringTok{"NJ"}\NormalTok{,   }\FloatTok{0.531656835425683}\NormalTok{,}
         \StringTok{"NM"}\NormalTok{,   }\FloatTok{0.528461049175538}\NormalTok{,}
         \StringTok{"NY"}\NormalTok{,     }\FloatTok{0.6010574821094}\NormalTok{,}
         \StringTok{"NC"}\NormalTok{,   }\FloatTok{0.452240849305449}\NormalTok{,}
         \StringTok{"ND"}\NormalTok{,   }\FloatTok{0.367690453757597}\NormalTok{,}
         \StringTok{"OH"}\NormalTok{,   }\FloatTok{0.456298880813516}\NormalTok{,}
         \StringTok{"OK"}\NormalTok{,   }\FloatTok{0.309578750918355}\NormalTok{,}
         \StringTok{"OR"}\NormalTok{,   }\FloatTok{0.455832591683007}\NormalTok{,}
         \StringTok{"PA"}\NormalTok{,    }\FloatTok{0.45819440292365}\NormalTok{,}
         \StringTok{"RI"}\NormalTok{,   }\FloatTok{0.536978574569609}\NormalTok{,}
         \StringTok{"SC"}\NormalTok{,   }\FloatTok{0.444870259057071}\NormalTok{,}
         \StringTok{"SD"}\NormalTok{,   }\FloatTok{0.377170366708612}\NormalTok{,}
         \StringTok{"TN"}\NormalTok{,   }\FloatTok{0.368615233253355}\NormalTok{,}
         \StringTok{"TX"}\NormalTok{,   }\FloatTok{0.428407014559672}\NormalTok{,}
         \StringTok{"UT"}\NormalTok{,   }\FloatTok{0.248496577141183}\NormalTok{,}
         \StringTok{"VT"}\NormalTok{,   }\FloatTok{0.553042362822573}\NormalTok{,}
         \StringTok{"VA"}\NormalTok{,   }\FloatTok{0.470739058046787}\NormalTok{,}
         \StringTok{"WA"}\NormalTok{,   }\FloatTok{0.496133477680592}\NormalTok{,}
         \StringTok{"WV"}\NormalTok{,   }\FloatTok{0.295062675817918}\NormalTok{,}
         \StringTok{"WI"}\NormalTok{,   }\FloatTok{0.489912969415965}\NormalTok{,}
         \StringTok{"WY"}\NormalTok{,   }\FloatTok{0.263567780036879}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Now let's find the ML estimates of the two shape parameters of the beta
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain ml estimates}
\NormalTok{log\_lik\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), y) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# pulling these out makes the code a bit easier to follow}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  log\_lik\_i }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(y, }\AttributeTok{shape1 =}\NormalTok{ a, }\AttributeTok{shape2 =}\NormalTok{ b, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  log\_lik }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(log\_lik\_i)}
  \FunctionTok{return}\NormalTok{(log\_lik)}
\NormalTok{\}}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9.56 11.49
\end{verbatim}

The mean is given by \(\frac{\alpha}{\alpha + \beta}\) and the variance
is given by
\(\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\).

We can use the invariance property to obtain ML estimates of the mean
and variance using our ML estimates of \(\alpha\) and \(\beta\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OtherTok{\textless{}{-}}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{]}
\NormalTok{b }\OtherTok{\textless{}{-}}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{]}

\NormalTok{a}\SpecialCharTok{/}\NormalTok{(a }\SpecialCharTok{+}\NormalTok{ b)  }\CommentTok{\# mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4542508
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(a }\SpecialCharTok{*}\NormalTok{ b)}\SpecialCharTok{/}\NormalTok{((a }\SpecialCharTok{+}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))  }\CommentTok{\# var}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01123986
\end{verbatim}

It's worth noting that these correspond closely, \emph{but not exactly}
to the observed mean and variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4524527
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01073633
\end{verbatim}

\hypertarget{the-parametric-bootstrap}{%
\section{The Parametric Bootstrap}\label{the-parametric-bootstrap}}

The parametric bootstrap is a powerful, general tool to obtain
confidence intervals for estimates from parametric models.

Importantly, we are going to lean \emph{pretty heavily} on the
assumption that we have a good model of the distribution of the data.
(The predictive distribution below allows us to assess this.) There's
also a \emph{\textbf{non}parametric} bootstrap, which is much more
popular. We consider that later in the semester.

Suppose we have a sample \(y\) from some known distribution
\(f(y; \theta)\) and use \(y\) to estimate the model parameter(s)
\(\theta\) or some quantity of interest \(\tau(\theta)\). Remember, we
can use ML to estimate either.

To compute a confidence interval, we can use a \emph{parametric}
bootstrap. To do implement the parametric bootstrap, do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximate \(f(y; \theta)\) with \(\hat{f} = f(y; \hat{\theta})\).
  Simulate a new outcome \(y^{\text{bs}}\) from the estimated
  distribution.
\item
  Re-compute the estimate of interest \(\hat{\theta}^{\text{bs}}\) or
  \(\hat{\tau}^{\text{bs}}\) using the bootstrapped outcome variable
  \(y^{\text{bs}}\) rather than the observed outcome \(y\).
\item
  Repeat 1 and 2 many times (say 2,000) to obtain many bootstrapped
  estimates. To obtain the 95\% confidence interval, take the 2.5th and
  97.5th percentiles of the estimates. This is known as the percentile
  method.
\end{enumerate}

\hypertarget{example-toothpaste-cap-problm}{%
\subsection{Example: Toothpaste Cap
Problm}\label{example-toothpaste-cap-problm}}

The code below implements the parametric bootstrap for the toothpaste
cap problem. For 2,000 iterations, it draws 150 observations from
\(Y \sim \text{Bernoulli}(\hat{\pi} = \frac{8}{150})\). For each
iteration, it computes the ML estimate of \(\pi\) for the bootstrapped
data set. Then it computes the percentiles to obtain the confidence
interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_bs)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \DecValTok{8}\SpecialCharTok{/}\DecValTok{150}\NormalTok{)}
\NormalTok{  bs\_est[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(bs\_y)}
\NormalTok{\}}
\FunctionTok{print}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(bs\_est, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# 95\% ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  2.5% 97.5% 
## 0.020 0.093
\end{verbatim}

We leave an evaluation of this confidence interval (i.e., Does it
capture \(\theta\) 95\% of the time?) to later in the semester.

\hypertarget{example-beta-distribution-1}{%
\subsection{Example: Beta
Distribution}\label{example-beta-distribution-1}}

Now let's apply the parametric bootrap to a two-parameter model: the
beta distribution.

First, let's simulate a (fake) data set to use.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set parameters}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{beta }\OtherTok{\textless{}{-}} \DecValTok{2}

\CommentTok{\# simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, alpha, beta)}
\end{Highlighting}
\end{Shaded}

Now let's find the ML estimates of the two shape parameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain ml estimates}
\NormalTok{log\_lik\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), y) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# pulling these out makes the code a bit easier to follow}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  log\_lik\_i }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(y, }\AttributeTok{shape1 =}\NormalTok{ a, }\AttributeTok{shape2 =}\NormalTok{ b, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  log\_lik }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(log\_lik\_i)}
  \FunctionTok{return}\NormalTok{(log\_lik)}
\NormalTok{\}}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ y,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.46 1.91
\end{verbatim}

Now let's use those ML estimates to perform a parametric bootstrap and
find 95\% CIs for the shape parameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain parametric bootstrap 95\% ci for alpha and beta}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ n\_bs, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, }\AttributeTok{shape1 =}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{], }\AttributeTok{shape2 =}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{])}
\NormalTok{  bs\_opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ bs\_y,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{  bs\_est[i, ] }\OtherTok{\textless{}{-}}\NormalTok{ bs\_opt}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}
\NormalTok{ci }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(bs\_est, }\AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\FunctionTok{print}\NormalTok{(ci, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)  }\CommentTok{\# 95\% ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1] [,2]
## 2.5%  4.25 1.52
## 97.5% 7.52 2.58
\end{verbatim}

If instead we cared about the mean of the beta distribution (which is
\(\frac{\alpha}{\alpha + \beta}\)), we can use the parametric bootstrap
to obtain a confidence interval for that quantity as well.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain parametric bootstrap 95\% ci for mean}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_bs)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, }\AttributeTok{shape1 =}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{], }\AttributeTok{shape2 =}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{])}
\NormalTok{  bs\_opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ bs\_y,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{  bs\_alpha }\OtherTok{\textless{}{-}}\NormalTok{ bs\_opt}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{1}\NormalTok{]}
\NormalTok{  bs\_beta }\OtherTok{\textless{}{-}}\NormalTok{ bs\_opt}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  bs\_est[i] }\OtherTok{\textless{}{-}}\NormalTok{ bs\_alpha}\SpecialCharTok{/}\NormalTok{(bs\_alpha }\SpecialCharTok{+}\NormalTok{ bs\_beta)}
\NormalTok{\}}
\FunctionTok{print}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(bs\_est, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# 95\% ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  2.5% 97.5% 
##  0.71  0.77
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# true mean }
\FunctionTok{print}\NormalTok{(alpha}\SpecialCharTok{/}\NormalTok{(alpha }\SpecialCharTok{+}\NormalTok{ beta), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.71
\end{verbatim}

\hypertarget{sampling-distribution}{%
\section{Sampling Distribution}\label{sampling-distribution}}

What's the most important concept in statistical inference? I don't
know, but it could be \textbf{the sampling distribution}. For effect,
let me back off the hedge.

\begin{quote}
The most important concept in statistical inference is the
\textbf{sampling distribution}.
\end{quote}

To define a sampling distribution, you need to imagine repeating a study
over and over. If each study has a random component (perhaps random
sampling or random assignment to treatment and control), then the
estimate will differ from study to study. The distribution of the
estimates across the studies is called the sampling distribution.

\hypertarget{example-the-toothpaste-cap-problem}{%
\subsection{Example: The Toothpaste Cap
Problem}\label{example-the-toothpaste-cap-problem}}

For a given sample of 150 tosses, we recognize the the ML estimate
\(\hat{\pi}\) does not (usually) exactly equal the parameter \(\pi\).
Instead, the particular \(\hat{\pi}\) that the study produces is draw
from a distribution.

Let's illustrate that with a simulation. For these simulations, I
suppose that we toss the toothpaste cap 150 times and the chance of a
head is 5\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_sims }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_sims)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  ml\_est[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{\}}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.027 0.060 0.060 0.053 0.073 0.093 0.040 0.053 0.027 0.033
\end{verbatim}

As you can see, the ML estimates vary to from sample to
sample--different data sets produce different ML estimates. We need a
way to create a confidence interval that consistently captures
\(\theta\).

If we repeat the simulations a large number of times, we can see an
accuracy picture of the sampling distribution via histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_sims)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  ml\_est[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{\}}

\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ml\_est =}\NormalTok{ ml\_est)}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-24-1.pdf}

Many of our methods of evaluating an estimator are statements about the
sampling distribution of that estimator. In general, we'd like the
sampling distribution to be centered over the true parameter of interest
and tightly dispersed.

\hypertarget{bias}{%
\section{Bias}\label{bias}}

Imagine repeatedly sampling and computing the estimate \(\hat{\theta}\)
of the parameter \(\theta\) for each sample. In this thought experiment,
\(\hat{\theta}\) is a random variable. We say that \(\hat{\theta}\) is
\textbf{biased} if \(E(\hat{\theta}) \neq \theta\). We say that
\(\hat{\theta}\) is \textbf{unbiased} if \(E(\hat{\theta}) = \theta\).
We say that the \textbf{bias} of \(\hat{\theta}\) is
\(E(\hat{\theta}) - \theta\).

Importantly, \textbf{ML estimators are not necessarily unbiased}. Of the
models we will see in this course, \emph{most} are biased.

\hypertarget{example-bernoulli-distribution-1}{%
\subsection{Example: Bernoulli
Distribution}\label{example-bernoulli-distribution-1}}

For example, we can compute the bias of our ML estimator of \(\pi\) in
the toothpaste cap problem.

\[
\begin{aligned}
E\left[ \frac{k}{N}\right] &= \frac{1}{N} E(k) = \frac{1}{N} E  \overbrace{ \left( \sum_{n = 1}^N x_n \right) }^{\text{recall } k = \sum_{n = 1}^N x_n } = \frac{1}{N} \sum_{n = 1}^N E(x_n) = \frac{1}{N} \sum_{n = 1}^N \pi = \frac{1}{N}N\pi \\
&= \pi
\end{aligned}
\]

Thus, \(\hat{\pi}^{ML}\) is an unbiased estimator of \(\pi\) in the
toothpaste cap problem.

We can use a Monte Carlo simulation to check this analytical result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{100000}
\NormalTok{pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  pi\_hat[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{\}}

\CommentTok{\# expected value of pi{-}hat}
\FunctionTok{mean}\NormalTok{(pi\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05006227
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimated monte carlo error}
\FunctionTok{sd}\NormalTok{(pi\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.631271e-05
\end{verbatim}

But notice that the property of unbiasedness does not follow the
estimate through transformation. Because the sample is relatively large
in this case (150 tosses), the bias is small, but detectable with
100,000 Monte Carlo simulations

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{odds\_hat }\OtherTok{\textless{}{-}}\NormalTok{ pi\_hat}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi\_hat)}

\CommentTok{\# actual odds}
\FloatTok{0.05}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05263158
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# expected value of odds{-}hat}
\FunctionTok{mean}\NormalTok{(odds\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05307323
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimated monte carlo error}
\FunctionTok{sd}\NormalTok{(odds\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.288517e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the z{-}statistic}
\NormalTok{(}\FunctionTok{mean}\NormalTok{(odds\_hat) }\SpecialCharTok{{-}} \FloatTok{0.05}\SpecialCharTok{/}\FloatTok{0.95}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sd}\NormalTok{(odds\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.023072
\end{verbatim}

\hypertarget{example-poisson-distribution-1}{%
\subsection{Example: Poisson
Distribution}\label{example-poisson-distribution-1}}

Using math almost identical to the toothpaste cap problem, we can show
that the ML estimator \(\hat{\lambda} = \text{avg}(x)\) is an unbiased
estimator of \(\lambda\).

We can also illustrate the unbiasedness with a computer simulation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda }\OtherTok{\textless{}{-}} \FloatTok{4.0}      \CommentTok{\# the parameter we\textquotesingle{}re trying to estimate}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \DecValTok{10}  \CommentTok{\# the sample size we\textquotesingle{}re using in each "study"}

\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}  \CommentTok{\# the number of times we repeat the "study"}
\NormalTok{lambda\_hat }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)  }\CommentTok{\# a container }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(sample\_size, }\AttributeTok{lambda =}\NormalTok{ lambda)}
\NormalTok{  lambda\_hat[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{\}}

\CommentTok{\# expected value of lambda{-}hat}
\FunctionTok{mean}\NormalTok{(lambda\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.99397
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimated monte carlo error}
\FunctionTok{sd}\NormalTok{(lambda\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.006300177
\end{verbatim}

\hypertarget{consistency}{%
\section{Consistency}\label{consistency}}

Imagine taking a sample of size \(N\) and computing the estimate
\(\hat{\theta}_N\) of the parameter \(\theta\). We say that
\(\hat{\theta}\) is a \textbf{consistent} estimator of \(\theta\) if
\(\hat{\theta}\) converges in probability to \(\theta\).

Intuitively, this means the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For a large enough sample, the estimator returns the exact right
  answer.
\item
  For a large enough sample, the estimate \(\hat{\theta}\) does not vary
  any more, but collapses onto a single point and that point is
  \(\theta\).
\end{enumerate}

Under weak, but somewhat technical, assumptions that usually hold, ML
estimators are consistent.

Given that we always have finite samples, why is consistency valuable?
In short, it's not valuable, directly. However, consistent estimators
tend to be decent with small samples.

But it does not follow that consistent estimators work well in small
samples. However, as a rough guideline, consistent estimators work well
for small samples. However, whether they actually work well in any
particular situation needs a more careful investigation.

\hypertarget{example-illustrative}{%
\subsection{Example: Illustrative}\label{example-illustrative}}

To illustrate the concept of consistency, consider this estimator of the
population mean
\(\hat{\mu}^{\text{silly}} = \frac{\sum_{i = 1}^N x_i}{N + 10}\). While
this estimator is biased, it is a consistent estimator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{sample\_sizes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{10000}\NormalTok{, }\DecValTok{100000}\NormalTok{)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{30}  \CommentTok{\# for each sample size}
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()  }\CommentTok{\# grow with each iteration; slow, but easy}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(sample\_sizes)) \{}
\NormalTok{  ml\_est\_i }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(population, sample\_sizes[i], }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    ml\_est\_i[j] }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(x)}\SpecialCharTok{/}\NormalTok{(sample\_sizes[i] }\SpecialCharTok{+} \DecValTok{10}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{  results\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{sample\_size =}\NormalTok{ sample\_sizes[i],}
                             \AttributeTok{ml\_est =}\NormalTok{ ml\_est\_i)}
\NormalTok{\}}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{bind\_rows}\NormalTok{(results\_list) }

\FunctionTok{ggplot}\NormalTok{(results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_size, }\AttributeTok{y =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{mean}\NormalTok{(population)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-28-1.pdf}

\hypertarget{example-bernoulli-odds-1}{%
\subsection{Example: Bernoulli Odds}\label{example-bernoulli-odds-1}}

There are two ways to see consistency for the Bernoulli. First, unless
our sample size is a multiple of 20, it is impossible to obtain an
estimated odds of 0.05/(1 - 0.05). Second, in small samples, the ML
estimate of the odds is biased. As the sample size increases, the bias
shrinks and the estimates collapse toward (and eventually onto) the true
value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_sizes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{750}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{10}  \CommentTok{\# for each sample size}
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()  }\CommentTok{\# grow with each iteration; slow, but easy}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(sample\_sizes)) \{}
\NormalTok{  ml\_est\_i }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(sample\_sizes[i], }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{    pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{    ml\_est\_i[j] }\OtherTok{\textless{}{-}}\NormalTok{ pi\_hat}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi\_hat)}
\NormalTok{  \}}
\NormalTok{  results\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{sample\_size =}\NormalTok{ sample\_sizes[i],}
                             \AttributeTok{ml\_est =}\NormalTok{ ml\_est\_i)}
\NormalTok{\}}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{bind\_rows}\NormalTok{(results\_list) }

\FunctionTok{ggplot}\NormalTok{(results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_size, }\AttributeTok{y =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FloatTok{0.05}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{shape =} \DecValTok{19}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-29-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_size, }\AttributeTok{y =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FloatTok{0.05}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-29-2.pdf}

\hypertarget{predictive-distribution}{%
\section{Predictive Distribution}\label{predictive-distribution}}

In Bayesian statistics, a popular tool for model evaluation is the
posterior predictive distribution. But we might use an analogous
approach for models fit with maximum likelihood.

The predictive distribution is just the distribution given the ML
estimates. Using our notation above, the predictive distribution is
\(f(y; \hat{\theta})\).

When you perform a parametric bootstrap, you are resampling from this
predictive distribution. Here, we're going to use it for a different
purpose: to understand and evaluate our model.

In my view, the predictive distribution is the best way to (1)
understand, (2) evaluate, and then (3) improve models.

You can use the predictive distribution as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit your model with maximum likelihood.
\item
  Simulate a new outcome variable using the estimated model parameters
  (i.e., \(f(y; \hat{theta})\)). Perhaps simulate a handful for
  comparison.
\item
  Compare the simulated outcome variable(s) to the observed outcome
  variables.
\end{enumerate}

\hypertarget{example-poisson-distribution-2}{%
\subsection{Example: Poisson
Distribution}\label{example-poisson-distribution-2}}

Earlier, we fit a Poisson distribution to a sample of data from Hultman,
Kathman, and Shannon (2013).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(civilian\_casualties)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 630
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(civilian\_casualties)}
\NormalTok{y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =}\NormalTok{ ml\_est)}
\FunctionTok{print}\NormalTok{(y\_pred[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 649 597 672 646 589 602 656 652 621 664 590 622 622 678 639 615 662 613 603
## [20] 653 644 598 622 643 606 657 646 646 667 617
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(civilian\_casualties[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  0  0  0  0  0 13  0  0 61  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0
## [26]  0  0  0  0 19
\end{verbatim}

Simply printing a few results, we can immediately see a problem with
data, when compared with the raw data

To see it even more clearly, we can create a histogram of the observed
and simulated data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(civilian\_casualties)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(y\_pred)}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-33-1.pdf}
These data sets are so different that the plots are difficult to read,
so we might put the x-axes on the log scale. Note, though, that the two
plots have very different ranges on the axes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(civilian\_casualties) }\SpecialCharTok{+} \FunctionTok{scale\_x\_log10}\NormalTok{()}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(y\_pred) }\SpecialCharTok{+} \FunctionTok{scale\_x\_log10}\NormalTok{()}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-34-1.pdf}

For a more accurate and complete comparison, let's simulate five fake
data sets and use common axes

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{observed\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(civilian\_casualties, }\AttributeTok{type =} \StringTok{"observed"}\NormalTok{)}

\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =}\NormalTok{ ml\_est)}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{civilian\_casualties =}\NormalTok{ y\_pred, }
                          \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,500
## Columns: 2
## $ civilian_casualties <dbl> 616, 666, 631, 595, 678, 664, 674, 642, 621, 633, ~
## $ type                <chr> "simulated #1", "simulated #1", "simulated #1", "s~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ civilian\_casualties)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-35-1.pdf}
The fit of this model is almost absurd.

\hypertarget{example-beta-distribution-2}{%
\subsection{Example: Beta
Distribution}\label{example-beta-distribution-2}}

Now let's return to our beta model of states' opinions toward the ACA in
the \texttt{br} data frame we loaded earlier.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain ml estimates}
\NormalTok{log\_lik\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), y) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# pulling these out makes the code a bit easier to follow}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  log\_lik\_i }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(y, }\AttributeTok{shape1 =}\NormalTok{ a, }\AttributeTok{shape2 =}\NormalTok{ b, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  log\_lik }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(log\_lik\_i)}
  \FunctionTok{return}\NormalTok{(log\_lik)}
\NormalTok{\}}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par}
\end{Highlighting}
\end{Shaded}

Now let's simulate some fake data from the predictive distribution and
compare that to the observed data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{observed\_data }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"observed"}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(br)}
\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, }\AttributeTok{shape1 =}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{], }\AttributeTok{shape2 =}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{])}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{prop\_favorable\_aca =}\NormalTok{ y\_pred, }
                          \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }

\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_favorable\_aca)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{adv-methods-notes_files/figure-latex/unnamed-chunk-38-1.pdf}

On the whole, we see hear a fairly close correspondence between the
observed and simulated data. That suggests that our model is a good
description of the data.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

From this week, you should be able to\ldots{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Starting with a given distribution (pdf or pmf), find the log-likihood
  function.
\item
  Optimize the log-likelihood function analytically or numerically
  (i.e., using \texttt{optim()}).
\item
  Use the invariance property of ML estimator to transform parameter
  estimates into estimates of quantities of interest.
\item
  Use the parametric bootstrap to compute confidence intervals.
\item
  Describe a ``sampling distribution'' and illustrate it with a computer
  simulation.
\item
  Describe the concepts of ``bias'' and ``consistency.'' Are ML
  estimates unbaised? Consistent? Under what conditions?
\item
  Use the predictive distribution to evaluate models.
\end{enumerate}

\hypertarget{questions-about-the-exponential-distribution}{%
\subsection{Questions About the Exponential
Distribution}\label{questions-about-the-exponential-distribution}}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:ml-exponential}{}{(\#exr:ml-exponential)
}Suppose we collect \(N\) random samples \(x = \{x_1, x_2, ..., x_N\}\)
and model each draw as a random variable
\(X \sim \text{exponential}(\lambda)\) with pdf
\(f(x_n | \lambda) = \lambda e^{-\lambda x_n}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the maximum likelihood estimator of \(\lambda\).
\item
  Perform a Monte Carlo simulation to assess the bias in the ML
  estimator of \(\lambda\) you found above. Use 100,000 Monte Carlo
  simulations. Estimate the Monte Carlo error as
  \(\frac{\text{SD of estimates}}{\sqrt{\text{number of MC simulations}}}\).
  Try a small sample size (e.g., \(N = 5\) and a large (e.g.,
  \(N = 1,000\)). Demonstrate analytically that the estimate is biased.
\item
  Is the ML estimator of \(\lambda\) consistent? Why or why not?
\item
  We interpret the parameter \(\lambda\) as a ``rate.'' Find the ML
  estimate of the mean, which is the reciprocal of the rate. Is this
  estimate unbiased? Consistent? \EndKnitrBlock{exercise}
\end{enumerate}

Solution

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The math follows the Poisson example closely. However, the solution is
  the
  inverse--\(\hat{\lambda} = \frac{N}{\sum_{n = 1}^N x_n } = \frac{1}{\text{avg}(x)}\).
\end{enumerate}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:unnamed-chunk-39}{}{(\#exr:unnamed-chunk-39)
}Suppose a data set
\texttt{x\ \textless{}-\ c(0.306,\ 0.023,\ 0.0471,\ 0.042,\ 0.227)}.
Model this data set using an exponential distribution and estimate the
rate \(\lambda\) using maximum likelihood. Find the estimates in two
ways. First, compute the ML estimates using the analytical solution you
found above. Second, derive the log-likelihood function, plot it, and
use \texttt{optim()} to find the maximum. Show that the two solutions
agree. \EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:unnamed-chunk-40}{}{(\#exr:unnamed-chunk-40)
}Load the \texttt{cancer} data frame from the survival package in R
using \texttt{data(cancer,\ package="survival")}. Model the variable
\texttt{time} using an exponential distribution. Estimate both the rate
and the mean. Use the parametric bootstrap to obtain a 95\% confidence
interval for each. Use the predictive distribution to evaluate the fit
of the model. Do the simulated data sets seems to match the observed
data sets? \EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:unnamed-chunk-41}{}{(\#exr:unnamed-chunk-41)
}Obtain the data for Barrilleaux and Rainey (2014) from GitHub:
\url{https://github.com/carlislerainey/aca-opinion/blob/master/Data/mrp_est.csv}.
Find the file \texttt{mrp\_est.csv}. The variable
\texttt{percent\_supporting\_expansion} gives the the percent of each
state that supports expanding Medicaid under the ACA.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Model the distribution of \emph{proportions} (you'll need to transform
  the variable from a percent to a proportion) as a beta distribution.
  Estimate the two shape parameters using maximum likelihood. Transform
  these estimates into estimates of the mean and SD. Compute a 95\%
  confidence interval for the mean and SD using the parametric bootstap.
\item
  Simulate fake data from the predictive distribution and graphically
  compare these draws to the observed values. How do the observed data
  deviate from the model? Compare the the observed and simulated
  distributions in several ways, such as histograms, ecdf plots,
  two-number summaries, five-number summaries, etc. Look for ways that
  the simulated data sets consistently deviate from the observed.
  \EndKnitrBlock{exercise}
\end{enumerate}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:unnamed-chunk-42}{}{(\#exr:unnamed-chunk-42)
}Suppose a discrete uniform distribution from 0 to \(K\). The pdf is
\(f(x; K) = \frac{1}{K}\) for \(x \in \{0, 1, ..., K\}\). Suppose I have
three samples from the distribution: 676, 459, and 912. Find the ML
estimate of \(K\). \EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:unnamed-chunk-43}{}{(\#exr:unnamed-chunk-43)
}\textbf{DeGroot and Schervish, q. 9, p.~425.} Suppose a distribution
\(f(x; \theta) = \theta x^{\theta - 1}\) for \(0 < x < 1\) and
\(\theta > 0\). Find the ML estimator of \(\theta\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}

\protect\hypertarget{exr:unnamed-chunk-44}{}{(\#exr:unnamed-chunk-44)
}Remember that the estimate \(\hat{\lambda} = \text{avg}(y)\) for the
Poisson distribution is unbiased. Remember that \(E(y) = \lambda\) and
\(\text{SD}(y)\) is \(\sqrt{\lambda}\). By the invariance properity, the
ML estimator of \(SD(y) = \sqrt{\hat{\lambda}}\). Use a Monte Carlo
simulation to assess whether (and how much) this estimator of the SD is
biased. Be sure to experiment with the sample size of y (e.g., N = 5, N
= 200, etc.), but use a large number of Monte Carlo simulations (e.g.,
100,000). \EndKnitrBlock{exercise}

\backmatter
\end{document}
