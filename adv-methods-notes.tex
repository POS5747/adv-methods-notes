% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Statistical Modeling: A Tools Approach},
  pdfauthor={Carlisle Rainey},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Statistical Modeling: A Tools Approach}
\author{Carlisle Rainey}
\date{2022-09-20}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{week-1-maximum-likelihood}{%
\chapter{Week 1: Maximum Likelihood}\label{week-1-maximum-likelihood}}

\hypertarget{class-agenda}{%
\section{Class agenda}\label{class-agenda}}

\textbf{Goal of the class} Make you competent users and consumers (applied and methods papers) of methods beyond least-squares. I'm deliberately avoiding causal-inference methods (matching, DID, etc) because we have a class that covers those specifically that we're offering regularly. I want you to learn a lot about specific tools, but also develop the skills to go and learn more on your own.

We can deviate into any particular topic you'd find helpful.

\textbf{Structure of the class}

We have three sources of information that we'll learn from:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{My lectures} I have a set of tools that I want to introduce you to throughout the semester. I think of the lecture as offering ``an overview'' as well as ``my take'' on the tool. I will not supply all the details--we don't have enough time and a lecture isn't the ideal medium for deep and subtle ideas. In the past, I have supplied all of my lecture notes to students. However, the research seems clear that student note-taking boosts learning.
\item
  \emph{Required readings} For each topic, I have a few readings selected to supply further details or offer a different perspective. I want you to carefully read the required readings, even if they seem familiar.
\item
  \emph{Suggested and other readings} I encourage you to engage readings beyond the required set. These might be ``easier'' readings (e.g., FPP) or more difficult readings (e.g., Greene). In this category, I want you to use judgement. If the required readings are easy, then I recommend moving on \emph{after} seriously engaging the required readings. If the required readings are too difficult, then seek out gentler introductions. You should NOT pursue the suggested or other readings at the expense of the required readings.
\end{enumerate}

\textbf{Assessments}

This semester, we have a large set of tools that you must demonstrate that you (1) understand and (2) can implement.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exams: We will have regular exams that require you to implement and explain particular tools. I'm open to suggestions on frequency, but I suggest a \emph{weekly}, open-book, take-home exam with about a one hour time limit. I will grade these as pass/fail. You can re-take a (slightly modified) exam up to three times if you fail.
\item
  Journal: I want to you to journal throughout the semester. I want you to spend \emph{at least} three hours (hopefully more most weeks) outside of class working on your journal. This journal should have several parts:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Class Notes
  \item
    Review Exercises
  \item
    Notes from the required readings, including summaries, reactions, and (especially) questions or flags for ideas you didn't understand. This latter is very important--it will make us all better.
  \item
    Notes from other readings. I want to give you a bit of space to explore things on your own. You could do a deeper dive on ideas covered carefully in the lectures or readings. Or you could pursue a tangential topic (but keep it somewhat related to class). Again, summaries, reactions, and questions are appropriate. I suggest engaging with reading from substantive course with this class in mind, and record your thoughts in your journal.
  \item
    Connections throughout the semester.
  \item
    Explorations of ideas for future projects.
  \end{enumerate}
\end{enumerate}

As I see it, ``regression modeling'' in political science is a several-step process:

You begin with a substantive understanding of the way the world works.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a regression model. I introduce many.
\item
  Fit a regression model. Maximum likelihood and Markov chain Monte Carlo methods are powerful and general.
\item
  Evaluate the fit. What are the properties of the procedure? How well does the model match the data?
\item
  Interpret the model. I emphasize quantities of interest and confidence intervals, but also discuss hypothesis tests.
\end{enumerate}

You then update your understanding of the world.

This week, I introduce our first ``engine'': maximum likelihood. As a starting point, we use ML to estimate the parameters of Bernoulli, Poisson, and beta distributions (without covariates). I introduce the parametric bootstrap as a tool to obtain confidence intervals. I introduce the invariance property and show how we can use the invariance property to transform the estimated parameters into other quantities of interest. To evaluate the models, we use the predictive distribution.

\hypertarget{maximum-likelihood}{%
\section{Maximum Likelihood}\label{maximum-likelihood}}

Suppose we have a random sample from a distribution \(f(x; \theta)\). We find the maximum likelihood (ML) estimator \(\hat{\theta}\) of \(\theta\) by maximizing the likelihood of the observed data with respect to \(\theta\).

In short, we take the likelihood of the data (given the model and a particular \(\theta\)) and find the parameter \(\theta\) that maximizes it.

In practice, to make the math and/or computation a bit easier, we manipulate the likelihood function in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relabel the likelihood function \(f(x; \theta) = L(\theta)\), since it's weird to maximize with respect to a ``conditioning variable''fixed'' variable. (The notation \(f(x; \theta)\) suggests \(x\) varies for a particular \(\theta\).)
\item
  Work with \(\log L(\theta)\) rather than \(L(\theta)\). Because \(\log()\) is a monotonically increasing function, the \(\theta\) that maximizes \(L(\theta)\) also maximizes \(\log L(\theta)\).
\end{enumerate}

Suppose we have samples \(x_1, x_2, ..., x_N\) from \(f(x; \theta)\). Then the joint density/probability is \(f(x; \theta) = \prod_{n = 1}^N f(x_n; \theta)\) and \(\log L(\theta) = \sum_{n = 1}^N \log \left[ f(x_n; \theta) \right]\). The ML estimator \(\hat{\theta}\) of \(\theta\) is \(\arg \max \log L(\theta)\).

In applied problems, we might be able to simplify \(\log L\) substantially. Occasionally, we can find a nice analytical maximum. In many cases, we have a computer find the parameter that maximizes \(\log L\).

\hypertarget{example-bernoulli-distribution}{%
\subsection{Example: Bernoulli Distribution}\label{example-bernoulli-distribution}}

As a running example, we use the \textbf{toothpaste cap problem}:

\begin{quote}
We have a toothpaste cap--one with a wide bottom and a narrow top. We're going to toss the toothpaste cap. It can either end up lying on its side, its (wide) bottom, or its (narrow) top.
\end{quote}

\begin{quote}
We want to estimate the probability of the toothpaste cap landing on its top.
\end{quote}

\begin{quote}
We can model each toss as a Bernoulli trial, thinking of each toss as a random variable \(X\) where \(X \sim \text{Bernoulli}(\pi)\). If the cap lands on its top, we think of the outcome as 1. If not, as 0.
\end{quote}

Suppose we toss the cap \(N\) times and observe \(k\) tops. What is the ML estimate \(\hat{\pi}\) of \(\pi\)?

According to the model \(f(x_i; \pi) = \pi^{x_i} (1 - \pi)^{(1 - x_i)}\). Because the samples are iid, we can find the \emph{joint} distribution \(f(x) = f(x_1) \times ... \times f(x_N) = \prod_{i = 1}^N f(x_i)\). We're just multiplying \(k\) \(\pi\)s (i.e., each of the \(k\) ones has probability \(\pi\)) and \((N - k)\) \((1 - \pi)\)s (i.e., each of the \(N - k\) zeros has probability \(1 - \pi\)), so that the \(f(x; \pi) = \pi^{k} (1 - \pi)^{(N - k)}\).
\[
\text{the likelihood:  } f(x; \pi) =  \pi^{k} (1 - \pi)^{(N - k)}, \text{where } k = \sum_{n = 1}^N x_n \\
\]
Then, we relabel.
\[
\text{the likelihood:  } L(\pi) = \pi^{k} (1 - \pi)^{(N - k)}\\
\]
Then, we take the log and simplify.
\[
\text{the log-likelihood:  } \log L(\pi) = k \log (\pi) + (N - k) \log(1 - \pi)\\
\]
To find the ML estimator, we find \(\hat{\pi}\) that maximizes \(\log L\).

The code below plots the log-likelihood function using the 8/150 data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pi }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{1000}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{pi =}\NormalTok{ pi) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_lik =} \DecValTok{18}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(pi) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{150} \SpecialCharTok{{-}} \DecValTok{8}\NormalTok{)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi))}
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pi, }\AttributeTok{y =}\NormalTok{ log\_lik)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{8}\SpecialCharTok{/}\DecValTok{150}\NormalTok{, }\AttributeTok{color =} \StringTok{"green"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-01-maximum-likelihood_files/figure-latex/unnamed-chunk-2-1.pdf}

In this case, the analytical optimum is easy.

\[
\begin{aligned}
\frac{d \log L}{d\hat{\pi}} = k \left( \frac{1}{\hat{\pi}}\right) + (N - k) \left( \frac{1}{1 - \hat{\pi}}\right)(-1) &= 0\\
\frac{k}{\hat{\pi}} - \frac{N - y}{1 - \hat{\pi}} &= 0 \\
\frac{k}{\hat{\pi}} &= \frac{N - y}{1 - \hat{\pi}} \\
k(1 - \hat{\pi}) &= (N - y)\hat{\pi} \\
k - y\hat{\pi} &= N\hat{\pi} - y\hat{\pi} \\
k  &= N\hat{\pi} \\
\hat{\pi} &= \frac{k}{N} = \text{avg}(x)\\
\end{aligned}
\]
The ML estimator of \(\pi\) is the average of the \(N\) Bernoulli trials, or, equivalently, the fraction of successes.

The collected data consist of 150 trials and 8 successes, so the ML estimate of \(\pi\) is \(\frac{8}{150} \approx 0.053\).

\hypertarget{example-poisson-distribution}{%
\subsection{Example: Poisson Distribution}\label{example-poisson-distribution}}

Suppose we collect \(N\) random samples \(x = \{x_1, x_2, ..., x_N\}\) and model each draw as a random variable \(X \sim \text{Poisson}(\lambda)\). Find the ML estimator of \(\lambda\).

\[
\begin{aligned}
\text{Poisson likelihood: } f(x; \lambda) &= \prod_{n = 1}^N \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \\
L(\lambda) &= \prod_{n = 1}^N \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \\
\log L(\lambda) &= \sum_{n = 1}^N \log \left[ \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \right]\\
&= \sum_{n = 1}^N \left[ x_n \log \lambda + (-\lambda) \log e - \log x_n! \right]\\
&= \log \lambda \left[ \sum_{n = 1}^N x_n \right]  -N\lambda + \sum_{n = 1}^N \log (x_n!) \\
\end{aligned}
\]

To find the ML estimator, we find \(\hat{\lambda}\) that maximizes \(\log L\). In this case, the analytical optimum is easy.

\[
\begin{aligned}
\frac{d \log L}{d\hat{\lambda}} = \frac{1}{\hat{\lambda}} \left[ \sum_{n = 1}^N x_n \right] - N &= 0\\
\frac{1}{\hat{\lambda}} \left[ \sum_{n = 1}^N x_n \right] &= N \\
\left[ \sum_{n = 1}^N x_n \right] &= N \hat{\lambda} \\
\hat{\lambda} &= \frac{ \sum_{n = 1}^N x_n }{N} = \text{avg}(x)  \\
\end{aligned}
\]
The ML estimator for the Poisson distribution is just the average of the samples.

\hypertarget{remarks}{%
\subsection{Remarks}\label{remarks}}

The ML estimator is extremely common in political science because they are general, fast, and work extremely well. Lots of models that you've heard of, such as logistic regression, are estimated with ML.

We can even obtain ML estimates for the linear regression model. We assume that the observed data are samples from a normal distribution with mean \(\mu_n = \alpha + \beta x_n\) and variance \(\sigma^2\). For this model, the least-squares estimate that we learned earlier is also the ML estimate.

\hypertarget{example-beta-distribution}{%
\subsection{Example: Beta Distribution}\label{example-beta-distribution}}

Questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the \textit{support} of the beta distribution? \([0, 1]\)
\item
  Is \(y\) a discrete random variable or a continuous random variable? Continuous.
\item
  What is the pdf/pmf? \(f(y_i; \alpha, \beta) = \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}\), where \(B(\alpha, \beta) = \displaystyle \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1}dt\).
\end{enumerate}

With the beta distribution, we add two complications that typically occur when using ML.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  multiple parameters
\item
  an intractable log-likelihood
\end{enumerate}

Start with the probability model \(Y_i \sim f(y_i; \theta)\). In the case of the beta model, we have \(Y_i \sim \text{beta}(y_i; \alpha, \beta)\). The \(\alpha\) and \(\beta\) here don't have a convenient interpretation. They are ``shape'' parameters. You can think of \(\alpha\) as pushing the distribution to the right and \(\beta\) as pushing the distribution to the left.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alphas }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{25}\NormalTok{)}
\NormalTok{betas }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{25}\NormalTok{)}

\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}

\NormalTok{pdfs }\OtherTok{\textless{}{-}} \FunctionTok{crossing}\NormalTok{(}\AttributeTok{alpha =}\NormalTok{ alphas, }
                 \AttributeTok{beta =}\NormalTok{ betas, }
                 \AttributeTok{x =}\NormalTok{ x) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pdf =} \FunctionTok{dbeta}\NormalTok{(x, alpha, beta)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{alpha\_lbl =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"alpha == "}\NormalTok{, alpha),}
         \AttributeTok{beta\_lbl =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"beta == "}\NormalTok{, beta)) }

\FunctionTok{ggplot}\NormalTok{(pdfs, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ pdf)) }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(}\AttributeTok{rows =} \FunctionTok{vars}\NormalTok{(beta\_lbl), }\AttributeTok{cols =} \FunctionTok{vars}\NormalTok{(alpha\_lbl), }
             \AttributeTok{labeller =} \StringTok{"label\_parsed"}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-01-maximum-likelihood_files/figure-latex/unnamed-chunk-3-1.pdf}

We now have two parameters to estimate and we're going to assume that we have multiple observations, so that \(y = [y_1, y_2, ,..., y_n]\).

In general, this is how we do ML:

\textbf{Step 1} Write down the likelihood function. Recall that we can obtain the joint density of \(y_1\) AND \(y_2\) AND \ldots{} AND \(y_n\) by multiplying the probabilities of each (assuming independence).
\[
\begin{aligned}
L(\alpha, \beta) = \displaystyle\prod_{i = 1}^n \overbrace{f(y_i;\alpha, \beta)}^{\text{density}} = \displaystyle\prod_{i = 1}^n \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}
\end{aligned}
\]
We see again, as will be usual, that we have this complicated product that will make our lives difficult.

\textbf{Step 2} Take the log and simplify.
\[
\begin{aligned}
L(\alpha, \beta) &= \displaystyle\prod_{i = 1}^n \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}\\
\log L(\alpha, \beta) &= \displaystyle\sum_{i = 1}^n \log \dfrac{y_i^{\alpha - 1}(1 - y_i)^{\beta - 1}}{B(\alpha, \beta)}\\
&= \displaystyle\sum_{i = 1}^n \left[ \log y_i^{\alpha - 1} + \log (1 - y_i)^{\beta - 1} - \log B(\alpha, \beta)\right]\\
&= \displaystyle\sum_{i = 1}^n \left[ (\alpha - 1)\log y_i + (\beta - 1)\log (1 - y_i) - \log B(\alpha, \beta)\right]\\
&= \displaystyle\sum_{i = 1}^n \left[ (\alpha - 1)\log y_i + (\beta - 1)\log (1 - y_i)\right] - n \log B(\alpha, \beta)\\
\log L(\alpha, \beta) &= (\alpha - 1) \sum_{i = 1}^n \log y_i + (\beta - 1) \sum_{i = 1}^n \log (1 - y_i) - n \log B(\alpha, \beta)
\end{aligned}
\]
\textbf{Step 3} Maximize

If we wanted, we could work on this one analytically.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the derivative w.r.t. \(\alpha\).
\item
  Take the derivative w.r.t. \(\beta\).
\item
  Set both equal to zero and solve. (Two equations and two unknowns.)
\end{enumerate}

But the last term \(B(\alpha, \beta) = \int_0^1 t^{\alpha - 1}(1 - t)^{\beta - 1}dt\) is tricky! So let's do it numerically.

To perform the optimization, we need a data set. For now, let's simulate a fake data set with known parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{shape1 =} \DecValTok{10}\NormalTok{, }\AttributeTok{shape2 =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's plot the log-likelihood function to see what we're dealing with.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(plotly)}

\NormalTok{alpha }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{beta  }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{crossing}\NormalTok{(alpha, beta) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_lik =}\NormalTok{ alpha}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(y)) }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)) }\SpecialCharTok{{-}} 
           \FunctionTok{length}\NormalTok{(y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\FunctionTok{beta}\NormalTok{(alpha, beta)))}

\FunctionTok{plot\_ly}\NormalTok{(}\AttributeTok{x =} \SpecialCharTok{\textasciitilde{}}\NormalTok{alpha, }\AttributeTok{y =} \SpecialCharTok{\textasciitilde{}}\NormalTok{beta, }\AttributeTok{z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{log\_lik, }\AttributeTok{data =}\NormalTok{ data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_mesh}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"alpha"}\NormalTok{, }\StringTok{"beta"}\NormalTok{, }\StringTok{"log{-}likelihood"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-01-maximum-likelihood_files/figure-latex/unnamed-chunk-5-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ alpha, }\AttributeTok{y =}\NormalTok{ beta, }\AttributeTok{z =}\NormalTok{ log\_lik)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_contour}\NormalTok{(}\AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-01-maximum-likelihood_files/figure-latex/unnamed-chunk-6-1.pdf}

Now let's program the log-likelihood function in R to handle the optimization numerically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ll\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta, y) \{}
\NormalTok{  alpha }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# optim() requires a single parameter vector}
\NormalTok{  beta }\OtherTok{\textless{}{-}}\NormalTok{ theta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  ll }\OtherTok{\textless{}{-}}\NormalTok{ alpha}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(y)) }\SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{*}\FunctionTok{sum}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)) }\SpecialCharTok{{-}} 
           \FunctionTok{length}\NormalTok{(y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\FunctionTok{beta}\NormalTok{(alpha, beta))}
  \FunctionTok{return}\NormalTok{(ll)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now let's use \texttt{optim()} to do the maximization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ ll\_fn, }\AttributeTok{y =}\NormalTok{ y,}
               \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
               \AttributeTok{method =} \StringTok{"Nelder{-}Mead"}\NormalTok{)}

\FunctionTok{print}\NormalTok{(est}\SpecialCharTok{$}\NormalTok{par, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.94 9.79
\end{verbatim}

We can also wrap the \texttt{optim()} in a function, to make obtaining the estimates a little bit easier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{est\_beta }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(y) \{}
\NormalTok{  est }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ ll\_fn, }\AttributeTok{y =}\NormalTok{ y,}
               \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{),}
               \AttributeTok{method =} \StringTok{"Nelder{-}Mead"}\NormalTok{) }\CommentTok{\# for \textgreater{}1d problems}
  \ControlFlowTok{if}\NormalTok{ (est}\SpecialCharTok{$}\NormalTok{convergence }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{) }\FunctionTok{print}\NormalTok{(}\StringTok{"Model did not converge!"}\NormalTok{)}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{est =}\NormalTok{ est}\SpecialCharTok{$}\NormalTok{par)}
  \FunctionTok{return}\NormalTok{(res)}
\NormalTok{\}}

\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{est\_beta}\NormalTok{(y)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $est
## [1] 9.94 9.79
\end{verbatim}

\hypertarget{the-invariance-property}{%
\section{The Invariance Property}\label{the-invariance-property}}

The parameter \(\pi\) has a nice interpretation--it's a probability or the expected fraction of 1s in the long-run. However, the model parameters might not always have nice interpretation. (See the ``shape'' parameters of the beta distribution.) Fortunately, it's easy to transform the ML estimates of the model parameters into ML estimates of a quantity of interest.

Suppose we obtain an ML estimate \(\hat{\theta}\) of a parameter \(\theta\). But we also (or instead) want to estimate a transformation \(\tau(\theta)\). The we can estimate \(\tau(\theta)\) by applying the transformation \(\tau\) to the ML estimate \(\hat{\theta}\), so that \(\widehat{\tau(\theta)} = \hat{\tau} = \tau(\hat{\theta})\).

\hypertarget{example-bernoulli-odds}{%
\subsection{Example: Bernoulli Odds}\label{example-bernoulli-odds}}

Suppose that we want an ML estimator of the \emph{odds} of getting a top for the toothpaste cap problem. We already used ML to estimate the \emph{probability} \(\pi\) of getting a top and came up with \(\frac{8}{150} \approx 0.053\). We can directly transform a probability into odds using \(\text{odds} = \frac{\pi}{1 - \pi}\). This has a nice interpretation: odds = 2 means that a top is twice as likely as not; odds = 0.5 means that a top is half as likely as not.

In our case, we can plug our ML estimate of \(\pi\) into the transformation to obtain the ML estimate of the odds.
\[
\begin{aligned}
\widehat{\text{odds}} &= \frac{\hat{\pi}}{1 - \hat{\pi}} \\
& = \frac{\frac{8}{150}}{1 - \frac{8}{150}} \\
& = \frac{\frac{8}{150}}{\frac{150}{150} - \frac{8}{150}} \\
& = \frac{\frac{8}{150}}{\frac{142}{150}} \\
& = \frac{8}{142} \\
& \approx 0.056
\end{aligned}
\]
This means that tops are about 0.06 times as likelihood as not-tops. Inverted, you're about \(\frac{142}{8} \approx 18\) times more likely to not get a top than get a top.

\hypertarget{example-poisson-sd}{%
\subsection{Example: Poisson SD}\label{example-poisson-sd}}

In this example, we use real data from Hultman, Kathman, and Shannon (2013). They are interested in civilian casualties during civil wars. They write:

\begin{quote}
To gauge the effectiveness of peacekeeping, we explore all intrastate armed conflicts in sub-Saharan Africa from 1991 to 2008 with monthly observations. Conflicts are identified using the Uppsala Conflict Data Program/Peace Research Institute, Oslo (UCDP/PRIO) Armed Conflict Dataset v.4--2010 (Gleditsch et al.~2002; Harbom and Wallensteen 2009), which employs a threshold of 25 battle deaths per year. The dataset covers 36 conflicts, 12 of which have a PKO present at some time. Consistent with previous research, we add two years of observations to the end of each conflict episode, as the theoretical processes associated with victimization may continue after the cessation of hostilities (Cunningham, Gleditsch, and Salehyan 2009).
\end{quote}

Below are a random sample of 250 observations from their 3,972 monthly observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{civilian\_casualties }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{61}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{147}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{934}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{42}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{124}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{145844}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{44}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{7971}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{72}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{444}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{48}\NormalTok{, }\DecValTok{109}\NormalTok{, }\DecValTok{33}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{84}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{104}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{24}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{104}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{41}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{37}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{27}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{576}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}
                          \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{94}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{ ) }
\end{Highlighting}
\end{Shaded}

We can estimate a single-parameter Poisson model to estimate a mean \(\lambda\) and a rate \(\frac{1}{\lambda}\). In the case of the Poisson model, the ML estimate \(\hat{lambda}\) of \(\lambda\) is \(\text{avg}(y)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(civilian\_casualties)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 630
\end{verbatim}

The mean is a nice, interpretable parameter, but we might want also want the SD. For the Poisson distribution, the variance equals the mean, so \(\text{Var}(y) = \text{E}(y) = \lambda\). Therefore, the SD is \(\sqrt{\lambda}\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ML estimate of SD}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{630}\NormalTok{)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25
\end{verbatim}

This is the ML estimate of the SD of the data, and it carries all the properties of ML estimators. We're using the invariance property to move from the mean to the SD by a simple transformation.

\hypertarget{example-beta-mean-and-variance}{%
\subsection{Example: Beta Mean and Variance}\label{example-beta-mean-and-variance}}

Now let's see an example of the beta distribution \(Y \sim \text{beta}(\alpha, \beta)\). The beta distribution does not have parameters that are easily interpretable in terms of mean and variance. Instead, it has two ``shape'' parameters \(\alpha\) and \(\beta\) that are in tension---one pulls the distribution to the left and the other pulls the distribution to the right.

For this example, I use opinion data from the 50 states from Barrilleaux and Rainey (2014). You can find the data here: \url{https://github.com/carlislerainey/aca-opinion/blob/master/Data/mrp_est.csv}

To make these data suitable for the beta distribution, I rescaled the observations from a percent to a proportion that ranges from 0 to 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{br }\OtherTok{\textless{}{-}}\NormalTok{ tibble}\SpecialCharTok{::}\FunctionTok{tribble}\NormalTok{(}
  \SpecialCharTok{\textasciitilde{}}\NormalTok{state\_abbr, }\SpecialCharTok{\textasciitilde{}}\NormalTok{prop\_favorable\_aca,}
         \StringTok{"AL"}\NormalTok{,   }\FloatTok{0.382711108911823}\NormalTok{,}
         \StringTok{"AK"}\NormalTok{,   }\FloatTok{0.374428493677838}\NormalTok{,}
         \StringTok{"AZ"}\NormalTok{,   }\FloatTok{0.396721609154912}\NormalTok{,}
         \StringTok{"AR"}\NormalTok{,   }\FloatTok{0.361623814680961}\NormalTok{,}
         \StringTok{"CA"}\NormalTok{,   }\FloatTok{0.560999240847165}\NormalTok{,}
         \StringTok{"CO"}\NormalTok{,   }\FloatTok{0.450011650633043}\NormalTok{,}
         \StringTok{"CT"}\NormalTok{,   }\FloatTok{0.522239143634457}\NormalTok{,}
         \StringTok{"DE"}\NormalTok{,   }\FloatTok{0.524637037667977}\NormalTok{,}
         \StringTok{"DC"}\NormalTok{,   }\FloatTok{0.853595690161985}\NormalTok{,}
         \StringTok{"FL"}\NormalTok{,    }\FloatTok{0.47022917052716}\NormalTok{,}
         \StringTok{"GA"}\NormalTok{,   }\FloatTok{0.460216990024346}\NormalTok{,}
         \StringTok{"HI"}\NormalTok{,    }\FloatTok{0.61965456264517}\NormalTok{,}
         \StringTok{"ID"}\NormalTok{,   }\FloatTok{0.282992730179373}\NormalTok{,}
         \StringTok{"IL"}\NormalTok{,   }\FloatTok{0.550517975187469}\NormalTok{,}
         \StringTok{"IN"}\NormalTok{,   }\FloatTok{0.421854785281297}\NormalTok{,}
         \StringTok{"IA"}\NormalTok{,   }\FloatTok{0.454007062646206}\NormalTok{,}
         \StringTok{"KS"}\NormalTok{,   }\FloatTok{0.394817640911206}\NormalTok{,}
         \StringTok{"KY"}\NormalTok{,   }\FloatTok{0.336156662764729}\NormalTok{,}
         \StringTok{"LA"}\NormalTok{,   }\FloatTok{0.425588396620569}\NormalTok{,}
         \StringTok{"ME"}\NormalTok{,   }\FloatTok{0.472319257331465}\NormalTok{,}
         \StringTok{"MD"}\NormalTok{,   }\FloatTok{0.583719023711148}\NormalTok{,}
         \StringTok{"MA"}\NormalTok{,   }\FloatTok{0.531871146279692}\NormalTok{,}
         \StringTok{"MI"}\NormalTok{,   }\FloatTok{0.509096426714406}\NormalTok{,}
         \StringTok{"MN"}\NormalTok{,   }\FloatTok{0.497981331879903}\NormalTok{,}
         \StringTok{"MS"}\NormalTok{,   }\FloatTok{0.468038078521612}\NormalTok{,}
         \StringTok{"MO"}\NormalTok{,   }\FloatTok{0.420161837905426}\NormalTok{,}
         \StringTok{"MT"}\NormalTok{,   }\FloatTok{0.351773944902139}\NormalTok{,}
         \StringTok{"NE"}\NormalTok{,   }\FloatTok{0.365225584190989}\NormalTok{,}
         \StringTok{"NV"}\NormalTok{,   }\FloatTok{0.459026605256376}\NormalTok{,}
         \StringTok{"NH"}\NormalTok{,    }\FloatTok{0.43886275738451}\NormalTok{,}
         \StringTok{"NJ"}\NormalTok{,   }\FloatTok{0.531656835425683}\NormalTok{,}
         \StringTok{"NM"}\NormalTok{,   }\FloatTok{0.528461049175538}\NormalTok{,}
         \StringTok{"NY"}\NormalTok{,     }\FloatTok{0.6010574821094}\NormalTok{,}
         \StringTok{"NC"}\NormalTok{,   }\FloatTok{0.452240849305449}\NormalTok{,}
         \StringTok{"ND"}\NormalTok{,   }\FloatTok{0.367690453757597}\NormalTok{,}
         \StringTok{"OH"}\NormalTok{,   }\FloatTok{0.456298880813516}\NormalTok{,}
         \StringTok{"OK"}\NormalTok{,   }\FloatTok{0.309578750918355}\NormalTok{,}
         \StringTok{"OR"}\NormalTok{,   }\FloatTok{0.455832591683007}\NormalTok{,}
         \StringTok{"PA"}\NormalTok{,    }\FloatTok{0.45819440292365}\NormalTok{,}
         \StringTok{"RI"}\NormalTok{,   }\FloatTok{0.536978574569609}\NormalTok{,}
         \StringTok{"SC"}\NormalTok{,   }\FloatTok{0.444870259057071}\NormalTok{,}
         \StringTok{"SD"}\NormalTok{,   }\FloatTok{0.377170366708612}\NormalTok{,}
         \StringTok{"TN"}\NormalTok{,   }\FloatTok{0.368615233253355}\NormalTok{,}
         \StringTok{"TX"}\NormalTok{,   }\FloatTok{0.428407014559672}\NormalTok{,}
         \StringTok{"UT"}\NormalTok{,   }\FloatTok{0.248496577141183}\NormalTok{,}
         \StringTok{"VT"}\NormalTok{,   }\FloatTok{0.553042362822573}\NormalTok{,}
         \StringTok{"VA"}\NormalTok{,   }\FloatTok{0.470739058046787}\NormalTok{,}
         \StringTok{"WA"}\NormalTok{,   }\FloatTok{0.496133477680592}\NormalTok{,}
         \StringTok{"WV"}\NormalTok{,   }\FloatTok{0.295062675817918}\NormalTok{,}
         \StringTok{"WI"}\NormalTok{,   }\FloatTok{0.489912969415965}\NormalTok{,}
         \StringTok{"WY"}\NormalTok{,   }\FloatTok{0.263567780036879}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Now let's find the ML estimates of the two shape parameters of the beta distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain ml estimates}
\NormalTok{log\_lik\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), y) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# pulling these out makes the code a bit easier to follow}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  log\_lik\_i }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(y, }\AttributeTok{shape1 =}\NormalTok{ a, }\AttributeTok{shape2 =}\NormalTok{ b, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  log\_lik }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(log\_lik\_i)}
  \FunctionTok{return}\NormalTok{(log\_lik)}
\NormalTok{\}}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  9.56 11.49
\end{verbatim}

The mean is given by \(\frac{\alpha}{\alpha + \beta}\) and the variance is given by \(\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}\).

We can use the invariance property to obtain ML estimates of the mean and variance using our ML estimates of \(\alpha\) and \(\beta\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OtherTok{\textless{}{-}}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{]}
\NormalTok{b }\OtherTok{\textless{}{-}}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{]}

\NormalTok{a}\SpecialCharTok{/}\NormalTok{(a }\SpecialCharTok{+}\NormalTok{ b)  }\CommentTok{\# mean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4542508
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(a }\SpecialCharTok{*}\NormalTok{ b)}\SpecialCharTok{/}\NormalTok{((a }\SpecialCharTok{+}\NormalTok{ b)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (a }\SpecialCharTok{+}\NormalTok{ b }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))  }\CommentTok{\# var}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01123986
\end{verbatim}

It's worth noting that these correspond closely, \emph{but not exactly} to the observed mean and variance.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4524527
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.01073633
\end{verbatim}

\hypertarget{the-parametric-bootstrap}{%
\section{The Parametric Bootstrap}\label{the-parametric-bootstrap}}

The parametric bootstrap is a powerful, general tool to obtain confidence intervals for estimates from parametric models.

Importantly, we are going to lean \emph{pretty heavily} on the assumption that we have a good model of the distribution of the data. (The predictive distribution below allows us to assess this.) There's also a \emph{\textbf{non}parametric} bootstrap, which is much more popular. We consider that later in the semester.

Suppose we have a sample \(y\) from some known distribution \(f(y; \theta)\) and use \(y\) to estimate the model parameter(s) \(\theta\) or some quantity of interest \(\tau(\theta)\). Remember, we can use ML to estimate either.

To compute a confidence interval, we can use a \emph{parametric} bootstrap. To do implement the parametric bootstrap, do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximate \(f(y; \theta)\) with \(\hat{f} = f(y; \hat{\theta})\). Simulate a new outcome \(y^{\text{bs}}\) from the estimated distribution.
\item
  Re-compute the estimate of interest \(\hat{\theta}^{\text{bs}}\) or \(\hat{\tau}^{\text{bs}}\) using the bootstrapped outcome variable \(y^{\text{bs}}\) rather than the observed outcome \(y\).
\item
  Repeat 1 and 2 many times (say 2,000) to obtain many bootstrapped estimates. To obtain the 95\% confidence interval, take the 2.5th and 97.5th percentiles of the estimates. This is known as the percentile method.
\end{enumerate}

\hypertarget{example-toothpaste-cap-problm}{%
\subsection{Example: Toothpaste Cap Problm}\label{example-toothpaste-cap-problm}}

The code below implements the parametric bootstrap for the toothpaste cap problem. For 2,000 iterations, it draws 150 observations from \(Y \sim \text{Bernoulli}(\hat{\pi} = \frac{8}{150})\). For each iteration, it computes the ML estimate of \(\pi\) for the bootstrapped data set. Then it computes the percentiles to obtain the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_bs)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \DecValTok{8}\SpecialCharTok{/}\DecValTok{150}\NormalTok{)}
\NormalTok{  bs\_est[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(bs\_y)}
\NormalTok{\}}
\FunctionTok{print}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(bs\_est, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# 95\% ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  2.5% 97.5% 
## 0.020 0.093
\end{verbatim}

We leave an evaluation of this confidence interval (i.e., Does it capture \(\theta\) 95\% of the time?) to later in the semester.

\hypertarget{example-beta-distribution-1}{%
\subsection{Example: Beta Distribution}\label{example-beta-distribution-1}}

Now let's apply the parametric bootrap to a two-parameter model: the beta distribution.

First, let's simulate a (fake) data set to use.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set parameters}
\NormalTok{alpha }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{beta }\OtherTok{\textless{}{-}} \DecValTok{2}

\CommentTok{\# simulate data}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, alpha, beta)}
\end{Highlighting}
\end{Shaded}

Now let's find the ML estimates of the two shape parameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain ml estimates}
\NormalTok{log\_lik\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), y) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# pulling these out makes the code a bit easier to follow}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  log\_lik\_i }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(y, }\AttributeTok{shape1 =}\NormalTok{ a, }\AttributeTok{shape2 =}\NormalTok{ b, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  log\_lik }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(log\_lik\_i)}
  \FunctionTok{return}\NormalTok{(log\_lik)}
\NormalTok{\}}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ y,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.46 1.91
\end{verbatim}

Now let's use those ML estimates to perform a parametric bootstrap and find 95\% CIs for the shape parameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain parametric bootstrap 95\% ci for alpha and beta}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ n\_bs, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, }\AttributeTok{shape1 =}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{], }\AttributeTok{shape2 =}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{])}
\NormalTok{  bs\_opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ bs\_y,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{  bs\_est[i, ] }\OtherTok{\textless{}{-}}\NormalTok{ bs\_opt}\SpecialCharTok{$}\NormalTok{par}
\NormalTok{\}}
\NormalTok{ci }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(bs\_est, }\AttributeTok{MARGIN =} \DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\FunctionTok{print}\NormalTok{(ci, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)  }\CommentTok{\# 95\% ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1] [,2]
## 2.5%  4.25 1.52
## 97.5% 7.52 2.58
\end{verbatim}

If instead we cared about the mean of the beta distribution (which is \(\frac{\alpha}{\alpha + \beta}\)), we can use the parametric bootstrap to obtain a confidence interval for that quantity as well.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain parametric bootstrap 95\% ci for mean}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_bs)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, }\AttributeTok{shape1 =}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{], }\AttributeTok{shape2 =}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{])}
\NormalTok{  bs\_opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ bs\_y,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{  bs\_alpha }\OtherTok{\textless{}{-}}\NormalTok{ bs\_opt}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{1}\NormalTok{]}
\NormalTok{  bs\_beta }\OtherTok{\textless{}{-}}\NormalTok{ bs\_opt}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  bs\_est[i] }\OtherTok{\textless{}{-}}\NormalTok{ bs\_alpha}\SpecialCharTok{/}\NormalTok{(bs\_alpha }\SpecialCharTok{+}\NormalTok{ bs\_beta)}
\NormalTok{\}}
\FunctionTok{print}\NormalTok{(}\FunctionTok{quantile}\NormalTok{(bs\_est, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{)), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)  }\CommentTok{\# 95\% ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  2.5% 97.5% 
##  0.71  0.77
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# true mean }
\FunctionTok{print}\NormalTok{(alpha}\SpecialCharTok{/}\NormalTok{(alpha }\SpecialCharTok{+}\NormalTok{ beta), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.71
\end{verbatim}

\hypertarget{sampling-distribution}{%
\section{Sampling Distribution}\label{sampling-distribution}}

What's the most important concept in statistical inference? I don't know, but it could be \textbf{the sampling distribution}. For effect, let me back off the hedge.

\begin{quote}
The most important concept in statistical inference is the \textbf{sampling distribution}.
\end{quote}

To define a sampling distribution, you need to imagine repeating a study over and over. If each study has a random component (perhaps random sampling or random assignment to treatment and control), then the estimate will differ from study to study. The distribution of the estimates across the studies is called the sampling distribution.

\hypertarget{example-the-toothpaste-cap-problem}{%
\subsection{Example: The Toothpaste Cap Problem}\label{example-the-toothpaste-cap-problem}}

For a given sample of 150 tosses, we recognize the the ML estimate \(\hat{\pi}\) does not (usually) exactly equal the parameter \(\pi\). Instead, the particular \(\hat{\pi}\) that the study produces is draw from a distribution.

Let's illustrate that with a simulation. For these simulations, I suppose that we toss the toothpaste cap 150 times and the chance of a head is 5\%.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_sims }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_sims)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  ml\_est[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{\}}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.060 0.040 0.040 0.073 0.067 0.060 0.067 0.053 0.040 0.033
\end{verbatim}

As you can see, the ML estimates vary to from sample to sample--different data sets produce different ML estimates. We need a way to create a confidence interval that consistently captures \(\theta\).

If we repeat the simulations a large number of times, we can see an accuracy picture of the sampling distribution via histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_sims)  }\CommentTok{\# a container for the estimates}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  ml\_est[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{\}}

\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ml\_est =}\NormalTok{ ml\_est)}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-04-bias-and-consistency_files/figure-latex/unnamed-chunk-3-1.pdf}

Many of our methods of evaluating an estimator are statements about the sampling distribution of that estimator. In general, we'd like the sampling distribution to be centered over the true parameter of interest and tightly dispersed.

\hypertarget{bias}{%
\section{Bias}\label{bias}}

Imagine repeatedly sampling and computing the estimate \(\hat{\theta}\) of the parameter \(\theta\) for each sample. In this thought experiment, \(\hat{\theta}\) is a random variable. We say that \(\hat{\theta}\) is \textbf{biased} if \(E(\hat{\theta}) \neq \theta\). We say that \(\hat{\theta}\) is \textbf{unbiased} if \(E(\hat{\theta}) = \theta\). We say that the \textbf{bias} of \(\hat{\theta}\) is \(E(\hat{\theta}) - \theta\).

Importantly, \textbf{ML estimators are not necessarily unbiased}. Of the models we will see in this course, \emph{most} are biased.

\hypertarget{example-bernoulli-distribution-1}{%
\subsection{Example: Bernoulli Distribution}\label{example-bernoulli-distribution-1}}

For example, we can compute the bias of our ML estimator of \(\pi\) in the toothpaste cap problem.

\[
\begin{aligned}
E\left[ \frac{k}{N}\right] &= \frac{1}{N} E(k) = \frac{1}{N} E  \overbrace{ \left( \sum_{n = 1}^N x_n \right) }^{\text{recall } k = \sum_{n = 1}^N x_n } = \frac{1}{N} \sum_{n = 1}^N E(x_n) = \frac{1}{N} \sum_{n = 1}^N \pi = \frac{1}{N}N\pi \\
&= \pi
\end{aligned}
\]

Thus, \(\hat{\pi}^{ML}\) is an unbiased estimator of \(\pi\) in the toothpaste cap problem.

We can use a Monte Carlo simulation to check this analytical result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{100000}
\NormalTok{pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{  pi\_hat[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y)}
\NormalTok{\}}

\CommentTok{\# expected value of pi{-}hat}
\FunctionTok{mean}\NormalTok{(pi\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05006227
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimated monte carlo error}
\FunctionTok{sd}\NormalTok{(pi\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.631271e-05
\end{verbatim}

But notice that the property of unbiasedness does not follow the estimate through transformation. Because the sample is relatively large in this case (150 tosses), the bias is small, but detectable with 100,000 Monte Carlo simulations

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{odds\_hat }\OtherTok{\textless{}{-}}\NormalTok{ pi\_hat}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi\_hat)}

\CommentTok{\# actual odds}
\FloatTok{0.05}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05263158
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# expected value of odds{-}hat}
\FunctionTok{mean}\NormalTok{(odds\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05307323
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimated monte carlo error}
\FunctionTok{sd}\NormalTok{(odds\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6.288517e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the z{-}statistic}
\NormalTok{(}\FunctionTok{mean}\NormalTok{(odds\_hat) }\SpecialCharTok{{-}} \FloatTok{0.05}\SpecialCharTok{/}\FloatTok{0.95}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sd}\NormalTok{(odds\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.023072
\end{verbatim}

\hypertarget{example-poisson-distribution-1}{%
\subsection{Example: Poisson Distribution}\label{example-poisson-distribution-1}}

Using math almost identical to the toothpaste cap problem, we can show that the ML estimator \(\hat{\lambda} = \text{avg}(x)\) is an unbiased estimator of \(\lambda\).

We can also illustrate the unbiasedness with a computer simulation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lambda }\OtherTok{\textless{}{-}} \FloatTok{4.0}      \CommentTok{\# the parameter we\textquotesingle{}re trying to estimate}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \DecValTok{10}  \CommentTok{\# the sample size we\textquotesingle{}re using in each "study"}

\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}  \CommentTok{\# the number of times we repeat the "study"}
\NormalTok{lambda\_hat }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)  }\CommentTok{\# a container }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(sample\_size, }\AttributeTok{lambda =}\NormalTok{ lambda)}
\NormalTok{  lambda\_hat[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{\}}

\CommentTok{\# expected value of lambda{-}hat}
\FunctionTok{mean}\NormalTok{(lambda\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.99397
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# estimated monte carlo error}
\FunctionTok{sd}\NormalTok{(lambda\_hat)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(n\_mc\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.006300177
\end{verbatim}

\hypertarget{consistency}{%
\section{Consistency}\label{consistency}}

Imagine taking a sample of size \(N\) and computing the estimate \(\hat{\theta}_N\) of the parameter \(\theta\). We say that \(\hat{\theta}\) is a \textbf{consistent} estimator of \(\theta\) if \(\hat{\theta}\) converges in probability to \(\theta\).

Intuitively, this means the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For a large enough sample, the estimator returns the exact right answer.
\item
  For a large enough sample, the estimate \(\hat{\theta}\) does not vary any more, but collapses onto a single point and that point is \(\theta\).
\end{enumerate}

Under weak, but somewhat technical, assumptions that usually hold, ML estimators are consistent.

Given that we always have finite samples, why is consistency valuable? In short, it's not valuable, directly. However, consistent estimators tend to be decent with small samples.

But it does not follow that consistent estimators work well in small samples. However, as a rough guideline, consistent estimators work well for small samples. However, whether they actually work well in any particular situation needs a more careful investigation.

\hypertarget{example-illustrative}{%
\subsection{Example: Illustrative}\label{example-illustrative}}

To illustrate the concept of consistency, consider this estimator of the population mean \(\hat{\mu}^{\text{silly}} = \frac{\sum_{i = 1}^N x_i}{N + 10}\). While this estimator is biased, it is a consistent estimator.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{sample\_sizes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{10000}\NormalTok{, }\DecValTok{100000}\NormalTok{)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{30}  \CommentTok{\# for each sample size}
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()  }\CommentTok{\# grow with each iteration; slow, but easy}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(sample\_sizes)) \{}
\NormalTok{  ml\_est\_i }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(population, sample\_sizes[i], }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{    ml\_est\_i[j] }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(x)}\SpecialCharTok{/}\NormalTok{(sample\_sizes[i] }\SpecialCharTok{+} \DecValTok{10}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{  results\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{sample\_size =}\NormalTok{ sample\_sizes[i],}
                             \AttributeTok{ml\_est =}\NormalTok{ ml\_est\_i)}
\NormalTok{\}}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{bind\_rows}\NormalTok{(results\_list) }

\FunctionTok{ggplot}\NormalTok{(results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_size, }\AttributeTok{y =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{mean}\NormalTok{(population)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-04-bias-and-consistency_files/figure-latex/unnamed-chunk-7-1.pdf}

\hypertarget{example-bernoulli-odds-1}{%
\subsection{Example: Bernoulli Odds}\label{example-bernoulli-odds-1}}

There are two ways to see consistency for the Bernoulli. First, unless our sample size is a multiple of 20, it is impossible to obtain an estimated odds of 0.05/(1 - 0.05). Second, in small samples, the ML estimate of the odds is biased. As the sample size increases, the bias shrinks and the estimates collapse toward (and eventually onto) the true value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_sizes }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{100}\NormalTok{, }\DecValTok{250}\NormalTok{, }\DecValTok{400}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{750}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{10}  \CommentTok{\# for each sample size}
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()  }\CommentTok{\# grow with each iteration; slow, but easy}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(sample\_sizes)) \{}
\NormalTok{  ml\_est\_i }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(sample\_sizes[i], }\DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.05}\NormalTok{)}
\NormalTok{    pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(x)}
\NormalTok{    ml\_est\_i[j] }\OtherTok{\textless{}{-}}\NormalTok{ pi\_hat}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi\_hat)}
\NormalTok{  \}}
\NormalTok{  results\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{sample\_size =}\NormalTok{ sample\_sizes[i],}
                             \AttributeTok{ml\_est =}\NormalTok{ ml\_est\_i)}
\NormalTok{\}}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{bind\_rows}\NormalTok{(results\_list) }

\FunctionTok{ggplot}\NormalTok{(results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_size, }\AttributeTok{y =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FloatTok{0.05}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{shape =} \DecValTok{19}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{01-04-bias-and-consistency_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(results, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_size, }\AttributeTok{y =}\NormalTok{ ml\_est)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FloatTok{0.05}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-04-bias-and-consistency_files/figure-latex/unnamed-chunk-8-2.pdf}

\hypertarget{predictive-distribution}{%
\section{Predictive Distribution}\label{predictive-distribution}}

In Bayesian statistics, a popular tool for model evaluation is the posterior predictive distribution. But we might use an analogous approach for models fit with maximum likelihood.

The predictive distribution is just the distribution given the ML estimates. Using our notation above, the predictive distribution is \(f(y; \hat{\theta})\).

When you perform a parametric bootstrap, you are resampling from this predictive distribution. Here, we're going to use it for a different purpose: to understand and evaluate our model.

In my view, the predictive distribution is the best way to (1) understand, (2) evaluate, and then (3) improve models.

You can use the predictive distribution as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit your model with maximum likelihood.
\item
  Simulate a new outcome variable using the estimated model parameters (i.e., \(f(y; \hat{theta})\)). Perhaps simulate a handful for comparison.
\item
  Compare the simulated outcome variable(s) to the observed outcome variables.
\end{enumerate}

\hypertarget{example-poisson-distribution-2}{%
\subsection{Example: Poisson Distribution}\label{example-poisson-distribution-2}}

Earlier, we fit a Poisson distribution to a sample of data from Hultman, Kathman, and Shannon (2013).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ml\_est }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(civilian\_casualties)}
\FunctionTok{print}\NormalTok{(ml\_est, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 630
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(civilian\_casualties)}
\NormalTok{y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =}\NormalTok{ ml\_est)}
\FunctionTok{print}\NormalTok{(y\_pred[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 598 646 640 635 599 644 675 647 660 621 604 648 664 645 637 610 607 632 660
## [20] 627 611 649 621 610 624 636 601 662 648 663
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(civilian\_casualties[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{30}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  0  0  0  0  0 13  0  0 61  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0
## [26]  0  0  0  0 19
\end{verbatim}

Simply printing a few results, we can immediately see a problem with data, when compared with the raw data

To see it even more clearly, we can create a histogram of the observed and simulated data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}

\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(civilian\_casualties)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(y\_pred)}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-05-predictive-distribution_files/figure-latex/unnamed-chunk-4-1.pdf}
These data sets are so different that the plots are difficult to read, so we might put the x-axes on the log scale. Note, though, that the two plots have very different ranges on the axes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(civilian\_casualties) }\SpecialCharTok{+} \FunctionTok{scale\_x\_log10}\NormalTok{()}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{qplot}\NormalTok{(y\_pred) }\SpecialCharTok{+} \FunctionTok{scale\_x\_log10}\NormalTok{()}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-05-predictive-distribution_files/figure-latex/unnamed-chunk-5-1.pdf}

For a more accurate and complete comparison, let's simulate five fake data sets and use common axes

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{observed\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(civilian\_casualties, }\AttributeTok{type =} \StringTok{"observed"}\NormalTok{)}

\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(n, }\AttributeTok{lambda =}\NormalTok{ ml\_est)}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{civilian\_casualties =}\NormalTok{ y\_pred, }
                          \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,500
## Columns: 2
## $ civilian_casualties <dbl> 685, 573, 666, 646, 627, 617, 673, 653, 668, 637, ~
## $ type                <chr> "simulated #1", "simulated #1", "simulated #1", "s~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ civilian\_casualties)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-05-predictive-distribution_files/figure-latex/unnamed-chunk-6-1.pdf}
The fit of this model is almost absurd.

\hypertarget{example-beta-distribution-2}{%
\subsection{Example: Beta Distribution}\label{example-beta-distribution-2}}

Now let's return to our beta model of states' opinions toward the ACA in the \texttt{br} data frame we loaded earlier.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain ml estimates}
\NormalTok{log\_lik\_fn }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), y) \{}
\NormalTok{  a }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# pulling these out makes the code a bit easier to follow}
\NormalTok{  b }\OtherTok{\textless{}{-}}\NormalTok{ par[}\DecValTok{2}\NormalTok{]}
\NormalTok{  log\_lik\_i }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(y, }\AttributeTok{shape1 =}\NormalTok{ a, }\AttributeTok{shape2 =}\NormalTok{ b, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{  log\_lik }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(log\_lik\_i)}
  \FunctionTok{return}\NormalTok{(log\_lik)}
\NormalTok{\}}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(}\AttributeTok{par =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\AttributeTok{fn =}\NormalTok{ log\_lik\_fn, }\AttributeTok{y =}\NormalTok{ br}\SpecialCharTok{$}\NormalTok{prop\_favorable\_aca,}
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}
\NormalTok{ml\_est }\OtherTok{\textless{}{-}}\NormalTok{ opt}\SpecialCharTok{$}\NormalTok{par}
\end{Highlighting}
\end{Shaded}

Now let's simulate some fake data from the predictive distribution and compare that to the observed data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{observed\_data }\OtherTok{\textless{}{-}}\NormalTok{ br }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"observed"}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(br)}
\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  y\_pred }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(n, }\AttributeTok{shape1 =}\NormalTok{ ml\_est[}\DecValTok{1}\NormalTok{], }\AttributeTok{shape2 =}\NormalTok{ ml\_est[}\DecValTok{2}\NormalTok{])}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{prop\_favorable\_aca =}\NormalTok{ y\_pred, }
                          \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }

\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_favorable\_aca)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01-05-predictive-distribution_files/figure-latex/unnamed-chunk-9-1.pdf}

On the whole, we see hear a fairly close correspondence between the observed and simulated data. That suggests that our model is a good description of the data.

\hypertarget{week-2-bayesian-inference}{%
\chapter{Week 2: Bayesian Inference}\label{week-2-bayesian-inference}}

This week, we introduce the following tools.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{engine} Bayesian inference
\item
  \textbf{distributions} normal, uniform
\item
  \textbf{confidence intervals} posterior simulation, Bayesian credible intervals, percentile intervals.
\item
  \textbf{quantities of interest} transforming posterior simulations
\item
  \textbf{evaluating models} posterior predictive distribution
\end{enumerate}

\hypertarget{bayesian-inference}{%
\section{Bayesian Inference}\label{bayesian-inference}}

Bayesian inference follows a simple recipe:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a distribution for the data.
\item
  Choose a distribution to describe your prior beliefs.
\item
  Update the prior distribution upon observing the data by computing the posterior distribution.
\end{enumerate}

In simple examples, we can implement this process analytically and obtain a closed-form posterior. In most applied cases, we can only \emph{sample from} the posterior distribution, but this turns out to work almost as well.

\hypertarget{mechanics}{%
\subsection{Mechanics}\label{mechanics}}

Suppose a random sample from a distribution \(f(x; \theta)\) that depends on the unknown parameter \(\theta\).

Bayesian inference models our \emph{beliefs} about the unknown parameter \(\theta\) as a distribution. It answers the question: what should we believe about \(\theta\), given the observed samples \(x = \{x_1, x_2, ..., x_n\}\) from \(f(x; \theta)\)? These beliefs are simply the conditional distribution \(f(\theta \mid x)\).

By Bayes' rule, \(\displaystyle f(\theta \mid x) = \frac{f(x \mid \theta)f(\theta)}{f(x)} = \frac{f(x \mid \theta)f(\theta)}{\displaystyle \int_{-\infty}^\infty f(x \mid \theta)f(\theta) d\theta}\).

\[
\displaystyle \underbrace{f(\theta \mid x)}_{\text{posterior}} = \frac{\overbrace{f(x \mid \theta)}^{\text{likelihood}} \times \overbrace{f(\theta)}^{\text{prior}}}{\displaystyle \underbrace{\int_{-\infty}^\infty f(x \mid \theta)f(\theta) d\theta}_{\text{normalizing constant}}}
\]
There are four parts to a Bayesian analysis.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(f(\theta \mid x)\). ``The posterior;'' what we're trying to find. This distribution models our beliefs about parameter \(\theta\) given the data \(x\).
\item
  \(f(x \mid \theta)\). ``The likelihood.'' This distribution model conditional density/probability of the data \(x\) given the parameter \(\theta\). We need to invert the conditioning in order to find the posterior.
\item
  \(f(\theta)\). ``The prior;'' our beliefs about \(\theta\) prior to observing the sample \(x\).
\item
  \(f(x) =\int_{-\infty}^\infty f(x \mid \theta)f(\theta) d\theta\). A normalizing constant. Recall that the role of the normalizing constant is to force the distribution to integrate or sum to one. Therefore, we can safely ignore this constant until the end, and then find proper normalizing constant.
\end{enumerate}

It's convenient to choose a \textbf{conjugate} prior distribution that, when combined with the likelihood, produces a posterior from the same family as the prior.

The resulting distribution is a complete and correct summary of our updated beliefs about the parameters.

\hypertarget{posterior-summaries}{%
\subsection{Posterior Summaries}\label{posterior-summaries}}

If we want to summarize the posterior distribution, then we can (though we lose some information).

First, we might summarize the distribution using a single point to make a ``best guess'' at the parameter of interest. We have three options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{The posterior mean}. The posterior mean minimizes a squared-error loss function.
\item
  \emph{The posterior median}: The posterior median minimizes an absolute loss function where the cost of guessing \(a\) when the truth is \(\alpha\) is \(|a - \alpha|\). Intuitively, there's a 50\% chance that \(\pi\) falls above and below the posterior median.
\item
  \emph{The posterior mode}: The posterior mode is the most likely value of \(\pi\), so it minimizes a loss function that penalizes all misses equally.
\end{enumerate}

Second, we might find an \(100(1 - \alpha)\%\) credible interval, by finding an interval that that integrates to \((1 - \alpha)\). That is, a region that has a \(100(1 - \alpha)\%\) chance of containing the parameter. This interval is not unique; there are many. However, \emph{one} \(100(1 - \alpha)\%\) credible interval is the \(100(1 - \alpha)\%\) \emph{percentile} credible interval. Construct this interval by finding the \(100\frac{\alpha}{2}th\) percentile and the \(100(1 - \frac{\alpha}{2})th\) percentile. For example, if we want a 90\% credible interval, we would find the 5th and 95th percentiles.

\hypertarget{posterior-simulation}{%
\subsection{Posterior Simulation}\label{posterior-simulation}}

In some cases, we have an analytical solution for the posterior---we can write down the equation for the posterior. But in most cases, we cannot write down the posterior. Perhaps unexpectedly, it is usually easier to \emph{sample from} the distribution that write down the posterior in closed form.

But notice that the samples are almost as good as the closed-form solution. We can sample from the distribution many times and then draw the histogram, compute the average, and find the percentiles. Except for sampling error that we can make arbitraryily small, these correspond to the posterior density, the posterior mean, and the 95\% (percentile) credible interval.

\hypertarget{example-bernoulli}{%
\section{Example: Bernoulli}\label{example-bernoulli}}

As a running example, we use the \textbf{toothpaste cap problem}:

\begin{quote}
We have a toothpaste cap--one with a wide bottom and a narrow top. We're going to toss the toothpaste cap. It can either end up lying on its side, its (wide) bottom, or its (narrow) top.
\end{quote}

\begin{quote}
We want to estimate the probability of the toothpaste cap landing on its top.
\end{quote}

\begin{quote}
We can model each toss as a Bernoulli trial, thinking of each toss as a random variable \(X\) where \(X \sim \text{Bernoulli}(\pi)\). If the cap lands on its top, we think of the outcome as 1. If not, as 0.
\end{quote}

\begin{quote}
Suppose we toss the cap \(N\) times and observe \(k\) tops. What is the posterior distribution of \(\pi\)?
\end{quote}

\hypertarget{likelihood}{%
\subsection{The Likelihood}\label{likelihood}}

According to the model \(f(x_i \mid \pi) = \pi^{x_i} (1 - \pi)^{(1 - x_i)}\). Because the samples are iid, we can find the \emph{joint} distribution \(f(x) = f(x_1) \times ... \times f(x_N) = \prod_{i = 1}^N f(x_i)\). We're just multiplying \(k\) \(\pi\)s (i.e., each of the \(k\) ones has probability \(\pi\)) and \((N - k)\) \((1 - \pi)\)s (i.e., each of the \(N - k\) zeros has probability \(1 - \pi\)), so that the \(f(x | \pi) = \pi^{k} (1 - \pi)^{(N - k)}\).

\[
\text{the likelihood:  } f(x | \pi) = \pi^{k} (1 - \pi)^{(N - k)}, \text{where } k = \sum_{n = 1}^N x_n \\
\]

\hypertarget{the-prior}{%
\subsection{The Prior}\label{the-prior}}

The prior describes your beliefs about \(\pi\) \emph{before} observing the data.

Here are some questions that we might ask ourselves the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What's the most likely value of \(\pi\)? \emph{Perhaps 0.15.}
\item
  Are our beliefs best summarizes by a distribution that's skewed to the left or right? \emph{To the right.}
\item
  \(\pi\) is about \_\_\_\_\_, give or take \_\_\_\_\_ or so. \emph{Perhaps 0.17 and 0.10.}
\item
  There's a 25\% chance that \(\pi\) is less than \_\_\_\_. \emph{Perhaps 0.05.}
\item
  There's a 25\% chance that \(\pi\) is greater than \_\_\_\_. \emph{Perhaps 0.20}.
\end{enumerate}

Given these answers, we can sketch the pdf of the prior distribution for \(\pi\).

\includegraphics{02-01-bayes_files/figure-latex/unnamed-chunk-2-1.pdf}

Now we need to find a density function that matches these prior beliefs. For this Bernoulli model, the \emph{beta distribution} is the conjugate prior. While a conjugate prior is not crucial in general, it makes the math much more tractable.

So then what beta distribution captures our prior beliefs?

There's a code snippet \href{https://gist.github.com/carlislerainey/45414e0d9f22e4e1960449402e6a8048}{here} to help you explore different beta distributions.

After some exploration, I find that setting the parameters \(\alpha\) and \(\beta\) of the beta distribution to 3 and 15, respectively, captures my prior beliefs about the probability of getting a top.

\includegraphics{02-01-bayes_files/figure-latex/unnamed-chunk-3-1.pdf}
The pdf of the beta distribution is \(f(x) = \frac{1}{B(\alpha, \beta)} x^{\alpha - 1}(1 - x)^{\beta - 1}\). Remember that \(B()\) is the beta function, so \(\frac{1}{B(\alpha, \beta)}\) is a constant.

Let's denote our chosen values of \(\alpha = 3\) and \(\beta = 15\) as \(\alpha^*\) and \(\beta^*\). As we see in a moment, it's convenient distinguish the parameters in the prior distribution from other parameters.

\[
\text{the prior:  }  f(\pi) = \frac{1}{B(\alpha^*, \beta^*)} \pi^{\alpha^* - 1}(1 - \pi)^{\beta^* - 1}
\]

\hypertarget{the-posterior}{%
\subsection{The Posterior}\label{the-posterior}}

Now we need to compute the posterior by multiplying the likelihood times the prior and then finding the normalizing constant.
\[
\text{the posterior: } \displaystyle \underbrace{f(\pi \mid x)}_{\text{posterior}} = \frac{\overbrace{f(x \mid \pi)}^{\text{likelihood}} \times \overbrace{f(\pi)}^{\text{prior}}}{\displaystyle \underbrace{\int_{-\infty}^\infty f(x \mid \pi)f(\pi) d\pi}_{\text{normalizing constant}}} \\
\]
Now we plug in the likelihood, plug in the prior, and denote the normalizing constant as \(C_1\) to remind ourselves that it's just a constant.

\[
\displaystyle f(\pi \mid x) = \frac{\left[ \pi^{k} (1 - \pi)^{(N - k) }\right] \times \left[ \frac{1}{B(\alpha^*, \beta^*)} \pi^{\alpha^* - 1}(1 - \pi)^{\beta^* - 1} \right]}{ C_1} \\
\]

\[
\text{the posterior: } \displaystyle \underbrace{f(\pi \mid x)}_{\text{posterior}} = \frac{\overbrace{\left[ \pi^{k} (1 - \pi)^{(N - k) }\right] }^{\text{likelihood}} \times \overbrace{ \left[ \frac{1}{B(\alpha^*, \beta^*)} \pi^{\alpha^* - 1}(1 - \pi)^{\beta^* - 1} \right] }^{\text{prior}}}{\displaystyle \underbrace{C_1}_{\text{normalizing constant}}} \\
\]

Now we need to simplify the right-hand side.

First, notice that the term \(\frac{1}{B(\alpha^*, \beta^*)}\) in the numerator is just a constant. We can incorporate that constant term with \(C_1\) by multiplying top and bottom by \(B(\alpha^*, \beta^*)\) and letting \(C_2 = C_1 \times B(\alpha^*, \beta^*)\).

\[
\text{the posterior: } \displaystyle \underbrace{f(\pi \mid x)}_{\text{posterior}} = \frac{\overbrace{\left[ \pi^{k} (1 - \pi)^{(N - k) }\right] }^{\text{likelihood}} \times  \left[ \pi^{\alpha^* - 1}(1 - \pi)^{\beta^* - 1} \right] }{\displaystyle \underbrace{C_2}_{\text{new normalizing constant}}} \\
\]

Now we can collect the exponents with base \(\pi\) and the exponents with base \((1 - \pi)\).

\[
\text{the posterior: } \displaystyle \underbrace{f(\pi \mid x)}_{\text{posterior}} = \frac{\left[ \pi^{k} \times \pi^{\alpha^* - 1} \right] \times  \left[ (1 - \pi)^{(N - k) } \times (1 - \pi)^{\beta^* - 1} \right] }{ C_2} \\
\]
Recalling that \(x^a \times x^b = x^{a + b}\), we combine the powers.

\[
\text{the posterior: } \displaystyle \underbrace{f(\pi \mid x)}_{\text{posterior}} = \frac{\left[ \pi^{(\alpha^* + k) - 1} \right] \times  \left[ (1 - \pi)^{[\beta^* + (N - k)] - 1} \right] }{ C_2} \\
\]
\[
\displaystyle f(\theta \mid x) = \frac{f(x \mid \theta) \times f(\theta)}{\displaystyle \int_{-\infty}^\infty f(x \mid \theta)f(\theta) d\theta}
\]

Because we're clever, we notice that this is \emph{almost} a beta distribution with \(\alpha = (\alpha^* + k)\) and \(\beta = [\beta^* + (N - k)]\). If \(C_2 = B(\alpha^* + k, \beta^* + (N - k))\), then the posterior is \emph{exactly} a \(\text{beta}(\alpha^* + k, \beta^* + [N - k]))\) distribution.

This is completely expected. We chose a beta distribution for the prior because it would give us a beta posterior distribution. For simplicity, we can denote the parameter for the beta posterior as \(\alpha^\prime\) and \(\beta^\prime\), so that \(\alpha^\prime = \alpha^* + k\) and \(\beta^\prime = \beta^* + [N - k]\)

\[
\begin{aligned}
\text{the posterior: } \displaystyle \underbrace{f(\pi \mid x)}_{\text{posterior}} &= \frac{ \pi^{\overbrace{(\alpha^* + k)}^{\alpha^\prime} - 1}  \times  (1 - \pi)^{\overbrace{[\beta^* + (N - k)]}^{\beta^\prime} - 1}  }{ B(\alpha^* + k, \beta^* + [N - k])} \\
&= \frac{ \pi^{\alpha^\prime - 1}  \times  (1 - \pi)^{\beta^\prime - 1}  }{ B(\alpha^\prime, \beta^\prime)}, \text{where } \alpha^\prime = \alpha^* + k \text{ and } \beta^\prime = \beta^* + [N - k]
\end{aligned}
\]

This is an elegant, simple solution. To obtain the parameters for the beta posterior distribution, we just add the number of tops (Bernoulli successes) to the prior value for \(\alpha\) and the number of not-tops (sides and bottoms; Bernoulli failures) to the prior value for \(\beta\).

Suppose that I tossed the toothpaste cap 150 times and got 8 tops.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# prior parameters}
\NormalTok{alpha\_prior }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{beta\_prior }\OtherTok{\textless{}{-}} \DecValTok{15}

\CommentTok{\# data }
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{8}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{150}

\CommentTok{\# posterior parameters}
\NormalTok{alpha\_posterior }\OtherTok{\textless{}{-}}\NormalTok{ alpha\_prior }\SpecialCharTok{+}\NormalTok{ k}
\NormalTok{beta\_posterior }\OtherTok{\textless{}{-}}\NormalTok{ beta\_prior }\SpecialCharTok{+}\NormalTok{ N }\SpecialCharTok{{-}}\NormalTok{ k}

\CommentTok{\# plot prior and posterior}
\NormalTok{gg\_prior }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ dbeta, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{shape1 =}\NormalTok{ alpha\_prior, }\AttributeTok{shape2 =}\NormalTok{ beta\_prior)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"prior distribution"}\NormalTok{, }\AttributeTok{x =} \StringTok{"pi"}\NormalTok{, }\AttributeTok{y =} \StringTok{"prior density"}\NormalTok{)}
\NormalTok{gg\_posterior }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ dbeta, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{shape1 =}\NormalTok{ alpha\_posterior, }\AttributeTok{shape2 =}\NormalTok{ beta\_posterior)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"posterior distribution"}\NormalTok{, }\AttributeTok{x =} \StringTok{"pi"}\NormalTok{, }\AttributeTok{y =} \StringTok{"posterior density"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{gg\_prior }\SpecialCharTok{+}\NormalTok{ gg\_posterior}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-01-bayes_files/figure-latex/unnamed-chunk-4-1.pdf}

\hypertarget{point-estimates}{%
\subsection{Point Estimates}\label{point-estimates}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{The posterior mean}. The posterior mean minimizes a squared-error loss function. That is, the cost of guessing \(a\) when the truth is \(\alpha\) is \((a - \alpha)^2\). In the case of the beta posterior, it's just \(\dfrac{\alpha^\prime}{\alpha^\prime + \beta^\prime}\). For our prior and data, we have \(\dfrac{3 + 8}{(3 + 8) + (15 + 150 - 8)} \approx 0.065\).
\item
  \emph{The posterior median}: The posterior median minimizes an absolute loss function where the cost of guessing \(a\) when the truth is \(\alpha\) is \(|a - \alpha|\). Intuitively, there's a 50\% chance that \(\pi\) falls above and below the posterior median. In the case of the beta posterior, it's just \(\dfrac{\alpha^\prime - \frac{1}{3}}{\alpha^\prime + \beta^\prime - \frac{2}{3}}\) (for \(\alpha^\prime, \beta^\prime > 1\)). For our prior and data, we have \(\dfrac{3 + 8 -\frac{1}{3}}{(3 + k) + (15 + 150 - 8) - \frac{2}{3}} \approx 0.064\).
\item
  \emph{The posterior mode}: The posterior mode is the most likely value of \(\pi\), so it minimizes a loss function that penalizes all misses equally. In the case of the beta posterior, it's just \(\dfrac{\alpha^\prime - 1}{\alpha^\prime + \beta^\prime - 2}\) (for \(\alpha^\prime, \beta^\prime > 1\)). For our prior and data, we have \(\dfrac{3 + 8 - 1}{(3 + k) + (15 + 150 - 8) - 2} \approx 0.060\).
\end{enumerate}

\hypertarget{credible-interval}{%
\subsection{Credible Interval}\label{credible-interval}}

Using the percentile method, we can compute the 90\% and 95\% credible intervals with \texttt{qbeta()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 90\% credible interval}
\FunctionTok{qbeta}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{), }\DecValTok{3} \SpecialCharTok{+} \DecValTok{8}\NormalTok{, }\DecValTok{15} \SpecialCharTok{+} \DecValTok{150} \SpecialCharTok{{-}} \DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03737493 0.09945329
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 95\% credible interval}
\FunctionTok{qbeta}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{), }\DecValTok{3} \SpecialCharTok{+} \DecValTok{8}\NormalTok{, }\DecValTok{15} \SpecialCharTok{+} \DecValTok{150} \SpecialCharTok{{-}} \DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03333712 0.10736323
\end{verbatim}

\hypertarget{simulation}{%
\subsection{Simulation}\label{simulation}}

We don't need to use simulation here---we have the simple closed-form posterior. However, let's see how simulation would work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post\_sims }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{3} \SpecialCharTok{+} \DecValTok{8}\NormalTok{, }\DecValTok{15} \SpecialCharTok{+} \DecValTok{150} \SpecialCharTok{{-}} \DecValTok{8}\NormalTok{)}

\CommentTok{\# posterior density}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(post\_sims)}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ post\_sims)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{02-01-bayes_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# posterior mean}
\FunctionTok{mean}\NormalTok{(post\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.06558303
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# credible interval}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-poisson-distribution-3}{%
\section{Example: Poisson Distribution}\label{example-poisson-distribution-3}}

Suppose we collect \(N\) random samples \(x = \{x_1, x_2, ..., x_N\}\) and model each draw as a random variable \(X \sim \text{Poisson}(\lambda)\). Find the posterior distribution of \(\lambda\) for the gamma prior distribution. Hint: the gamma distribution is the conjugate prior for the Poisson likelihood.

\[
\begin{aligned}
\text{Poisson likelihood: } f(x \mid \lambda) &= \prod_{n = 1}^N \frac{\lambda^{x_n} e^{-\lambda}}{x_n!} \\
&= \displaystyle \left[ \frac{1}{\prod_{n = 1}^N x_n !} \right]e^{-N\lambda}\lambda^{\sum_{n = 1}^N x_n}
\end{aligned}
\]

\[
\text{Gamma prior: } f( \lambda; \alpha^*, \beta^*) = \frac{{\beta^*}^{\alpha^*}}{\Gamma(\alpha^*)} \lambda^{\alpha^* - 1} e^{-\beta^*\lambda}
\]
To find the posterior, we multiply the likelihood times the prior and normalize. Because the gamma prior distribution is the conjugate prior for the Poisson likelihood, we know that the posterior will be a gamma distribution.

\[
\begin{aligned}
\text{Gamma posterior: } f( \lambda  \mid x) &= \frac{\left( \displaystyle \left[ \frac{1}{\prod_{n = 1}^N x_n !} \right]e^{-N\lambda}\lambda^{\sum_{n = 1}^N x_n}\right) \times \left( \left[ \frac{{\beta^*}^{\alpha^*}}{\Gamma(\alpha^*)} \right] \lambda^{\alpha^* - 1} e^{-\beta^*\lambda}\right)}{C_1} \\
\end{aligned}
\]
Because \(x\), \(\alpha_*\), and \(\beta\) are fixed, the terms in square brackets are constant, so we can safely consider those part of the normalizing constant.

\[
\begin{aligned}
&= \frac{\left( \displaystyle  e^{-N\lambda}\lambda^{\sum_{n = 1}^N x_n}\right) \times \left( \lambda^{\alpha^* - 1} e^{-\beta^*\lambda}\right)}{C_2} \\
\end{aligned}
\]
Now we can collect the exponents with the same base.

\[
\begin{aligned}
&= \frac{\left( \lambda^{\alpha^* - 1} \times \lambda^{\sum_{n = 1}^N x_n}\right) \times \left( \displaystyle  e^{-N\lambda} \times e^{-\beta^*\lambda} \right)}{C_2} \\
&= \frac{\lambda^{ \overbrace{\left[ \alpha^* + \sum_{n = 1}^N x_n \right]}^{\alpha^\prime} - 1}  e^{-\overbrace{[\beta^* + N]}^{\beta^\prime}\lambda} }{C_2} \\
\end{aligned}
\]

We recognize this as \emph{almost} a Gamma distribution with parameters \(\alpha^\prime = \alpha^* + \sum_{n = 1}^N x_n\) and \(\beta^\prime = \beta^* + N\). Indeed, if \(\frac{1}{C_2} = \frac{{\beta^\prime}^{\alpha^\prime}}{\Gamma(\alpha^{\prime})}\), then we have exactly a gamma distribution.

\[
\begin{aligned}
&= \frac{{\beta^\prime}^{\alpha^\prime}}{\Gamma(\alpha^{\prime})} \lambda^{ \alpha^\prime - 1}  e^{-\beta^\prime\lambda}, \text{where } \alpha^\prime = \alpha^* +  \sum_{n = 1}^N x_n \text{ and } \beta^\prime = \beta^* + N
\end{aligned}
\]

Like the Bernoulli likelihood with the beta prior, the Poisson likelihood withe the gamma prior gives a nice result. We start with values parameters of the gamma distribution \(\alpha = \alpha^*\) and \(\beta + \beta^*\) so that the gamma prior distribution describes our prior beliefs about the parameters \(\lambda\) of the Poisson distribution. Then we add the sum of the data \(x\) to \(\alpha^*\) and the number of samples \(N\) to \(\beta^*\) to obtain the parameters of the gamma posterior distribution.

The code below shows the posterior distribution

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set see to make reproducible}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\CommentTok{\# prior parameters}
\NormalTok{alpha\_prior }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{beta\_prior }\OtherTok{\textless{}{-}} \DecValTok{3}

\CommentTok{\# create an "unknown" value of lambda to estimate}
\NormalTok{lambda }\OtherTok{\textless{}{-}} \DecValTok{2}

\CommentTok{\# generate a data set}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{5}  \CommentTok{\# number of samples}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(N, }\AttributeTok{lambda =}\NormalTok{ lambda)}
\FunctionTok{print}\NormalTok{(x)  }\CommentTok{\# print the data set}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 2 2 2 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# posterior parameters}
\NormalTok{alpha\_posterior }\OtherTok{\textless{}{-}}\NormalTok{ alpha\_prior }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{(x)}
\NormalTok{beta\_posterior }\OtherTok{\textless{}{-}}\NormalTok{ beta\_prior }\SpecialCharTok{+}\NormalTok{ N}

\CommentTok{\# plot prior and posterior}
\NormalTok{gg\_prior }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ dgamma, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{shape =}\NormalTok{ alpha\_prior, }\AttributeTok{rate =}\NormalTok{ beta\_prior)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"prior distribution"}\NormalTok{, }\AttributeTok{x =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{y =} \StringTok{"prior density"}\NormalTok{)}
\NormalTok{gg\_posterior }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ dgamma, }\AttributeTok{args =} \FunctionTok{list}\NormalTok{(}\AttributeTok{shape =}\NormalTok{ alpha\_posterior, }\AttributeTok{rate =}\NormalTok{ beta\_posterior)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"posterior distribution"}\NormalTok{, }\AttributeTok{x =} \StringTok{"lambda"}\NormalTok{, }\AttributeTok{y =} \StringTok{"posterior density"}\NormalTok{)}
\NormalTok{gg\_prior }\SpecialCharTok{+}\NormalTok{ gg\_posterior  }\CommentTok{\# uses patchwork package}
\end{Highlighting}
\end{Shaded}

\includegraphics{02-01-bayes_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# posterior mean: alpha/beta}
\NormalTok{alpha\_posterior}\SpecialCharTok{/}\NormalTok{beta\_posterior}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.625
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# posterior mode: (alpha {-} 1)/beta for alpha \textgreater{} 1}
\NormalTok{(alpha\_posterior }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\NormalTok{beta\_posterior}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 90\% credible interval}
\FunctionTok{qgamma}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{), alpha\_posterior, beta\_posterior)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9611973 2.4303212
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 95\% credible interval}
\FunctionTok{qgamma}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{), alpha\_posterior, beta\_posterior)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8652441 2.6201981
\end{verbatim}

In the case of the posterior median, there is no closed-form solution, even though we know the form of the posterior. We can use simulation to obtain the median.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# posterior median: no closed form, so simulate}
\NormalTok{post\_sims }\OtherTok{\textless{}{-}} \FunctionTok{rgamma}\NormalTok{(}\DecValTok{1000}\NormalTok{, alpha\_posterior, beta\_posterior)}
\FunctionTok{median}\NormalTok{(post\_sims)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.59919
\end{verbatim}

\hypertarget{remarks-1}{%
\section{Remarks}\label{remarks-1}}

Bayesian inference presents two difficulties.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choosing a prior.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    It can be hard to actually construct a prior distribution. It's challenging when dealing with a single parameter. It becomes much more difficult when dealing with several or many parameters.
  \item
    Priors are subjective, so that one researcher's prior might not work for another.
  \end{enumerate}
\item
  Computing the posterior. Especially for many-parameter problems and non-conjugate priors, computing the posterior can be nearly intractable.
\end{enumerate}

However, there are several practical solutions to these difficulties.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choosing a prior.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    We can use a ``uninformative'' or constant prior. Sometimes, we can use an improper prior that doesn't integrate to one, but places equal prior weight on all values.
  \item
    We can use an extremely diffuse prior. For example, if we wanted to estimate the average height in a population in inches, we might use a normal distribution centered at zero with an SD of 10,000. This prior says: ``The average height is about zero, give or take 10,000 inches or so.''
  \item
    We can use an informative prior, but conduct careful robustness checks to assess whether the conclusions depend on the particular prior.
  \item
    We can use a weakly informative prior, that rules places meaningful prior weight on all the plausible values and little prior weight only on the most implausible values. As a guideline, you might create a weakly informative prior by doubling or tripling the SD of the informative prior.
  \end{enumerate}
\item
  Computing the posterior.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    While analytically deriving the posterior becomes intractable for most applied problems, it's relatively easy to \emph{sample} from the posterior distribution for many models.
  \item
    Algorithms like Gibbs samplers, MCMC, and HMC make this sampling procedure straightforward for a given model.
  \item
    Software such as Stan make sampling easy to set up and very fast. Post-processing R packages such as tidybayes make it each to work with the posterior simulations.
  \end{enumerate}
\end{enumerate}

\hypertarget{week-3-adding-predictors}{%
\chapter{Week 3: Adding Predictors}\label{week-3-adding-predictors}}

So far, we have discussed several major ideas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Maximum likelihood} to obtain point estimates of model parameters and the \textbf{invariance property} to transform those estimates into quantities of interest. In this framework, we can use the \textbf{parametric bootstrap} to create confidence intervals and the \textbf{predictive distribution} to understand fitted models.
\item
  \textbf{Bayesian inference} to obtain posterior beliefs (i.e., distributions) of model parameters. In most applied cases, we will \textbf{simulate from the posterior}. We can \textbf{transform} those simulations to obtain posterior distributions of the quantities of interest. We can use the \textbf{posterior predictive distribution} to understand the fit.
\item
  We've discussed the tools above in the context of the \textbf{Bernoulli}, \textbf{Poisson}, and \textbf{exponential} models. Using the toothpaste cap, binary survey responses, civilian casualties, and government survival data.
\end{enumerate}

Today, we're going to focus on two narrow parts of \textbf{models} and explore how two current tools generalize to the regression context.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The linear predictor \(X\beta\).
\item
  The inverse link function.
\item
  How the {[}posterior{]} predictive distribution generalizes to regression.
\item
  How quantities of interest generalize to regression.
\end{enumerate}

For this week, we'll need the following packages:

\begin{itemize}
\tightlist
\item
  rstan/rstanarm
\item
  tidybayes
\item
  Zelig; \texttt{devtools::install\_github(\textquotesingle{}IQSS/Zelig\textquotesingle{})}
\end{itemize}

\hypertarget{review-the-normal-model}{%
\section{Review: The Normal Model}\label{review-the-normal-model}}

To fix ideas, we are going to re-develop the linear model from POS 5746.

We imagine a continuous outcome \(y = \{y_1, y_1,..., y_n\}\) and a set of predictors or ``explanatory variables'' \(x_1 = \{x_{11}, x_{21}, ..., x_{n1}\}, x_2 = \{x_{12}, x_{22}, ..., x_{n2}\}, ..., x_k = \{x_{1k}, x_{2k}, ..., x_{nk}\}\).

The notation \(y_i\) refers to the \(i\)th observation of the outcome variable.

The notation \(x_{ij}\) refers to the \(i\)th observation of the \(j\)th control variable.

The we write the linear regression model as

\[
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_k x_{ik} + r_i.
\]

We might then assume that the \(r_i\)s follow a normal distribution, so that \(r_i \sim N(0, \sigma^2)\) for all \(i\).

The we can define \(\mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_k x_{ik}\) and see that

\[
y_i \sim N(\mu_i, \sigma^2).
\]
Taking the expectation, we have \(E(y_i \mid x_1, x_2, ... , x_n) = \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_k x_{ik}\). This is just a \emph{conditional average} (the average of the outcome conditional on the explanatory variables). Sometimes we refer to this quantity as \(\hat{y}_i\).

There are two important features of this model that I want to explore: the \textbf{distribution} or ``stochastic component'' and the \textbf{linear predictor} of the model.

\hypertarget{distribution}{%
\subsection{Distribution}\label{distribution}}

This model uses the normal distribution to describe the unexplained variation in \(y_i - \hat{y}_i\). POS 5746 focuses (mostly) on models that assume a normal distribution for the outcome. King (1998) calls this the ``stochastic'' component of the model.

For now, simply note that we are not restricted to a normal model, we could easily adapt the model to use a Bernoulli, exponential, or Poisson distribution, for example.

\hypertarget{linear-predictor}{%
\subsection{Linear Predictor}\label{linear-predictor}}

The linear predictor \(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_k x_{ik}\) is \textbf{critically} important. So we should spend some time to get familiar with it.

For the sake of this exercise, the values of the \(\beta\)s and the \(x_{ij}\)s are arbitrary,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# devtools::install\_github(\textquotesingle{}IQSS/Zelig\textquotesingle{})}
\FunctionTok{data}\NormalTok{(macro, }\AttributeTok{package =} \StringTok{"Zelig"}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{small\_macro }\OtherTok{\textless{}{-}}\NormalTok{ macro }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(unem, gdp, capmob, trade) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{sample\_n}\NormalTok{(}\DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.fns =}\NormalTok{ signif, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)) }
\NormalTok{kableExtra}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(small\_macro, }\AttributeTok{format =} \StringTok{"markdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule
& unem & gdp & capmob & trade \\
\midrule
\endhead
284 & 1.5 & 5.2 & -1 & 88 \\
336 & 2.0 & 4.8 & -1 & 27 \\
101 & 2.7 & 3.2 & -2 & 74 \\
111 & 6.8 & 5.3 & 0 & 94 \\
133 & 2.6 & 5.4 & -1 & 33 \\
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(unem }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gdp }\SpecialCharTok{+}\NormalTok{ capmob }\SpecialCharTok{+}\NormalTok{ trade, }\AttributeTok{data =}\NormalTok{ small\_macro)}
\FunctionTok{signif}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept)         gdp      capmob       trade 
##     19.0000     -2.4000      4.5000      0.0027
\end{verbatim}

\textbf{In-Class Exercise} For the \(\beta\)s and the \(x_{ij}\)s above, compute each \(\mu_i\) and \(r_i = y_i - \mu_i\).

Now, let's bind the explanatory variables into a matrix, so that

\[
X = [x_1, x_2, ..., x_k] = \begin{bmatrix} 
    x_{11} & x_{12} &\dots  & x_{1k}\\
    x_{21} & x_{22} &\dots  & x_{2k}\\
    \vdots & \vdots &\ddots & \vdots\\
    x_{n1} & x_{n2} & \dots  & x_{nk} 
    \end{bmatrix}.
\]
And let's bind the \(\beta\)s into a column-vector, so that

\[
\beta = \begin{bmatrix} 
    \beta_{1} \\
    \beta_{2} \\
    \vdots\\
    \beta_{k}  
    \end{bmatrix}.
\]

\(\beta = [\beta_0, \beta_1, \beta_2, ..., \beta_k]\).

Notice that we have a \(n \times (k + 1)\) matrix \(X\) and a \((k + 1) x 1\) matrix \(\beta\). I content that the matrix multiplication \(\mu = X\beta\) is identical to \(\mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_k x_{ik}\).

\textbf{In-Class Exercise} For the \(\beta\)s and the \(x_{ij}\)s above, compute each \(\mu = X\beta\) and \(r = y - \mu\).

We can confirm with R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ small\_macro}\SpecialCharTok{$}\NormalTok{unem}

\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, }
\NormalTok{           small\_macro}\SpecialCharTok{$}\NormalTok{gdp, }
\NormalTok{           small\_macro}\SpecialCharTok{$}\NormalTok{capmob,}
\NormalTok{           small\_macro}\SpecialCharTok{$}\NormalTok{trade)}
\FunctionTok{print}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1  5.2   -1   88
## [2,]    1  4.8   -1   27
## [3,]    1  3.2   -2   74
## [4,]    1  5.3    0   94
## [5,]    1  5.4   -1   33
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{signif}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit), }\AttributeTok{digits =} \DecValTok{2}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\FunctionTok{print}\NormalTok{(beta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1]
## [1,] 19.0000
## [2,] -2.4000
## [3,]  4.5000
## [4,]  0.0027
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}}\NormalTok{ X}\SpecialCharTok{\%*\%}\NormalTok{beta; mu}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        [,1]
## [1,] 2.2576
## [2,] 3.0529
## [3,] 2.5198
## [4,] 6.5338
## [5,] 1.6291
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r }\OtherTok{\textless{}{-}}\NormalTok{ small\_macro}\SpecialCharTok{$}\NormalTok{unem }\SpecialCharTok{{-}}\NormalTok{ mu; r}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1]
## [1,] -0.7576
## [2,] -1.0529
## [3,]  0.1802
## [4,]  0.2662
## [5,]  0.9709
\end{verbatim}

From now on, we can just write\ldots{}

\begin{itemize}
\tightlist
\item
  \(X_i\beta\) rather than \(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_k x_{ik}\) (returns a \emph{scalar} \(\mu_i\))
\item
  \(X\beta\) rather than \(\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_k x_{k}\) (returns a \emph{vector} \(\mu\))
\end{itemize}

\hypertarget{fitting-the-normal-linear-model}{%
\subsection{Fitting the Normal-Linear Model}\label{fitting-the-normal-linear-model}}

\hypertarget{maximum-likelihood-1}{%
\subsubsection{Maximum Likelihood}\label{maximum-likelihood-1}}

It turns out that the usual least-squares solution from POS 5746 is the maximum likelihood estimate of \(\beta\). And the RMS of the residuals is the ML estimator of \(\sigma\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta\_hat }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X)}\SpecialCharTok{\%*\%}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{y}
\FunctionTok{print}\NormalTok{(beta\_hat, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         [,1]
## [1,] 18.8277
## [2,] -2.3766
## [3,]  4.5184
## [4,]  0.0027
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigma\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{((y }\SpecialCharTok{{-}}\NormalTok{ X}\SpecialCharTok{\%*\%}\NormalTok{beta\_hat)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\FunctionTok{print}\NormalTok{(sigma\_hat, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(unem }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gdp }\SpecialCharTok{+}\NormalTok{ capmob }\SpecialCharTok{+}\NormalTok{ trade, }\AttributeTok{data =}\NormalTok{ small\_macro)}
\NormalTok{arm}\SpecialCharTok{::}\FunctionTok{display}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## lm(formula = unem ~ gdp + capmob + trade, data = small_macro)
##             coef.est coef.se
## (Intercept) 18.83    11.64  
## gdp         -2.38     1.77  
## capmob       4.52     2.33  
## trade        0.00     0.03  
## ---
## n = 5, k = 4
## residual sd = 1.64, R-Squared = 0.85
\end{verbatim}

We can get confidence intervals with the parametric bootstrap.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get ml estimates}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(unem }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gdp }\SpecialCharTok{+}\NormalTok{ capmob }\SpecialCharTok{+}\NormalTok{ trade, }\AttributeTok{data =}\NormalTok{ small\_macro)}
\NormalTok{mu\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit)  }\CommentTok{\# same as X\%*\%beta\_hat}
\NormalTok{sigma\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(fit)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}

\CommentTok{\# do parametric bootstrap}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{bs\_est }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)), }\AttributeTok{nrow =}\NormalTok{ n\_bs)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_y }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(small\_macro), }\AttributeTok{mean =}\NormalTok{ mu\_hat, }\AttributeTok{sd =}\NormalTok{ sigma\_hat)}
\NormalTok{  bs\_fit }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, bs\_y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\NormalTok{  bs\_est[i, ] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(bs\_fit)}
\NormalTok{\}}

\CommentTok{\# compute the quantiles for each coef}
\FunctionTok{apply}\NormalTok{(bs\_est, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]       [,2]     [,3]        [,4]
## 5%   0.6518702 -4.9998081 1.690274 -0.04557350
## 95% 35.5920062  0.4865889 7.987606  0.06363354
\end{verbatim}

\hypertarget{bayesian}{%
\subsubsection{Bayesian}\label{bayesian}}

The \texttt{stan\_glm()} function allows us to easily get posterior simulations for the coefficients (and \(\sigma\)) for the normal linear model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm); }\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\NormalTok{stan\_fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(unem }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gdp }\SpecialCharTok{+}\NormalTok{ capmob }\SpecialCharTok{+}\NormalTok{ trade, }\AttributeTok{data =}\NormalTok{ small\_macro, }
                     \AttributeTok{family =} \StringTok{"gaussian"}\NormalTok{, }
                     \AttributeTok{chains =} \DecValTok{1}\NormalTok{, }
                     \AttributeTok{prior =} \ConstantTok{NULL}\NormalTok{,}
                     \AttributeTok{prior\_intercept =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(stan\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## stan_glm
##  family:       gaussian [identity]
##  formula:      unem ~ gdp + capmob + trade
##  observations: 5
##  predictors:   4
## ------
##             Median MAD_SD
## (Intercept) 19.3   11.7  
## gdp         -2.4    1.7  
## capmob       4.5    2.3  
## trade        0.0    0.0  
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 1.8    0.9   
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg
\end{verbatim}

\hypertarget{applied-example}{%
\subsection{Applied Example}\label{applied-example}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load data}
\NormalTok{cg }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/parties.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 555
## Columns: 10
## $ country              <chr> "Albania", "Albania", "Albania", "Argentina", "Ar~
## $ year                 <dbl> 1992, 1996, 1997, 1946, 1951, 1954, 1958, 1960, 1~
## $ average_magnitude    <dbl> 1.00, 1.00, 1.00, 10.53, 10.53, 4.56, 8.13, 4.17,~
## $ eneg                 <dbl> 1.106929, 1.106929, 1.106929, 1.342102, 1.342102,~
## $ enep                 <dbl> 2.190, 2.785, 2.870, 5.750, 1.970, 1.930, 2.885, ~
## $ upper_tier           <dbl> 28.57, 17.86, 25.80, 0.00, 0.00, 0.00, 0.00, 0.00~
## $ en_pres              <dbl> 0.00, 0.00, 0.00, 2.09, 1.96, 1.96, 2.65, 2.65, 3~
## $ proximity            <dbl> 0.00, 0.00, 0.00, 1.00, 1.00, 0.20, 1.00, 0.20, 1~
## $ social_heterogeneity <chr> "Bottom 3rd of ENEG", "Bottom 3rd of ENEG", "Bott~
## $ electoral_system     <chr> "Single-Member District", "Single-Member District~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fitting model with ls/ml}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ enep }\SpecialCharTok{\textasciitilde{}}\NormalTok{ eneg}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(average\_magnitude) }\SpecialCharTok{+}\NormalTok{ eneg}\SpecialCharTok{*}\NormalTok{upper\_tier }\SpecialCharTok{+}\NormalTok{ en\_pres}\SpecialCharTok{*}\NormalTok{proximity}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ cg)}
\NormalTok{arm}\SpecialCharTok{::}\FunctionTok{display}\NormalTok{(fit, }\AttributeTok{detail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## lm(formula = f, data = cg)
##                             coef.est coef.se t value Pr(>|t|)
## (Intercept)                  2.81     0.20   14.31    0.00   
## eneg                         0.19     0.08    2.47    0.01   
## log(average_magnitude)       0.33     0.11    2.88    0.00   
## upper_tier                   0.05     0.01    4.98    0.00   
## en_pres                      0.35     0.07    4.84    0.00   
## proximity                   -3.42     0.38   -8.98    0.00   
## eneg:log(average_magnitude)  0.08     0.06    1.28    0.20   
## eneg:upper_tier             -0.02     0.00   -3.37    0.00   
## en_pres:proximity            0.80     0.15    5.34    0.00   
## ---
## n = 555, k = 9
## residual sd = 1.59, R-Squared = 0.30
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fitting model with Stan}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ cg, }\AttributeTok{chains =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## stan_glm
##  family:       gaussian [identity]
##  formula:      enep ~ eneg * log(average_magnitude) + eneg * upper_tier + en_pres * 
##     proximity
##  observations: 555
##  predictors:   9
## ------
##                             Median MAD_SD
## (Intercept)                  2.8    0.2  
## eneg                         0.2    0.1  
## log(average_magnitude)       0.3    0.1  
## upper_tier                   0.0    0.0  
## en_pres                      0.3    0.1  
## proximity                   -3.4    0.4  
## eneg:log(average_magnitude)  0.1    0.1  
## eneg:upper_tier              0.0    0.0  
## en_pres:proximity            0.8    0.1  
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 1.6    0.0   
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg
\end{verbatim}

\hypertarget{bernoulli-model}{%
\section{Bernoulli Model}\label{bernoulli-model}}

In the case of the normal model, we used \(y_i \sim N(\mu_i, \sigma^2)\), where \(\mu_i = X_i\beta\). The normal model does a great job with roughly continuous outcomes like ENEP.

But sometimes we care about \textbf{binary outcomes}.

\begin{itemize}
\tightlist
\item
  Binary outcomes are categorical outcome variables with exactly two categories, such as whether or not someone voted, whether two countries are at war, and so on.
\item
  These variables are usually coded as \(y_i \in \{0, 1\}\), with one representing ``an event'' and zero representing ``a non-event.''
\item
  In generic language, we'll say that \(y_i = 1\) means that ``an event has occurred'' and \(y_i = 0\) means that ``an event has not occurred.''
\item
  This allows us to talk about the ``probability of an event'' (e.g., the probability of war, etc)
\end{itemize}

The normal model cannot describe a binary outcome well. But it doesn't make much conceptual sense to model 0s and 1s as following a normal distribution.

\hypertarget{the-linear-probability-model}{%
\subsection{The Linear Probability Model}\label{the-linear-probability-model}}

We can use the linear model (i.e., OLS) with binary outcome variables.

\begin{itemize}
\tightlist
\item
  Recall that we the linear model is represented by the equation \(E(y_i) = X_i\beta\).
\item
  It is important to note that a probability is just a particular kind of expected value---a probability is an expected value of a binary variable.
\item
  Since \(y_i\) is binary, the \(E(y_i) = \Pr(y_i = 1) = \Pr(y_i)\), giving us \(\Pr(y_i) = X_i\beta\).
\end{itemize}

The LPM has two advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It's is \emph{very} easy to estimate (i.e., OLS; \(\hat{\beta} = (X'X)^{-1}X'y\)).
\item
  It is easy to interpret (i.e., a one unit change in \(x_j\) leads to a \(\hat{\beta_j}\) unit increase in \(\Pr(y)\)).
\end{enumerate}

The LPM has several disadvantages

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Unbounded Predictions} Because the potential values for the explanatory variables are unbounded, you can obtain predicted probabilities above one and below zero. Of course, these predictions make no sense.
\item
  \textbf{Conditional Heteroskedasticity} The normal-linear model assumes a constant variance \(\sigma^2\). However, it is impossible to have homoskedastic residuals of a binary outcome if the probability of an event varies. Specifically, if \(y_i\) is binary, then \(\text{Var}(y_i) = \Pr(y_i)[1 - \Pr(y_i)]\), which, for the LPM, equals \(X_i\beta(1 - X_i\beta)\). (Non-zero coefficients imply heteroskedasticity.)
\item
  \textbf{Non-Normal Errors} Normal errors implies that the residuals can take on any value along the real line, with values closer to zero being more likely and errors outside three standard deviations being quite unlikely. However, if \(y_i\) is binary, then the residual can take on only two values: \(-Pr(y_i)\) or \(1 - Pr(y_i)\).
\item
  \textbf{Functional Form} Theoretically, you'd probably expect explanatory variables to have smaller effects as \(Pr(y_i)\) approaches zero or one (called ``compression''). The LPM assumes that the effects are constant.
\end{enumerate}

Let's fit the normal model to data from Wolfinger and Rosenstone (1993), Nagler (1994, the ``Scobit'' paper), and Berry, DeMeritt, and Esarey (2010).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scobit }\OtherTok{\textless{}{-}}\NormalTok{ haven}\SpecialCharTok{::}\FunctionTok{read\_dta}\NormalTok{(}\StringTok{"data/scobit.dta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(newvote }\SpecialCharTok{!=} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# weird {-}1s in data; unsure if sufficient}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 99,676
## Columns: 16
## $ state    <dbl> 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 9~
## $ vote     <dbl> 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2~
## $ age      <dbl> 60, 80, 32, 25, 55, 63, 20, 53, 49, 27, 58, 56, 34, 34, 35, 3~
## $ educ     <dbl> 13, 13, 13, 13, 11, 14, 11, 11, 13, 13, 11, 13, 19, 19, 15, 1~
## $ citizen  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ rweight  <dbl> 207134, 215836, 184639, 184883, 168557, 179148, 181510, 19285~
## $ south    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ gov      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ closing  <dbl> 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2~
## $ age2     <dbl> 3600, 6400, 1024, 625, 3025, 3969, 400, 2809, 2401, 729, 3364~
## $ educ2    <dbl> 25, 25, 25, 25, 16, 36, 16, 16, 25, 25, 16, 25, 64, 64, 36, 2~
## $ cloeduc  <dbl> 145, 145, 145, 145, 116, 174, 116, 116, 145, 145, 116, 145, 2~
## $ cloeduc2 <dbl> 725, 725, 725, 725, 464, 1044, 464, 464, 725, 725, 464, 725, ~
## $ newvote  <dbl> 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0~
## $ newage   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ neweduc  <dbl> 5, 5, 5, 5, 4, 6, 4, 4, 5, 5, 4, 5, 8, 8, 6, 5, 5, 3, 5, 1, 6~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ newvote }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(neweduc, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+} \FunctionTok{poly}\NormalTok{(age, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit)}

\NormalTok{mu\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit)  }\CommentTok{\# the linear predictor for each row of data frame}
\NormalTok{sigma\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(fit)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\NormalTok{y\_tilde }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(scobit), mu\_hat, sigma\_hat)}

\CommentTok{\# note: the code below uses variables NOT in the data frame; this is sloppy}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{gg1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(scobit, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mu\_hat, }\AttributeTok{y =}\NormalTok{ newvote)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{height =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{)}
\NormalTok{gg2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(scobit, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mu\_hat, }\AttributeTok{y =}\NormalTok{ y\_tilde)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{)}
\NormalTok{gg1 }\SpecialCharTok{+}\NormalTok{ gg2}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-02-bernoulli-model_files/figure-latex/unnamed-chunk-2-1.pdf}

rstanarm has a convenient \texttt{pp\_check()} function that allows you to compare the posterior predictive distribution to the observed distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm); }\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}

\NormalTok{stan\_fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \StringTok{"gaussian"}\NormalTok{, }\AttributeTok{chains =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pp\_check}\NormalTok{(stan\_fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-02-bernoulli-model_files/figure-latex/unnamed-chunk-4-1.pdf}

\hypertarget{the-logit-model}{%
\subsection{The Logit Model}\label{the-logit-model}}

As an initial effort to handle the ``non-normal'' distribution of the data, we might then use the Bernoulli model \(y_i \sim \text{Bernoulli}(\pi_i)\), where \(\pi_i = X_i\beta\). However, this has a \emph{big} problem that can make the approach unworkable: \(X_i\beta\) might be less than zero or greater than one.

To address bounds of \(\pi_i\) and \(X_i\beta\), we are going to introduce a new concept called the ``inverse link function.'' Many of the ``disadvantages'' of the LPM above follow from the fact that the linear predictor is unbounded. For the normal model, the inverse link function is \textbf{not necessary} because the parameter of interest \(\mu\) is unbounded and maps to the entire real line. But for other models, the key parameter has a restricted domain. In the case of the Bernoulli distribution, \(\pi_i \in [0, 1] \subset \mathbb{R}\).

The idea of the inverse link function is to wrap around the linear predictor and force its values into the desired domain.

For the Bernoulli distribution, we might use the inverse link function \(g^{-1}(x) = \frac{e^x}{1 + e^x}\). This is called the ``inverse logit'' and it has an ``S''-shape. It's job is to map \(X\beta\) into \([0, 1]\). (It's also the cdf of the standard logistic distribution.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inv\_logit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  (}\FunctionTok{exp}\NormalTok{(x))}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(x))}
\NormalTok{\}}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{stat\_function}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ inv\_logit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-02-bernoulli-model_files/figure-latex/unnamed-chunk-5-1.pdf}

Hint: The inverse-logit function is the cdf of the standard logistic distribution, so you can just use \texttt{plogis()} in R, rather than hard-coding the \texttt{inv\_logit()} function I create above.

Swapping the normal distribution for the Bernoulli and adding the inverse-logit inverse-link function gives us the logit model (or ``logistic regression'').

\[
y_i \sim \text{Bernoulli}(\pi_i)\text{, where } \pi_i = \text{logit}^{-1}(X_i\beta).
\]

We can fit this model using maximum likelihood or posterior simulation.

\hypertarget{fitting-a-logit-model}{%
\subsection{Fitting a Logit Model}\label{fitting-a-logit-model}}

\hypertarget{with-optim}{%
\subsubsection{\texorpdfstring{With \texttt{optim()}}{With optim()}}\label{with-optim}}

To develop the log-likelihood of the logit model, we start with the Bernoulli likelihood from Week 1.

\[
f(y; \beta) = L(\beta) = \prod_{i = 1}^{N}\pi_i^{y_i} (1 - \pi_i)^{(1 - y_i)}\text{, where } \pi_i = \text{logit}^{-1}(X_i\beta)
\]
Taking the log, we have

\[
\log L(\beta) = \sum_{i = 1}^{N} y_i \log \pi_i +  \sum_{i = 1}^{N}(1 - y_i) \log(1 - \pi_i)\text{, where } \pi_i = \text{logit}^{-1}(X_i\beta)
\]
We can program this into R for use in \texttt{optim()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logit\_ll }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, y, X) \{}
\NormalTok{  mu }\OtherTok{\textless{}{-}}\NormalTok{ X}\SpecialCharTok{\%*\%}\NormalTok{beta  }\CommentTok{\# pi is special in R, so I use p}
\NormalTok{  p }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(mu)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(mu))}
\NormalTok{  ll }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(y}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(p)) }\SpecialCharTok{+} \FunctionTok{sum}\NormalTok{((}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p))}
  \FunctionTok{return}\NormalTok{(ll)}
\NormalTok{\}}

\CommentTok{\# alternatively}
\NormalTok{logit\_ll2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, y, X) \{}
\NormalTok{  ll }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{dbinom}\NormalTok{(y, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(X}\SpecialCharTok{\%*\%}\NormalTok{beta), }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{))  }\CommentTok{\# easier to use R\textquotesingle{}s d*() functions}
  \FunctionTok{return}\NormalTok{(ll)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The tricky part about using \texttt{optim()} here is not the log-likelihood function, but setting up \texttt{X} and \texttt{y}. The code below creates the outcome vector \(y\) and the matrix \(X\) of explanatory variables (with a leading columns of 1s).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create formula}
\NormalTok{st }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{ arm}\SpecialCharTok{::}\FunctionTok{rescale}\NormalTok{(x) \}}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ newvote }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(}\FunctionTok{st}\NormalTok{(neweduc), }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{st}\NormalTok{(closing) }\SpecialCharTok{+} \FunctionTok{poly}\NormalTok{(}\FunctionTok{st}\NormalTok{(age), }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{st}\NormalTok{(south) }\SpecialCharTok{+} \FunctionTok{st}\NormalTok{(gov)}
\CommentTok{\#f \textless{}{-} newvote \textasciitilde{} poly(neweduc, 2, raw = TRUE) + closing + poly(age, 2, raw = TRUE) + south + gov}


\CommentTok{\# obtain the model matrix X}
\NormalTok{mf }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit)  }\CommentTok{\# model frame}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(f, mf)         }\CommentTok{\# model matrix X}

\CommentTok{\# obtain the outcome variable y}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{model.response}\NormalTok{(mf)}
\end{Highlighting}
\end{Shaded}

Then we can use \texttt{optim()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# for some reason, this isn\textquotesingle{}t converging}
\NormalTok{par\_start }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(X) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(par\_start, }\AttributeTok{fn =}\NormalTok{ logit\_ll, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{X =}\NormalTok{ X, }
             \AttributeTok{method =} \StringTok{"BFGS"}\NormalTok{,}
      \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{reltol =}\NormalTok{ .Machine}\SpecialCharTok{$}\NormalTok{double.eps))}
\NormalTok{opt}\SpecialCharTok{$}\NormalTok{par}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  1.055387404  1.574811426  0.239661630 -0.267674764  1.524032427
## [6] -1.073869727 -0.190402661  0.005271742
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \# test w/ same X and y; works}
\CommentTok{\# coef(glm.fit(X, y, family = binomial()))}
\CommentTok{\# }
\CommentTok{\# \# try optimx}
\CommentTok{\# library(optimx)}
\CommentTok{\# }
\CommentTok{\# optx \textless{}{-} optimx(par\_start, fn = logit\_ll, y = y, X = X, }
\CommentTok{\#              method = c("BFGS", "Nelder{-}Mead", "CG"),}
\CommentTok{\#       control = list(fnscale = {-}1, reltol = .Machine$double.eps, maxit = 1000))}
\CommentTok{\# optx}
\end{Highlighting}
\end{Shaded}

\hypertarget{with-glm}{%
\subsubsection{\texorpdfstring{With \texttt{glm()}}{With glm()}}\label{with-glm}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\FunctionTok{coef}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                       (Intercept) poly(st(neweduc), 2, raw = TRUE)1 
##                       1.055387325                       1.574811361 
## poly(st(neweduc), 2, raw = TRUE)2                       st(closing) 
##                       0.239661717                      -0.267674768 
##     poly(st(age), 2, raw = TRUE)1     poly(st(age), 2, raw = TRUE)2 
##                       1.524032313                      -1.073869592 
##                         st(south)                           st(gov) 
##                      -0.190402654                       0.005271703
\end{verbatim}

\hypertarget{with-stan}{%
\subsubsection{With Stan}\label{with-stan}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small\_scobit }\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(scobit, }\DecValTok{1000}\NormalTok{)}
\NormalTok{stan\_fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ small\_scobit, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(stan\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Model Info:
##  function:     stan_glm
##  family:       binomial [logit]
##  formula:      newvote ~ poly(neweduc, 2, raw = TRUE) + closing + poly(age, 
##     2, raw = TRUE) + south + gov
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help('prior_summary')
##  observations: 1000
##  predictors:   8
## 
## Estimates:
##                                 mean   sd   10%   50%   90%
## (Intercept)                   -4.4    0.8 -5.4  -4.3  -3.3 
## poly(neweduc, 2, raw = TRUE)1  0.4    0.3  0.1   0.4   0.7 
## poly(neweduc, 2, raw = TRUE)2  0.0    0.0  0.0   0.0   0.0 
## closing                        0.0    0.0  0.0   0.0   0.0 
## poly(age, 2, raw = TRUE)1      0.1    0.0  0.1   0.1   0.1 
## poly(age, 2, raw = TRUE)2      0.0    0.0  0.0   0.0   0.0 
## south                         -0.4    0.2 -0.6  -0.4  -0.1 
## gov                            0.2    0.2 -0.1   0.2   0.4 
## 
## Fit Diagnostics:
##            mean   sd   10%   50%   90%
## mean_PPD 0.7    0.0  0.7   0.7   0.7  
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).
## 
## MCMC diagnostics
##                               mcse Rhat n_eff
## (Intercept)                   0.0  1.0  2719 
## poly(neweduc, 2, raw = TRUE)1 0.0  1.0  2109 
## poly(neweduc, 2, raw = TRUE)2 0.0  1.0  2045 
## closing                       0.0  1.0  4060 
## poly(age, 2, raw = TRUE)1     0.0  1.0  2148 
## poly(age, 2, raw = TRUE)2     0.0  1.0  2190 
## south                         0.0  1.0  3620 
## gov                           0.0  1.0  3621 
## mean_PPD                      0.0  1.0  3924 
## log-posterior                 0.0  1.0  1825 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).
\end{verbatim}

\hypertarget{poisson-model}{%
\section{Poisson Model}\label{poisson-model}}

We've now got two models:

\[
y_i \sim N(\mu_i, \sigma^2)\text{, where } \mu_i = X_i\beta
\]

and
\[
y_i \sim \text{Bernoulli}(\pi_i)\text{, where } \pi_i = \text{logit}^{-1}(X_i\beta).
\]
We can extend this in many ways by modifying the distribution \(f\) and the inverse-link function \(g^{-1}\) appropriately.

\[
y_i \sim f(\theta_i)\text{, where } \theta_i = g^{-1}(X_i\beta).
\]

In the case of the normal model, \(f\) and \(g^{-1}\) are the normal distribution and the identity function. In the case of the logit model, they are the Bernoulli distribution and the inverse logit function.

To build a Poisson regression model, we can use the Poisson distribution for \(f\). We just need to identify an appropriate inverse-link function.

The Poisson distribution has a mean parameter \(\lambda\) that must be positive. Therefore, we need a function that maps the real line to the positive (or non-negative) reals. The exponential function \(g^{-1}(x) = e^x\) does this.

\[
y_i \sim \text{Poisson}(\lambda_i)\text{, where } \lambda_i = e^{X_i\beta}.
\]

We can program the log-likelihood function into R for use in \texttt{optim()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create log{-}likelihood}
\NormalTok{poisson\_ll }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, y, X) \{}
\NormalTok{  lambda }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(X}\SpecialCharTok{\%*\%}\NormalTok{beta)}
\NormalTok{  ll }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{dpois}\NormalTok{(y, }\AttributeTok{lambda =}\NormalTok{ lambda, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{))}
  \FunctionTok{return}\NormalTok{(ll)}
\NormalTok{\}}

\CommentTok{\# load hks data}
\NormalTok{hks }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/hks.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 3972 Columns: 10
## -- Column specification --------------------------------------------------------
## Delimiter: ","
## dbl (10): osvAll, troopLag, policeLag, militaryobserversLag, brv_AllLag, osv...
## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create X and y}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ osvAll }\SpecialCharTok{\textasciitilde{}}\NormalTok{ troopLag }\SpecialCharTok{+}\NormalTok{ policeLag }\SpecialCharTok{+}\NormalTok{ militaryobserversLag }\SpecialCharTok{+} 
\NormalTok{  brv\_AllLag }\SpecialCharTok{+}\NormalTok{ osvAllLagDum }\SpecialCharTok{+}\NormalTok{ incomp }\SpecialCharTok{+}\NormalTok{ epduration }\SpecialCharTok{+} 
\NormalTok{  lntpop}
\NormalTok{mf }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks)  }\CommentTok{\# model frame}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(f, mf)          }\CommentTok{\# model matrix X}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{model.response}\NormalTok{(mf)           }\CommentTok{\# outcome variable y}

\NormalTok{par\_start }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(X))}
\CommentTok{\# this poisson model is so bad that optim has a bit of trouble }
\CommentTok{\# intutitively, there\textquotesingle{}s a single outlier that basically makes}
\CommentTok{\# all poissons nearly impossible.}
\NormalTok{opt }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(par\_start, }\AttributeTok{fn =}\NormalTok{ poisson\_ll, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{X =}\NormalTok{ X, }
             \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{fnscale =} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{))}

\NormalTok{opt}\SpecialCharTok{$}\NormalTok{par}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2.032493946  0.056296852 -1.000415138 -0.629900495 -0.002084798
## [6] -0.015359471  1.949812320 -0.009831003  0.357133706
\end{verbatim}

Or we can use the \texttt{glm()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =}\NormalTok{ poisson)}
\FunctionTok{coef}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          (Intercept)             troopLag            policeLag 
##         -3.579287811         -0.169658063         -3.272474092 
## militaryobserversLag           brv_AllLag         osvAllLagDum 
##          8.099848984          0.000560565          0.291144441 
##               incomp           epduration               lntpop 
##          3.486201819         -0.022230231          0.189391395
\end{verbatim}

We could also use \texttt{stan\_glm()} to obtain simulations from the posterior distribution.

\hypertarget{predictive-distribution-1}{%
\subsection{Predictive Distribution}\label{predictive-distribution-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{observed\_data }\OtherTok{\textless{}{-}}\NormalTok{ hks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"observed"}\NormalTok{, }
         \AttributeTok{linpred\_hat =} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{))}

\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ observed\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{osvAll =} \FunctionTok{rpois}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(observed\_data), }
                          \AttributeTok{lambda =} \FunctionTok{exp}\NormalTok{(observed\_data}\SpecialCharTok{$}\NormalTok{linpred\_hat)),}
           \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 22,476
## Columns: 12
## $ osvAll               <dbl> 266, 292, 1046, 280, 435, 219, 203, 189, 190, 188~
## $ troopLag             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ policeLag            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ militaryobserversLag <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ brv_AllLag           <dbl> 0, 138, 2428, 30, 850, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ osvAllLagDum         <dbl> 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ incomp               <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2~
## $ epduration           <dbl> 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1~
## $ lntpop               <dbl> 10.88525, 10.88525, 10.88525, 10.88525, 10.88525,~
## $ conflict_id          <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~
## $ type                 <chr> "simulated #1", "simulated #1", "simulated #1", "~
## $ linpred_hat          <dbl> 5.701372, 5.756500, 7.017963, 5.651498, 6.088931,~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ linpred\_hat, }\AttributeTok{y =}\NormalTok{ osvAll }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-03-poisson-model_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ troopLag, }\AttributeTok{y =}\NormalTok{ osvAll }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-03-poisson-model_files/figure-latex/unnamed-chunk-4-2.pdf}

\hypertarget{posterior-predictive-distribution}{%
\subsection{Posterior Predictive Distribution}\label{posterior-predictive-distribution}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm); }\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: Rcpp
\end{verbatim}

\begin{verbatim}
## Warning: package 'Rcpp' was built under R version 4.1.2
\end{verbatim}

\begin{verbatim}
## This is rstanarm version 2.21.1
\end{verbatim}

\begin{verbatim}
## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!
\end{verbatim}

\begin{verbatim}
## - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.
\end{verbatim}

\begin{verbatim}
## - For execution on a local, multicore CPU with excess RAM we recommend calling
\end{verbatim}

\begin{verbatim}
##   options(mc.cores = parallel::detectCores())
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stan\_fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =} \StringTok{"poisson"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(tidybayes)}
\NormalTok{ppd }\OtherTok{\textless{}{-}}\NormalTok{ hks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_predicted\_draws}\NormalTok{(stan\_fit, }\AttributeTok{ndraws =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{.draw =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Draw \#"}\NormalTok{, .draw)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ .draw, }\AttributeTok{values\_from =}\NormalTok{ .prediction) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Observed}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ osvAll) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Draw \#1}\StringTok{\textasciigrave{}}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{Observed}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"osvAll2"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 33,714
## Columns: 15
## Groups: osvAll, troopLag, policeLag, militaryobserversLag, brv_AllLag, osvAllLagDum, incomp, epduration, lntpop, conflict_id, .row [3,746]
## $ osvAll               <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1~
## $ troopLag             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ policeLag            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ militaryobserversLag <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ brv_AllLag           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 138, 138, 138, 138, 13~
## $ osvAllLagDum         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ incomp               <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2~
## $ epduration           <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3~
## $ lntpop               <dbl> 10.88525, 10.88525, 10.88525, 10.88525, 10.88525,~
## $ conflict_id          <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~
## $ .row                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2~
## $ .chain               <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ .iteration           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ type                 <chr> "Draw #1", "Draw #2", "Draw #3", "Draw #4", "Draw~
## $ osvAll2              <dbl> 300, 312, 295, 310, 305, 294, 334, 294, 4, 315, 2~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ppd, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ troopLag, }\AttributeTok{y =}\NormalTok{ osvAll2 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-03-poisson-model_files/figure-latex/unnamed-chunk-5-1.pdf}

\hypertarget{posterior-predictive-distribution-1}{%
\section{{[}Posterior{]} Predictive Distribution}\label{posterior-predictive-distribution-1}}

As with simple models without covariates, we can use the predictive distribution and the posterior predictive distribution to understand models \emph{with} covariate. In fact, these tools become more valuable as the complexity of the model increases.

\hypertarget{for-the-logit-model}{%
\subsection{\ldots{} for the logit model}\label{for-the-logit-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scobit }\OtherTok{\textless{}{-}}\NormalTok{ haven}\SpecialCharTok{::}\FunctionTok{read\_dta}\NormalTok{(}\StringTok{"data/scobit.dta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(newvote }\SpecialCharTok{!=} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# weird {-}1s in data; unsure if sufficient}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 99,676
## Columns: 16
## $ state    <dbl> 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 9~
## $ vote     <dbl> 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2~
## $ age      <dbl> 60, 80, 32, 25, 55, 63, 20, 53, 49, 27, 58, 56, 34, 34, 35, 3~
## $ educ     <dbl> 13, 13, 13, 13, 11, 14, 11, 11, 13, 13, 11, 13, 19, 19, 15, 1~
## $ citizen  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ rweight  <dbl> 207134, 215836, 184639, 184883, 168557, 179148, 181510, 19285~
## $ south    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ gov      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ closing  <dbl> 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2~
## $ age2     <dbl> 3600, 6400, 1024, 625, 3025, 3969, 400, 2809, 2401, 729, 3364~
## $ educ2    <dbl> 25, 25, 25, 25, 16, 36, 16, 16, 25, 25, 16, 25, 64, 64, 36, 2~
## $ cloeduc  <dbl> 145, 145, 145, 145, 116, 174, 116, 116, 145, 145, 116, 145, 2~
## $ cloeduc2 <dbl> 725, 725, 725, 725, 464, 1044, 464, 464, 725, 725, 464, 725, ~
## $ newvote  <dbl> 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0~
## $ newage   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ neweduc  <dbl> 5, 5, 5, 5, 4, 6, 4, 4, 5, 5, 4, 5, 8, 8, 6, 5, 5, 3, 5, 1, 6~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ newvote }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(neweduc, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+} \FunctionTok{poly}\NormalTok{(age, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov}

\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}

\CommentTok{\# compute estimates of linear predictor and pi}
\NormalTok{linpred\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{)  }\CommentTok{\# on scale of linear predictor}
\NormalTok{pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)   }\CommentTok{\# on probability scale}

\CommentTok{\# put observed data into a data frame with linpred and pi ests}
\NormalTok{observed\_data }\OtherTok{\textless{}{-}}\NormalTok{ scobit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"observed"}\NormalTok{, }
         \AttributeTok{linpred\_hat =}\NormalTok{ linpred\_hat, }
         \AttributeTok{pi\_hat =}\NormalTok{ pi\_hat)}

\CommentTok{\# create data frames with simulated data from predictive distribution}
\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  y\_tilde }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(observed\_data), }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ pi\_hat)}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ observed\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{newvote =}\NormalTok{ y\_tilde, }
           \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}

\CommentTok{\# bind data together}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 598,056
## Columns: 19
## $ state       <dbl> 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93~
## $ vote        <dbl> 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2~
## $ age         <dbl> 60, 80, 32, 25, 55, 63, 20, 53, 49, 27, 58, 56, 34, 34, 35~
## $ educ        <dbl> 13, 13, 13, 13, 11, 14, 11, 11, 13, 13, 11, 13, 19, 19, 15~
## $ citizen     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ rweight     <dbl> 207134, 215836, 184639, 184883, 168557, 179148, 181510, 19~
## $ south       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ gov         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ closing     <dbl> 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29~
## $ age2        <dbl> 3600, 6400, 1024, 625, 3025, 3969, 400, 2809, 2401, 729, 3~
## $ educ2       <dbl> 25, 25, 25, 25, 16, 36, 16, 16, 25, 25, 16, 25, 64, 64, 36~
## $ cloeduc     <dbl> 145, 145, 145, 145, 116, 174, 116, 116, 145, 145, 116, 145~
## $ cloeduc2    <dbl> 725, 725, 725, 725, 464, 1044, 464, 464, 725, 725, 464, 72~
## $ newvote     <dbl> 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0~
## $ newage      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ neweduc     <dbl> 5, 5, 5, 5, 4, 6, 4, 4, 5, 5, 4, 5, 8, 8, 6, 5, 5, 3, 5, 1~
## $ type        <chr> "simulated #1", "simulated #1", "simulated #1", "simulated~
## $ linpred_hat <dbl> 1.35582513, 1.33800426, 0.27569097, -0.19579006, 0.7610264~
## $ pi_hat      <dbl> 0.7950803, 0.7921616, 0.5684895, 0.4512082, 0.6815766, 0.8~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot fake and obs data against linear predictor.}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ linpred\_hat, }\AttributeTok{y =}\NormalTok{ newvote)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{height =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-2-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot fake and obs data against age.}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ newvote)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{height =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-2-2.pdf}

This model, because we included a second-order polynomial for \texttt{age}, does a great job of picking up the nonlinear relationship between age and voting. If we replace the polynomial with a simple linear term, then the observed and predictive distributions show a stark dissimilarity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ newvote }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(neweduc, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}

\NormalTok{observed\_data }\OtherTok{\textless{}{-}}\NormalTok{ scobit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"observed"}\NormalTok{, }
         \AttributeTok{linpred\_hat =} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{))}

\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  y\_tilde }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(observed\_data), }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{plogis}\NormalTok{(observed\_data}\SpecialCharTok{$}\NormalTok{linpred\_hat))}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ observed\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{newvote =}\NormalTok{ y\_tilde, }
           \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 598,056
## Columns: 18
## $ state       <dbl> 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93~
## $ vote        <dbl> 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2~
## $ age         <dbl> 60, 80, 32, 25, 55, 63, 20, 53, 49, 27, 58, 56, 34, 34, 35~
## $ educ        <dbl> 13, 13, 13, 13, 11, 14, 11, 11, 13, 13, 11, 13, 19, 19, 15~
## $ citizen     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ rweight     <dbl> 207134, 215836, 184639, 184883, 168557, 179148, 181510, 19~
## $ south       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ gov         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ closing     <dbl> 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29~
## $ age2        <dbl> 3600, 6400, 1024, 625, 3025, 3969, 400, 2809, 2401, 729, 3~
## $ educ2       <dbl> 25, 25, 25, 25, 16, 36, 16, 16, 25, 25, 16, 25, 64, 64, 36~
## $ cloeduc     <dbl> 145, 145, 145, 145, 116, 174, 116, 116, 145, 145, 116, 145~
## $ cloeduc2    <dbl> 725, 725, 725, 725, 464, 1044, 464, 464, 725, 725, 464, 72~
## $ newvote     <dbl> 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1~
## $ newage      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ neweduc     <dbl> 5, 5, 5, 5, 4, 6, 4, 4, 5, 5, 4, 5, 8, 8, 6, 5, 5, 3, 5, 1~
## $ type        <chr> "simulated #1", "simulated #1", "simulated #1", "simulated~
## $ linpred_hat <dbl> 1.227725576, 1.973989420, 0.182956194, -0.078236152, 0.510~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ newvote)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{height =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.01}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-3-1.pdf}

We can do this same thing with Stan. However, working with the posterior simulations can be tricky. I use tidybayes \texttt{add\_predicted\_draws()} function along with some clever pivoting to get the data ready for \texttt{ggplot()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rstanarm); }\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: Rcpp
\end{verbatim}

\begin{verbatim}
## Warning: package 'Rcpp' was built under R version 4.1.2
\end{verbatim}

\begin{verbatim}
## This is rstanarm version 2.21.1
\end{verbatim}

\begin{verbatim}
## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!
\end{verbatim}

\begin{verbatim}
## - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.
\end{verbatim}

\begin{verbatim}
## - For execution on a local, multicore CPU with excess RAM we recommend calling
\end{verbatim}

\begin{verbatim}
##   options(mc.cores = parallel::detectCores())
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small\_scobit }\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(scobit, }\DecValTok{1000}\NormalTok{)  }\CommentTok{\# subsample b/c model is slow}
\NormalTok{stan\_fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ small\_scobit, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidybayes)}

\NormalTok{ppd }\OtherTok{\textless{}{-}}\NormalTok{ small\_scobit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_predicted\_draws}\NormalTok{(stan\_fit, }\AttributeTok{ndraws =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{.draw =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Draw \#"}\NormalTok{, .draw)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ .draw, }\AttributeTok{values\_from =}\NormalTok{ .prediction) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Observed}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ newvote) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Draw \#1}\StringTok{\textasciigrave{}}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{Observed}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"newvote2"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 9,000
## Columns: 21
## Groups: state, vote, age, educ, citizen, rweight, south, gov, closing, age2, educ2, cloeduc, cloeduc2, newvote, newage, neweduc, .row [1,000]
## $ state      <dbl> 74, 74, 74, 74, 74, 74, 74, 74, 74, 61, 61, 61, 61, 61, 61,~
## $ vote       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,~
## $ age        <dbl> 55, 55, 55, 55, 55, 55, 55, 55, 55, 25, 25, 25, 25, 25, 25,~
## $ educ       <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 10, 10, 10, 10, 10, 10,~
## $ citizen    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
## $ rweight    <dbl> 179915, 179915, 179915, 179915, 179915, 179915, 179915, 179~
## $ south      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ gov        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ closing    <dbl> 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,~
## $ age2       <dbl> 3025, 3025, 3025, 3025, 3025, 3025, 3025, 3025, 3025, 625, ~
## $ educ2      <dbl> 25, 25, 25, 25, 25, 25, 25, 25, 25, 16, 16, 16, 16, 16, 16,~
## $ cloeduc    <dbl> 150, 150, 150, 150, 150, 150, 150, 150, 150, 120, 120, 120,~
## $ cloeduc2   <dbl> 750, 750, 750, 750, 750, 750, 750, 750, 750, 480, 480, 480,~
## $ newvote    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,~
## $ newage     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ neweduc    <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7,~
## $ .row       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3,~
## $ .chain     <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
## $ .iteration <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
## $ type       <chr> "Draw #1", "Draw #2", "Draw #3", "Draw #4", "Draw #5", "Dra~
## $ newvote2   <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ppd, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ newvote2)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{height =} \FloatTok{0.15}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-5-1.pdf}

\hypertarget{for-the-poisson-model}{%
\subsection{\ldots{} for the Poisson model}\label{for-the-poisson-model}}

The code below repeated this exercise for the Poisson model using the HKS data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load hks data}
\NormalTok{hks }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/hks.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 3972 Columns: 10
## -- Column specification --------------------------------------------------------
## Delimiter: ","
## dbl (10): osvAll, troopLag, policeLag, militaryobserversLag, brv_AllLag, osv...
## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit poisson model}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ osvAll }\SpecialCharTok{\textasciitilde{}}\NormalTok{ troopLag }\SpecialCharTok{+}\NormalTok{ policeLag }\SpecialCharTok{+}\NormalTok{ militaryobserversLag }\SpecialCharTok{+} 
\NormalTok{  brv\_AllLag }\SpecialCharTok{+}\NormalTok{ osvAllLagDum }\SpecialCharTok{+}\NormalTok{ incomp }\SpecialCharTok{+}\NormalTok{ epduration }\SpecialCharTok{+} 
\NormalTok{  lntpop}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =}\NormalTok{ poisson)}

\CommentTok{\# simulate fake data from predictive distribution}
\NormalTok{observed\_data }\OtherTok{\textless{}{-}}\NormalTok{ hks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"observed"}\NormalTok{, }
         \AttributeTok{linpred\_hat =} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{))}
\NormalTok{sim\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
\NormalTok{  sim\_list[[i]] }\OtherTok{\textless{}{-}}\NormalTok{ observed\_data }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{osvAll =} \FunctionTok{rpois}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(observed\_data), }
                          \AttributeTok{lambda =} \FunctionTok{exp}\NormalTok{(observed\_data}\SpecialCharTok{$}\NormalTok{linpred\_hat)),}
           \AttributeTok{type =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"simulated \#"}\NormalTok{, i))}
\NormalTok{\}}
\NormalTok{gg\_data }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(sim\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(observed\_data) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 22,476
## Columns: 12
## $ osvAll               <dbl> 307, 301, 1075, 278, 422, 223, 195, 195, 187, 182~
## $ troopLag             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ policeLag            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ militaryobserversLag <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ brv_AllLag           <dbl> 0, 138, 2428, 30, 850, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
## $ osvAllLagDum         <dbl> 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ incomp               <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2~
## $ epduration           <dbl> 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1~
## $ lntpop               <dbl> 10.88525, 10.88525, 10.88525, 10.88525, 10.88525,~
## $ conflict_id          <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~
## $ type                 <chr> "simulated #1", "simulated #1", "simulated #1", "~
## $ linpred_hat          <dbl> 5.701372, 5.756500, 7.017963, 5.651498, 6.088931,~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot fake and observed data against linear predictor}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ linpred\_hat, }\AttributeTok{y =}\NormalTok{ osvAll }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot fake and observed data against number of troops}
\FunctionTok{ggplot}\NormalTok{(gg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ troopLag, }\AttributeTok{y =}\NormalTok{ osvAll }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-6-2.pdf}

And below is code to work with the posterior predictive distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stan\_fit }\OtherTok{\textless{}{-}} \FunctionTok{stan\_glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =} \StringTok{"poisson"}\NormalTok{, }\AttributeTok{chains =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppd }\OtherTok{\textless{}{-}}\NormalTok{ hks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_predicted\_draws}\NormalTok{(stan\_fit, }\AttributeTok{ndraws =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{.draw =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Draw \#"}\NormalTok{, .draw)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ .draw, }\AttributeTok{values\_from =}\NormalTok{ .prediction) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Observed}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ osvAll) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Draw \#1}\StringTok{\textasciigrave{}}\SpecialCharTok{:}\StringTok{\textasciigrave{}}\AttributeTok{Observed}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"osvAll2"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 33,714
## Columns: 15
## Groups: osvAll, troopLag, policeLag, militaryobserversLag, brv_AllLag, osvAllLagDum, incomp, epduration, lntpop, conflict_id, .row [3,746]
## $ osvAll               <dbl> 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1~
## $ troopLag             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ policeLag            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ militaryobserversLag <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ brv_AllLag           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 138, 138, 138, 138, 13~
## $ osvAllLagDum         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ incomp               <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2~
## $ epduration           <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3~
## $ lntpop               <dbl> 10.88525, 10.88525, 10.88525, 10.88525, 10.88525,~
## $ conflict_id          <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7~
## $ .row                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2~
## $ .chain               <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ .iteration           <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ type                 <chr> "Draw #1", "Draw #2", "Draw #3", "Draw #4", "Draw~
## $ osvAll2              <dbl> 283, 295, 292, 308, 344, 307, 305, 324, 4, 348, 3~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ppd, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ troopLag, }\AttributeTok{y =}\NormalTok{ osvAll2 }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{shape =} \DecValTok{21}\NormalTok{, }\AttributeTok{size =} \FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\FunctionTok{vars}\NormalTok{(type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{scale\_y\_log10}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'
\end{verbatim}

\includegraphics{03-04-predictive-distributions_files/figure-latex/unnamed-chunk-8-1.pdf}

\hypertarget{quantities-of-interest}{%
\section{Quantities of Interest}\label{quantities-of-interest}}

We've now got three models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the normal model
\item
  the logit model
\item
  the Poisson model
\end{enumerate}

In only the case of the normal distribution are the parameters directly interpretable. The meaning of the coefficients in the case of the logit and Poisson model is especially unclear.

Thus, we are interested not in the coefficients themselves, but in other ``quantities of interest.''

\hypertarget{expected-value}{%
\subsection{Expected Value}\label{expected-value}}

The first quantity of interest is the \textbf{expected value} \(E(y \mid X_s)\).

Imagine a particular scenario of interest \(X_s\). For the logit model, we can compute the expected value of \(y\) for that scenario using \(\hat{E}(y \mid X_p) = \hat{\pi}_s = \text{logit}^{-1}(X_s\hat{\beta})\). For the Poisson model, it's \(\hat{E}(y \mid X_p) = \hat{\lambda}_s = e^{X_s\hat{\beta}}\).

Let's see how this would work with the scobit data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scobit }\OtherTok{\textless{}{-}}\NormalTok{ haven}\SpecialCharTok{::}\FunctionTok{read\_dta}\NormalTok{(}\StringTok{"data/scobit.dta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(newvote }\SpecialCharTok{!=} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }

\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ newvote }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(neweduc, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+} \FunctionTok{poly}\NormalTok{(age, }\DecValTok{2}\NormalTok{, }\AttributeTok{raw =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

First, let's create the scenario of interest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create the scenario of interest X\_s (but a data frame)}

\NormalTok{scenario }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{neweduc),}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1
## Columns: 5
## $ neweduc <dbl> 5
## $ closing <dbl> 30
## $ age     <dbl> 40
## $ south   <dbl> 0
## $ gov     <dbl> 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# now use the predict() function to get pi\_hat}
\NormalTok{pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ scenario, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

So when all the \(x\)s are set to their median, the estimated probability of voting is \texttt{pi\_hat}.

We could all compute the expected value for all values of a single variable with other variables set to their median.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scenarios }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \FunctionTok{sort}\NormalTok{(}\FunctionTok{unique}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{neweduc)),}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 8
## Columns: 5
## $ neweduc <dbl> 1, 2, 3, 4, 5, 6, 7, 8
## $ closing <dbl> 30, 30, 30, 30, 30, 30, 30, 30
## $ age     <dbl> 40, 40, 40, 40, 40, 40, 40, 40
## $ south   <dbl> 0, 0, 0, 0, 0, 0, 0, 0
## $ gov     <dbl> 0, 0, 0, 0, 0, 0, 0, 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scenarios}\SpecialCharTok{$}\NormalTok{pi\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ scenarios, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(scenarios, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ neweduc, }\AttributeTok{y =}\NormalTok{ pi\_hat))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{03-05-first-difference_files/figure-latex/unnamed-chunk-4-1.pdf}

\hypertarget{first-difference}{%
\subsection{First Difference}\label{first-difference}}

Perhaps the most important quantity of interest, though, is the \textbf{first difference}. Imagine \emph{two} scenario of interest \(X_{lo}\) and \(X_{hi}\). We then compute the \emph{difference} \(\Delta\) between the expected values \(X_{lo}\) and \(X_{hi}\), so that \(\hat{\Delta} = \hat{E}(y \mid X_{hi}) - \hat{E}(y \mid X_{lo})\). This works for (almost?) all models we'll see in this course.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lo\_scenario }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \FunctionTok{quantile}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{neweduc, }\FloatTok{0.10}\NormalTok{), }\CommentTok{\# 25th percentile}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1
## Columns: 5
## $ neweduc <dbl> 3
## $ closing <dbl> 30
## $ age     <dbl> 40
## $ south   <dbl> 0
## $ gov     <dbl> 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hi\_scenario }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \FunctionTok{quantile}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{neweduc, }\FloatTok{0.90}\NormalTok{), }\CommentTok{\# 75th percentile}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1
## Columns: 5
## $ neweduc <dbl> 7
## $ closing <dbl> 30
## $ age     <dbl> 40
## $ south   <dbl> 0
## $ gov     <dbl> 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fd\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ hi\_scenario, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{{-}} 
  \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ lo\_scenario, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\FunctionTok{print}\NormalTok{(fd\_hat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1 
## 0.4240654
\end{verbatim}

This shows that if we move education from it minimum value (1) to its maximum value (8), the expected value goes up by 0.70. Since the expected value here is a probability, we can say that the chance of voting goes up by 42 percentage points (from 44\% to 87\%).

\hypertarget{week-4-confidence-intervals}{%
\chapter{Week 4: Confidence Intervals}\label{week-4-confidence-intervals}}

This week, we expand our confidence interval toolkit. We have three core methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  parametric bootstrap, which can be used directly for coefficients or quantities of interest.
\item
  nonparametric bootstrap, which can be used directly for coefficients or quantities of interest.
\item
  Wald confidence interval for coefficients, extended to quantities of interest using the delta method.
\end{enumerate}

\hypertarget{the-parametric-bootstrap-1}{%
\section{The Parametric Bootstrap}\label{the-parametric-bootstrap-1}}

We've already seen the parametric bootstrap, but a brief review is worthwhile.

To do compute a confidence interval using the parametric bootstrap, do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximate \(f(y; \theta)\) with \(\hat{f} = f(y; \hat{\theta})\). Simulate a new outcome \(y^{\text{bs}}\) from the estimated distribution.
\item
  Re-compute the estimate of interest \(\hat{\theta}^{\text{bs}}\) or \(\hat{\tau}^{\text{bs}}\) using the bootstrapped outcome variable \(y^{\text{bs}}\) rather than the observed outcome \(y\).
\item
  Repeat 1 and 2 many times (say 2,000) to obtain many bootstrapped estimates. To obtain the 95\% confidence interval, take the 2.5th and 97.5th percentiles of the estimates. In general, to obtain a \(100(1 - \alpha)\%\) confidence interval, \(\frac{\alpha}{2}\)th and \((1 - \frac{\alpha}{2})\)th percentiles. This is known as the percentile method.
\end{enumerate}

The parametric bootstrap is a powerful, general tool to obtain confidence intervals for estimates from parametric models, \emph{but it relies heavily on the parametric assumptions}.

\hypertarget{the-nonparametric-bootstrap}{%
\section{\texorpdfstring{The \emph{Non}parametric Bootstrap}{The Nonparametric Bootstrap}}\label{the-nonparametric-bootstrap}}

The nonparametric bootstrap works similarly to the parametric bootstrap. But rather than simulate a new outcome from the fitted parametric distribution, we sample (with replacement) a new data set from the observed data set (of the same size).

Suppose a data set \(D\) with \(N\) rows. To do compute a confidence interval using the \emph{non}parametric bootstrap, do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample \(N\) rows with replacement from \(D\) to create a single bootstrap data set \(D^{bs}\).
\item
  Re-compute the estimate of interest \(\hat{\theta}^{\text{bs}}\) or \(\hat{\tau}^{\text{bs}}\) using the bootstrap data set \(D^{bs}\) (rather than the observed data set \(D\)).
\item
  Repeat 1 and 2 many times (say 2,000) to obtain many bootstrap estimates. To obtain the 95\% confidence interval, take the 2.5th and 97.5th percentiles of the estimates. In general, to obtain a \(100(1 - \alpha)\%\) confidence interval, \(\frac{\alpha}{2}\)th and \((1 - \frac{\alpha}{2})\)th percentiles. This is known as the percentile method.
\end{enumerate}

\textbf{Important}: The parametric bootstrap obtains a single bootstrap data set by simulated a new outcome from the fitted parametric distribution. The \emph{non}parametric bootstrap creates a single bootstrap data set by sampling from the data set with replacement.

\hypertarget{example-coefficients-from-the-civilian-casualties-model}{%
\subsection{Example: Coefficients from the Civilian Casualties Model}\label{example-coefficients-from-the-civilian-casualties-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load hks data}
\NormalTok{hks }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/hks.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}

\CommentTok{\# fit poisson regression model}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ osvAll }\SpecialCharTok{\textasciitilde{}}\NormalTok{ troopLag }\SpecialCharTok{+}\NormalTok{ policeLag }\SpecialCharTok{+}\NormalTok{ militaryobserversLag }\SpecialCharTok{+} 
\NormalTok{  brv\_AllLag }\SpecialCharTok{+}\NormalTok{ osvAllLagDum }\SpecialCharTok{+}\NormalTok{ incomp }\SpecialCharTok{+}\NormalTok{ epduration }\SpecialCharTok{+} 
\NormalTok{  lntpop}

\CommentTok{\# fit poisson regression model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =}\NormalTok{ poisson)}
\NormalTok{texreg}\SpecialCharTok{::}\FunctionTok{screenreg}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## =====================================
##                       Model 1        
## -------------------------------------
## (Intercept)                 -3.58 ***
##                             (0.04)   
## troopLag                    -0.17 ***
##                             (0.00)   
## policeLag                   -3.27 ***
##                             (0.02)   
## militaryobserversLag         8.10 ***
##                             (0.01)   
## brv_AllLag                   0.00 ***
##                             (0.00)   
## osvAllLagDum                 0.29 ***
##                             (0.00)   
## incomp                       3.49 ***
##                             (0.02)   
## epduration                  -0.02 ***
##                             (0.00)   
## lntpop                       0.19 ***
##                             (0.00)   
## -------------------------------------
## AIC                    2139137.24    
## BIC                    2139193.29    
## Log Likelihood        -1069559.62    
## Deviance               2134800.81    
## Num. obs.                 3746       
## =====================================
## *** p < 0.001; ** p < 0.01; * p < 0.05
\end{verbatim}

First, for comparison, let's using the familiar parametric bootstrap.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# parametric bs for coefficients}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}
\NormalTok{coef\_bs }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =}\NormalTok{ n\_bs, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)))}
\FunctionTok{names}\NormalTok{(coef\_bs) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  lambda\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  y\_bs }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(}\FunctionTok{length}\NormalTok{(lambda\_hat), }\AttributeTok{lambda =}\NormalTok{ lambda\_hat)}
\NormalTok{  fit\_bs }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, }\AttributeTok{formula =}\NormalTok{ y\_bs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\NormalTok{  coef\_bs[i, ] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit\_bs)}
\NormalTok{\}}

\CommentTok{\# compute the 2.5th and 97.5th percentiles}
\NormalTok{cis }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(coef\_bs, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{)); cis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]       [,2]      [,3]     [,4]         [,5]      [,6]     [,7]
## 5%  -3.649697 -0.1726033 -3.313168 8.080218 0.0005465790 0.2835646 3.457448
## 95% -3.510606 -0.1668235 -3.234154 8.118577 0.0005749272 0.2988524 3.516634
##            [,8]      [,9]
## 5%  -0.02235827 0.1859763
## 95% -0.02209953 0.1925350
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# put the cis into a nice little data frame for use later}
\NormalTok{pbs\_ci\_df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{var =} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)),}
                \AttributeTok{est =} \FunctionTok{coef}\NormalTok{(fit),}
                \AttributeTok{lwr =}\NormalTok{ cis[}\StringTok{"5\%"}\NormalTok{, ],}
                \AttributeTok{upr =}\NormalTok{ cis[}\StringTok{"95\%"}\NormalTok{, ], }
                \AttributeTok{type =} \StringTok{"Parametric Bootstrap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now let's use the nonparametric bootstrap. The \emph{two} changed lines are flagged with comments

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nonparametric bs for coefficients}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}  
\NormalTok{coef\_bs }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =}\NormalTok{ n\_bs, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)))}
\FunctionTok{names}\NormalTok{(coef\_bs) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_data }\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(hks, }\AttributeTok{size =} \FunctionTok{nrow}\NormalTok{(hks), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# sample from hks w/ repl.}
\NormalTok{  fit\_bs }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, }\AttributeTok{data =}\NormalTok{ bs\_data)                       }\CommentTok{\# fit same model on resampled data}
\NormalTok{  coef\_bs[i, ] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit\_bs)}
\NormalTok{\}}
\NormalTok{cis }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(coef\_bs, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{)); cis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]        [,2]       [,3]       [,4]          [,5]      [,6]
## 5%  -14.096651 -0.38742086 -7.5353896 -0.1254471 -0.0004506079 -1.725904
## 95%   4.645469  0.04859871 -0.9892208 19.6680175  0.0023147196  2.579036
##         [,7]         [,8]       [,9]
## 5%  2.186409 -0.037873901 -0.5417483
## 95% 4.203617 -0.006679758  1.0661023
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nbs\_ci\_df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{var =} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)),}
                \AttributeTok{est =} \FunctionTok{coef}\NormalTok{(fit),}
                \AttributeTok{lwr =}\NormalTok{ cis[}\StringTok{"5\%"}\NormalTok{, ],}
                \AttributeTok{upr =}\NormalTok{ cis[}\StringTok{"95\%"}\NormalTok{, ], }
                \AttributeTok{type =} \StringTok{"Nonparametric Bootstrap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now let's compare the estimates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# combined the two dfs w/ the cis into a single df}
\NormalTok{ci\_df }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(pbs\_ci\_df, nbs\_ci\_df)}

\CommentTok{\# plots the coefficient estimates and cis}
\FunctionTok{ggplot}\NormalTok{(ci\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ est, }\AttributeTok{xmin =}\NormalTok{ lwr, }\AttributeTok{xmax =}\NormalTok{ upr,}
                  \AttributeTok{y =}\NormalTok{ var, }\AttributeTok{color =}\NormalTok{ type)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_errorbarh}\NormalTok{(}\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width =}\NormalTok{ .}\DecValTok{4}\NormalTok{), }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width =}\NormalTok{ .}\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{04-01-nonparametric-bootstrap_files/figure-latex/unnamed-chunk-5-1.pdf}

We could also make a little table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ci\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ci\_chr =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"["}\NormalTok{, scales}\SpecialCharTok{::}\FunctionTok{number}\NormalTok{(lwr, }\FloatTok{0.001}\NormalTok{), }\StringTok{", "}\NormalTok{, scales}\SpecialCharTok{::}\FunctionTok{number}\NormalTok{(upr, }\FloatTok{0.001}\NormalTok{), }\StringTok{"]"}\NormalTok{), }
         \AttributeTok{est\_chr =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{number}\NormalTok{(est, }\FloatTok{0.001}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(var, est\_chr, ci\_chr, type) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ type, }\AttributeTok{values\_from =}\NormalTok{ ci\_chr) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Variable}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ var, }\StringTok{\textasciigrave{}}\AttributeTok{Coefficient Estimate}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ est\_chr) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  kableExtra}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{format =} \StringTok{"markdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2759}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Coefficient Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Parametric Bootstrap
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Nonparametric Bootstrap
\end{minipage} \\
\midrule
\endhead
(Intercept) & -3.579 & {[}-3.650, -3.511{]} & {[}-14.097, 4.645{]} \\
troopLag & -0.170 & {[}-0.173, -0.167{]} & {[}-0.387, 0.049{]} \\
policeLag & -3.272 & {[}-3.313, -3.234{]} & {[}-7.535, -0.989{]} \\
militaryobserversLag & 8.100 & {[}8.080, 8.119{]} & {[}-0.125, 19.668{]} \\
brv\_AllLag & 0.001 & {[}0.001, 0.001{]} & {[}0.000, 0.002{]} \\
osvAllLagDum & 0.291 & {[}0.284, 0.299{]} & {[}-1.726, 2.579{]} \\
incomp & 3.486 & {[}3.457, 3.517{]} & {[}2.186, 4.204{]} \\
epduration & -0.022 & {[}-0.022, -0.022{]} & {[}-0.038, -0.007{]} \\
lntpop & 0.189 & {[}0.186, 0.193{]} & {[}-0.542, 1.066{]} \\
\bottomrule
\end{longtable}

\hypertarget{example-first-difference-from-the-civilian-casualties-model}{%
\subsection{Example: First Difference from the Civilian Casualties Model}\label{example-first-difference-from-the-civilian-casualties-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load hks data}
\NormalTok{hks }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/hks.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}

\CommentTok{\# fit poisson regression model}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ osvAll }\SpecialCharTok{\textasciitilde{}}\NormalTok{ troopLag }\SpecialCharTok{+}\NormalTok{ policeLag }\SpecialCharTok{+}\NormalTok{ militaryobserversLag }\SpecialCharTok{+} 
\NormalTok{  brv\_AllLag }\SpecialCharTok{+}\NormalTok{ osvAllLagDum }\SpecialCharTok{+}\NormalTok{ incomp }\SpecialCharTok{+}\NormalTok{ epduration }\SpecialCharTok{+} 
\NormalTok{  lntpop}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =}\NormalTok{ poisson)}

\CommentTok{\# compute qi using the invariance property}
\NormalTok{X\_lo }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{troopLag =} \DecValTok{0}\NormalTok{,}
               \AttributeTok{policeLag =} \DecValTok{0}\NormalTok{, }
               \AttributeTok{militaryobserversLag =} \DecValTok{0}\NormalTok{, }
               \AttributeTok{brv\_AllLag =} \DecValTok{0}\NormalTok{,}
               \AttributeTok{osvAllLagDum =} \DecValTok{0}\NormalTok{, }
               \AttributeTok{incomp =} \DecValTok{2}\NormalTok{,}
               \AttributeTok{epduration =} \DecValTok{46}\NormalTok{,}
               \AttributeTok{lntpop =} \FloatTok{9.19}\NormalTok{)}
\NormalTok{lambda\_hat\_lo }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ X\_lo, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{X\_hi }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(X\_lo, }\AttributeTok{troopLag =} \FloatTok{29.209}\NormalTok{)}
\NormalTok{lambda\_hat\_hi }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ X\_hi, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{fd\_hat }\OtherTok{\textless{}{-}}\NormalTok{ lambda\_hat\_hi }\SpecialCharTok{{-}}\NormalTok{ lambda\_hat\_lo; fd\_hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1 
## -60.57989
\end{verbatim}

Now let's get a 90\% confidence interval for the first difference using the \textbf{parametric bootstrap}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# parametric bs for coefficients}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{fd\_bs }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_bs)}
\FunctionTok{names}\NormalTok{(coef\_bs) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  lambda\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  y\_bs }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(}\FunctionTok{length}\NormalTok{(lambda\_hat),  }\AttributeTok{lambda =}\NormalTok{ lambda\_hat)}
\NormalTok{  fit\_bs }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, }\AttributeTok{formula =}\NormalTok{ y\_bs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\NormalTok{  lh\_lo\_bs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit\_bs, }\AttributeTok{newdata =}\NormalTok{ X\_lo, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  lh\_hi\_bs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit\_bs, }\AttributeTok{newdata =}\NormalTok{ X\_hi, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  fd\_bs[i] }\OtherTok{\textless{}{-}}\NormalTok{ lh\_hi\_bs }\SpecialCharTok{{-}}\NormalTok{ lh\_lo\_bs}
\NormalTok{\}}
\FunctionTok{quantile}\NormalTok{(fd\_bs, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        5%       95% 
## -60.90088 -60.21937
\end{verbatim}

Now let's get a 90\% confidence interval for the first difference using the \textbf{nonparametric bootstrap}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# nonparametric bs for coefficients}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{fd\_bs }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_bs)}
\FunctionTok{names}\NormalTok{(coef\_bs) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  bs\_data }\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(hks, }\AttributeTok{size =} \FunctionTok{nrow}\NormalTok{(hks), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\# sample from hks w/ repl.}
\NormalTok{  fit\_bs }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, }\AttributeTok{data =}\NormalTok{ bs\_data)                       }\CommentTok{\# fit same model on resampled data}
\NormalTok{  lh\_lo\_bs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit\_bs, }\AttributeTok{newdata =}\NormalTok{ X\_lo, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  lh\_hi\_bs }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit\_bs, }\AttributeTok{newdata =}\NormalTok{ X\_hi, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  fd\_bs[i] }\OtherTok{\textless{}{-}}\NormalTok{ lh\_hi\_bs }\SpecialCharTok{{-}}\NormalTok{ lh\_lo\_bs}
\NormalTok{\}}
\FunctionTok{quantile}\NormalTok{(fd\_bs, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        5%       95% 
## -90.96778  39.29382
\end{verbatim}

\hypertarget{wald-confidence-intervals}{%
\section{Wald Confidence Intervals}\label{wald-confidence-intervals}}

We can easily use the log-likelihood function to obtain point estimates. It turns out, though, that this same log-likelihood function contains information that helps use estimate the \emph{precision} of those estimates as well.

As an example, consider the following two log-likelihood functions:

\includegraphics{04-02-wald-cis_files/figure-latex/unnamed-chunk-2-1.pdf}
Which of these two log-likelihood functions do you think provides a more \emph{precise} estimate?

\emph{Note: These likelihoods are from a normal model with unknown mean. I simulated 100 observations for \(y_1\) and 500 observations for \(y_2\). (I centered the data so the sample means both occurred exactly at two.}

\textbf{Key Idea}: We can use the curvature around the maximum likelihood estimate to get a sense of the uncertainty.

What quantity tells us about the amount of curvature at the maximum? The second derivative. As the second derivative goes down, the curvature goes up.
As the curvature goes up, the uncertainty goes down.

\hypertarget{curvature-in-a-single-dimmension}{%
\subsection{Curvature in a Single Dimmension}\label{curvature-in-a-single-dimmension}}

To develop our intuition about ``curvature'' and confidence intervals, I analyze the \emph{Stylized Normal Model} (\(\sigma = 1\)). Here, we model the data as a normal distribution with \(\mu\) unknown (and to be estimated), but \(\sigma = 1\) (known; not estimated). That is, \(y \sim N(\mu, 1)\).

\[
\begin{aligned}
\log \mathcal{L}(\mu) &= -\frac{n}{2\pi} - \frac{1}{2}\sum_{i = 1}^n (y_i - \mu)^2\\
\dfrac{\partial \log \mathcal{L}(\mu)}{\partial \mu} &= \sum_{i = 1}^n y_i - \mu n\\
\dfrac{\partial^2 \log \mathcal{L}(\mu)}{\partial^2 \mu} &=  - n
\end{aligned}
\]

Facts:

\begin{itemize}
\tightlist
\item
  As \(n\) increases, \(\frac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu}\) becomes more negative.
\item
  As \(\frac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu}\) gets more negative, the curvature increases.
\item
  As the curvature increases, the uncertainty decreases.
\end{itemize}

Wouldn't it be really nice if we could use \(\frac{\partial^2 \log \mathcal{L}(\mu)}{\partial^2 \mu}\) to estimate the standard error?

It turns out that this quantity is a direct, almost magically intuitive estimator of the standard error.

In the single parameter case, we have the following approximation.

\[
\widehat{\text{Var}}(\hat{\theta}) \approx \left[\left. - \frac{\partial^2 \log \mathcal{L}(\theta)}{\partial^2 \theta}\right| _{\theta = \hat{\theta}}\right] ^{-1}
\]

I should be careful here. This is an asymptotic result. As the sample size grows, the variance of \(\hat{\beta}\) gets closer and closer to \(\left[\left. - \frac{\partial^2 \log \mathcal{L}(\theta)}{\partial^2 \theta}\right| _{\theta = \hat{\theta}}\right] ^{-1}\). I'm interpreting this large sample result as a small sample approximation.

This mean that we find the second derivative, evaluate it at the maximum (\(\theta = \hat{\theta}\)), and find the inverse (\(-1\)). That's an estimate of the variance. To convert it to a standard error, just take the square root.

\[
\widehat{\text{SE}}(\hat{\theta}) \approx \sqrt{\left[\left. - \frac{\partial^2 \log \mathcal{L}(\theta)}{\partial^2 \theta}\right| _{\theta = \hat{\theta}}\right] ^{-1}}
\]
If we continue the stylized normal example, we have the following.

\[
\begin{equation*}
\dfrac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu} =  - n
 ~{\color{purple}{\Longrightarrow}}~
\left[\left. - \frac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu}\right| _{\mu = \hat{\mu}}\right] ^{-1} 
 = \dfrac{1}{n} 
\approx \widehat{\text{Var}}(\hat{\mu})
\end{equation*}
\]

And then

\[
\begin{equation*}
\widehat{\text{SE}}(\hat{\mu}) \approx \sqrt{\dfrac{1}{n}} 
\end{equation*}
\]
Does this answer make sense? What is the standard error of the mean from Methods II? Hint: It's \(\text{SE}[\text{avg}(y)] \approx \sqrt{\dfrac{\text{population SD}}{n}}\). In this case, the ``population SD'' is \(\sigma = 1\), as assumed by the \emph{stylized} normal model.

\hypertarget{curvature-in-a-multiple-dimmensions}{%
\subsection{Curvature in a Multiple Dimmensions}\label{curvature-in-a-multiple-dimmensions}}

To add multiple dimensions, let's consider the beta model from Week 1. Here, we assume that \(y \sim \text{Beta}(\alpha, \beta)\), and our goal is to estimate \(\alpha\) and \(\beta\). The key is that we have \emph{multiple} (i.e., two) parameters to estimate.

It's a bit trickier to think about curvature in multiple dimensions.

Here's what the log-likelihood function might look like for a give data set.

\includegraphics{04-02-wald-cis_files/figure-latex/unnamed-chunk-3-1.pdf}

To make more sense, of this 3D plot, let's look at the contour plot.

\includegraphics{04-02-wald-cis_files/figure-latex/unnamed-chunk-4-1.pdf}

The curvature around the maximum \emph{vertically} tells use the variance in \(\hat{\beta}\).

\includegraphics{04-02-wald-cis_files/figure-latex/unnamed-chunk-5-1.pdf}

The curvature around the maximum \emph{horizontally} tells use the variance in \(\hat{\alpha}\).

\includegraphics{04-02-wald-cis_files/figure-latex/unnamed-chunk-6-1.pdf}

But there's a third direction that's relevant here: the curvature \emph{diagonally}. The diagonal curvature tells us the covariance of \(\hat{\alpha}\) and \(\hat{\beta}\). That is, if we \emph{over}-estimate \(\alpha\), how much do we tend to over- (or under-)estimate \(\beta\)?

\includegraphics{04-02-wald-cis_files/figure-latex/unnamed-chunk-7-1.pdf}

Rather than a single variance, we get a variance \textbf{matrix} (sometimes called the ``covariance matrix'' or the ``variance-covariance matrix''.

\[
\begin{equation*}
\widehat{\text{Var}}(\hat{\theta})= \widehat{\text{Cov}}(\hat{\theta}) \approx \left. \left[ 
\displaystyle \begin{matrix}
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1} & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2}\\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1} & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}
\end{equation*}
\]
The elements along the diagonal (in red) are the variances for each parameter, so the square root of the diagonal gives you the standard errors. This is exactly what we'd expect.

\[
\begin{equation*}
\widehat{\text{Var}}(\hat{\theta}) \approx \left. \left[ 
\displaystyle \begin{matrix}
\color{red}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1}} & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2}\\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1} & \color{red}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2}}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}
\end{equation*}
\]

The off-diagonal elements (in blue) are the covariances--they'll be really important to us later, but we don't have a direct use for them at the moment.

\[
\begin{equation*}
\widehat{\text{Var}}(\hat{\theta}) \approx \left. \left[ 
\displaystyle \begin{matrix}
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1} & \color{blue}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2}}\\
\color{blue}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1}} & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}
\end{equation*}
\]

But what about more than two parameters? It's exactly what you'd expect. We call this matrix of second-derivatives the ``information matrix'' \(\mathcal{I}(\theta)\). When evaluated at the ML estimate, we call it the ``observed information matrix'' \(\mathcal{I}(\hat{\theta})\).

\[
\begin{equation*}
\begin{aligned}
\widehat{\text{Var}}(\hat{\theta}) &\approx \left. \left[ 
\displaystyle \begin{matrix}
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1} & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2} & \ldots &- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_k}\\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1} & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2} & \ldots & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_k}\\
\vdots & \vdots & \ddots & \vdots \\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_k \partial \theta_1}     & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_k \partial \theta_2} & \ldots & - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_k}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}\\
 & \approx \mathcal{I}(\theta)^{-1}|_{\theta = \hat{\theta}}\\
 &\approx \mathcal{I}(\hat{\theta})^{-1}
\end{aligned}
\end{equation*}
\]

\hypertarget{from-curvature-to-confidence-intervals}{%
\subsection{From Curvature to Confidence Intervals}\label{from-curvature-to-confidence-intervals}}

To convert this variance estimate into a confidence interval, we need the following large sample result. It turns out that, as the sample size grows large, the ML estimate converges to a normally distributed random variable with mean \(theta_{true}\) and variance \(\mathcal{I}(\theta_{true})^{-1}\)..

\textbf{key fact}: \(\hat{\theta} \overset{a}{\sim} N\left[ \theta_{true}, \mathcal{I}(\theta_{true})^{-1}\right]\)

\pause In practice, we'll take this to mean it's \emph{approximately} normal.

\[
\begin{align*}
 90\%~\text{C.I.}  &\approx \hat{\theta} \pm 1.64\dfrac{1}{\sqrt{\mathcal{I}(\hat{\theta})}}\\
 95\%~\text{C.I.}  &\approx \hat{\theta} \pm 1.96\dfrac{1}{\sqrt{\mathcal{I}(\hat{\theta})}}
\end{align*}
\]

To work with these intervals, then, we just need the variance matrix \(\widehat{\text{Var}}(\hat{\theta}) = \mathcal{I}(\hat{\theta})^{-1}\). Much like we can access the ML estimates of the model coefficients \(\hat{\beta}\) with \texttt{coef()}, we can access \(\widehat{\text{Var}}(\hat{\theta})\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load hks data}
\NormalTok{hks }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/hks.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}

\CommentTok{\# fit poisson regression model}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ osvAll }\SpecialCharTok{\textasciitilde{}}\NormalTok{ troopLag }\SpecialCharTok{+}\NormalTok{ policeLag }\SpecialCharTok{+}\NormalTok{ militaryobserversLag }\SpecialCharTok{+} 
\NormalTok{  brv\_AllLag }\SpecialCharTok{+}\NormalTok{ osvAllLagDum }\SpecialCharTok{+}\NormalTok{ incomp }\SpecialCharTok{+}\NormalTok{ epduration }\SpecialCharTok{+} 
\NormalTok{  lntpop}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ hks, }\AttributeTok{family =}\NormalTok{ poisson)}

\CommentTok{\# compute 90\% confidence intervals}
\NormalTok{beta\_hat }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit)}
\NormalTok{var\_hat }\OtherTok{\textless{}{-}} \FunctionTok{vcov}\NormalTok{(fit)}
\NormalTok{se\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{diag}\NormalTok{(var\_hat))  }\CommentTok{\# keep only the diagonal elements}
\NormalTok{ci\_lwr }\OtherTok{\textless{}{-}}\NormalTok{ beta\_hat }\SpecialCharTok{{-}} \FloatTok{1.64}\SpecialCharTok{*}\NormalTok{se\_hat}
\NormalTok{ci\_upr }\OtherTok{\textless{}{-}}\NormalTok{ beta\_hat }\SpecialCharTok{+} \FloatTok{1.64}\SpecialCharTok{*}\NormalTok{se\_hat}

\CommentTok{\# make a nice table}
\FunctionTok{tibble}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Variable}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{names}\NormalTok{(beta\_hat),}
       \StringTok{\textasciigrave{}}\AttributeTok{Coefficient Estimate}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{number}\NormalTok{(beta\_hat, }\FloatTok{0.001}\NormalTok{),}
       \StringTok{\textasciigrave{}}\AttributeTok{90\% Confidence Interval}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\StringTok{"["}\NormalTok{, scales}\SpecialCharTok{::}\FunctionTok{number}\NormalTok{(ci\_lwr, }\FloatTok{0.001}\NormalTok{), }\StringTok{","}\NormalTok{, scales}\SpecialCharTok{::}\FunctionTok{number}\NormalTok{(ci\_upr, }\FloatTok{0.001}\NormalTok{), }\StringTok{"]"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  kableExtra}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\AttributeTok{format =} \StringTok{"markdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lll@{}}
\toprule
Variable & Coefficient Estimate & 90\% Confidence Interval \\
\midrule
\endhead
(Intercept) & -3.579 & {[}-3.649,-3.509{]} \\
troopLag & -0.170 & {[}-0.173,-0.167{]} \\
policeLag & -3.272 & {[}-3.313,-3.232{]} \\
militaryobserversLag & 8.100 & {[}8.080,8.120{]} \\
brv\_AllLag & 0.001 & {[}0.001,0.001{]} \\
osvAllLagDum & 0.291 & {[}0.283,0.299{]} \\
incomp & 3.486 & {[}3.457,3.516{]} \\
epduration & -0.022 & {[}-0.022,-0.022{]} \\
lntpop & 0.189 & {[}0.186,0.193{]} \\
\bottomrule
\end{longtable}

Compare these confidence intervals to the parametric and nonparametric intervals from the previous section.

\hypertarget{final-notes}{%
\subsection{Final Notes}\label{final-notes}}

\begin{itemize}
\tightlist
\item
  The Wald confidence interval does not easily extend to quantities of interest. It turns out that their is a way, called the ``delta method.'' It's a bit tedious and not necessary since we have other methods. But I'll mention it here: ``the delta method.'' King, Tomz, and Wittenberg (2001) give us an easy alternative to the delta method; we'll see their method in a couple of weeks.
\item
  If you use \texttt{optim()} to find the ML estimates, then you can have it return the observed information matrix \(\mathcal{I}(\hat{\theta})\) to you by supplying the argument \texttt{hession\ =\ TRUE} to \texttt{optim()}. \texttt{optim()} returns a list; the component named \texttt{"hessian"} is the Hessian matrix. You simple need to find the inverse of the negative of the Hessian to obtain the estimated variance matrix. Something like \texttt{est\ \textless{}-\ optim(...,\ hessian\ =\ TRUE)} followed by \texttt{var\_hat\ \textless{}-\ solve(-est\$hessian)}.
\end{itemize}

\hypertarget{evaluating-confidence-intervals}{%
\section{Evaluating Confidence Intervals}\label{evaluating-confidence-intervals}}

\hypertarget{coverage}{%
\subsection{Coverage}\label{coverage}}

Before we discuss these three intervals, let's review how we \textbf{evaluate} intervals. How do we know if a particular method works well?

We evaluate confidence intervals in terms of their coverage: a \(100(1 - \alpha)\%\) confidence interval, should capture the parameter \(100(1 - \alpha)\%\) of the time under repeated sampling. That is, if we imagine repeating the study over-and-over (in the usual frequentist sense), then \(100(1 - \alpha)\%\) of the confidence intervals contain the true parameter.

\hypertarget{monte-carlo-simulation-to-assess-coverage}{%
\subsection{Monte Carlo Simulation to Assess Coverage}\label{monte-carlo-simulation-to-assess-coverage}}

\hypertarget{a-simple-example}{%
\subsubsection{A Simple Example}\label{a-simple-example}}

As an example, let's consider the usual 90\% confidence interval for the mean: 95\% CI = \([\text{avg}(y) - 1.64 \times \hat{\text{SE}}, \text{avg}(y) + 1.64 \times \hat{\text{SE}}]\), where \(\hat{\text{SE}} = \frac{\text{SD}(y)}{\sqrt{n}}\). We learned in an earlier class that this interval should capture the population average in about 90\% of repeated trials. For out purposes, the ``population average'' refers to the mean parameter of some probability distribution.

Let's let the unknown distribution be \(Y \sim \text{Poisson}(\lambda = 10)\). The ``population'' mean hear is \(E(Y) = \lambda = 10\). Now let's use a Monte Carlo simulation to evaluate this particular interval. For this study, let's use a small sample size of 15 observations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of MC simulations (i.e., repeated trials)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}

\CommentTok{\# contains for lower and upper bounds of 90\% cis}
\NormalTok{lwr }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
\NormalTok{upr }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}

\CommentTok{\# mc simulations}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(}\DecValTok{15}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{10}\NormalTok{)}
\NormalTok{  se\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(y)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{length}\NormalTok{(y))}
\NormalTok{  lwr[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y) }\SpecialCharTok{{-}} \FloatTok{1.64}\SpecialCharTok{*}\NormalTok{se\_hat}
\NormalTok{  upr[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y) }\SpecialCharTok{+} \FloatTok{1.64}\SpecialCharTok{*}\NormalTok{se\_hat}
\NormalTok{\}}

\CommentTok{\# combine results into a data frame}
\NormalTok{mc\_sims }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{iteration =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims,}
\NormalTok{                  lwr, upr) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{captured =}\NormalTok{ lwr }\SpecialCharTok{\textless{}} \DecValTok{10} \SpecialCharTok{\&}\NormalTok{ upr }\SpecialCharTok{\textgreater{}} \DecValTok{10}\NormalTok{)}

\CommentTok{\# compute the proportion of simulations that capture the parameter}
\FunctionTok{mean}\NormalTok{(mc\_sims}\SpecialCharTok{$}\NormalTok{captured)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8797
\end{verbatim}

This simulation demonstrates that this simple interval captures the parameter \(\lambda = 10\) in about 90\% of repeated samples. This interval is \emph{slightly} too narrow, because we should really use the \(t\)-interval here due to the small sample size.

The simulation below shows that this interval has better coverage (i.e., closer to 90\%).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of MC simulations (i.e., repeated trials)}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}

\CommentTok{\# contains for lower and upper bounds of 90\% cis}
\NormalTok{lwr }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}
\NormalTok{upr }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(n\_mc\_sims)}

\CommentTok{\# mc simulations}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
\NormalTok{  y }\OtherTok{\textless{}{-}} \FunctionTok{rpois}\NormalTok{(}\DecValTok{15}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{10}\NormalTok{)}
\NormalTok{  se\_hat }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(y)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{length}\NormalTok{(y))}
\NormalTok{  lwr[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y) }\SpecialCharTok{{-}} \FunctionTok{qt}\NormalTok{(.}\DecValTok{95}\NormalTok{, }\AttributeTok{df =} \FunctionTok{length}\NormalTok{(y) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{se\_hat}
\NormalTok{  upr[i] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(y) }\SpecialCharTok{+} \FunctionTok{qt}\NormalTok{(.}\DecValTok{95}\NormalTok{, }\AttributeTok{df =} \FunctionTok{length}\NormalTok{(y) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{se\_hat}
\NormalTok{\}}

\CommentTok{\# combine results into a data frame}
\NormalTok{mc\_sims }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{iteration =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims,}
\NormalTok{                  lwr, upr) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{captured =}\NormalTok{ lwr }\SpecialCharTok{\textless{}} \DecValTok{10} \SpecialCharTok{\&}\NormalTok{ upr }\SpecialCharTok{\textgreater{}} \DecValTok{10}\NormalTok{)}

\CommentTok{\# compute the proportion of simulations that capture the parameter}
\FunctionTok{mean}\NormalTok{(mc\_sims}\SpecialCharTok{$}\NormalTok{captured)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8993
\end{verbatim}

With this criterion in mind, let's consider three types of confidence intervals that we can use in the context of maximum likelihood estimation.

\hypertarget{the-parametric-bootstrap-2}{%
\subsubsection{The Parametric Bootstrap}\label{the-parametric-bootstrap-2}}

We can evaluate the parametric bootstrap using a similar Monte Carlo approach. However, this can be \textbf{confusing} because we have two types of simulation happening.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Monte Carlo simulation to evaluate the CI. We're having our computer conduct the same ``study'' over and over to compute the long-run, frequentist properties of the CI.
\item
  Parametric bootstrap to compute each CI. To compute each CI, we're simulating many fake outcome variables \(y^{bs}\) from the fitted parametric distribution and refitting the model to obtain new estimates \(\hat{\beta}^{bs}\) that we then summarize to find a single CI.
\end{enumerate}

To start, let's create the data-generating process or ``probability model'' or population. The data are Bernoulli and the model is logit.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model parameters}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{b0 }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{b1 }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{b2 }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FloatTok{0.5}

\CommentTok{\# data}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# probability of success; Pr(y | x)}
\NormalTok{Xb }\OtherTok{\textless{}{-}}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{x1 }\SpecialCharTok{+}\NormalTok{ b2}\SpecialCharTok{*}\NormalTok{x2}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(Xb)}
\end{Highlighting}
\end{Shaded}

Now let's simulate \emph{just one} ``observed'' data set and compute the confidence intervals for that data set using the parametric bootstrap.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# simulate *one* sample and compute the 90\% ci using parametric bs}
\NormalTok{y\_obs }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ p)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y\_obs, x1, x2)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y\_obs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{family =}\NormalTok{ binomial)}

\CommentTok{\# parametric bs for coefficients}
\NormalTok{n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}  \CommentTok{\# should be 2000 or more}
\NormalTok{coef\_bs }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =}\NormalTok{ n\_bs, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)))}
\FunctionTok{names}\NormalTok{(coef\_bs) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{  p\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{  y\_bs }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{length}\NormalTok{(p\_hat), }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ p\_hat)}
\NormalTok{  fit\_bs }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, }\AttributeTok{formula =}\NormalTok{ y\_bs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\NormalTok{  coef\_bs[i, ] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit\_bs)}
\NormalTok{\}}

\CommentTok{\# compute quantiles}
\FunctionTok{apply}\NormalTok{(coef\_bs, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]       [,2]       [,3]
## 5%  -0.3184240 -0.5713317 -1.1992735
## 95%  0.5284262  0.9098523  0.2692181
\end{verbatim}

Now let's simulate \emph{many} ``observed'' data sets and compute the confidence intervals for each using the parametric bootstrap.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# do a monte carlo simulation to compute the coverage for parametric bs}
\CommentTok{\# note: this is not *at all* optimized for speed}
\NormalTok{n\_mc\_sims }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{ci\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_mc\_sims) \{}
  \CommentTok{\# simulate the "observed" data for one "study"}
\NormalTok{  y\_obs }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ p)}
\NormalTok{  data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(y\_obs, x1, x2)}
\NormalTok{  fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y\_obs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{family =}\NormalTok{ binomial)}
  \CommentTok{\# parametric bs for coefficients}
\NormalTok{  n\_bs }\OtherTok{\textless{}{-}} \DecValTok{2000}  \CommentTok{\# should be 2000 or more}
\NormalTok{  coef\_bs }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow =}\NormalTok{ n\_bs, }\AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit)))}
  \FunctionTok{colnames}\NormalTok{(coef\_bs) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(}\FunctionTok{coef}\NormalTok{(fit))}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_bs) \{}
\NormalTok{    p\_hat }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{    y\_bs }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{length}\NormalTok{(p\_hat), }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ p\_hat)}
\NormalTok{    fit\_bs }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, }\AttributeTok{formula =}\NormalTok{ y\_bs }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\NormalTok{    coef\_bs[j, ] }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit\_bs)}
\NormalTok{  \}}
  \CommentTok{\# compute quantiles}
\NormalTok{  cis }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(coef\_bs, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.95}\NormalTok{))}
  \CommentTok{\# put cis into data frame; these are the intervals for a single "study"}
\NormalTok{  ci\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{coef\_name =} \FunctionTok{colnames}\NormalTok{(cis),}
                         \AttributeTok{true =} \FunctionTok{c}\NormalTok{(b0, b1, b2),}
                  \AttributeTok{lwr =}\NormalTok{ cis[}\StringTok{"5\%"}\NormalTok{, ],}
                  \AttributeTok{upr =}\NormalTok{ cis[}\StringTok{"95\%"}\NormalTok{, ], }
                  \AttributeTok{bs\_id =}\NormalTok{ i)}
\NormalTok{\}}

\CommentTok{\# combine the intervals for the many "studies" in a single data frame}
\NormalTok{ci\_df }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(ci\_list)}

\CommentTok{\# compute the coverage for each parameter}
\NormalTok{ci\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{captured =}\NormalTok{ lwr }\SpecialCharTok{\textless{}}\NormalTok{ true }\SpecialCharTok{\&}\NormalTok{ upr }\SpecialCharTok{\textgreater{}}\NormalTok{ true) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(coef\_name) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{coverage =} \FunctionTok{mean}\NormalTok{(captured), }
            \AttributeTok{se\_hat =} \FunctionTok{sd}\NormalTok{(captured)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{n}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   coef_name   coverage se_hat
##   <chr>          <dbl>  <dbl>
## 1 (Intercept)     0.83 0.0378
## 2 x1              0.89 0.0314
## 3 x2              0.83 0.0378
\end{verbatim}

Thus the parametric bootstrap works well, the coverage is about 90\%. But notice that we know the parametric model here, because we created the ``observed'' data ourselves and matched the GDP exactly with the parametric bootstrap.

\hypertarget{week-5-models-of-binary-outcomes-and-model-fit-summaries}{%
\chapter{Week 5: Models of Binary Outcomes and Model Fit Summaries}\label{week-5-models-of-binary-outcomes-and-model-fit-summaries}}

This week, we have two goals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Develop out measures of model fit beyond the predictive distribution, adding cross validation and information criteria to our tool kit.
\item
  Modify the usual logit model in subtle and not-so-subtle ways to expand our options for modeling binary data
\end{enumerate}

\hypertarget{measures-of-model-fit}{%
\section{Measures of Model Fit}\label{measures-of-model-fit}}

\hypertarget{scoring-binary-predictions}{%
\subsection{Scoring Binary Predictions}\label{scoring-binary-predictions}}

Suppose you have a binary outcome \(y_i\) for \(i = \{1, 2, ..., n\}\) and you develop a set of predictions for each outcome in the from of probabilities \(p_i\) for \(i = \{1, 2, ..., n\}\) and a competitor develops the set \(q_i\) for \(i = \{1, 2, ..., n\}\). Intuitively, if the \(p_i\)s are ``closer'' to the \(y_i\)s than the \(q_i\)s, then the \(p_i\)s are a better prediction. By extension, the model that produced the \(p_i\)s is a better model than the model that produced the \(q_i\)s.

But we need a formal rule for defining what we mean by ``closer.'' There are two common scoring rules at the level of the individual predictions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Brier Score} The Brier score is squared error of the prediction \(p_i\) and the outcome \(y_i\), so that \(\text{Brier Score_i} = (p_i -y_i)^2\). This is analogous to linear regression, where we minimize the RMS of the residuals.
\item
  \textbf{Log Score} The log score is the logarithm of the probabilities assigned to the event that occurred. This can be awkward to interpret, since better predictions produce \emph{less negative} values. Therefore, it's common to multiply log scores by \(-1\). In practice, we tend to we can compute this as \(\text{Log Score}_i = - [y_i \log(p_i) + (1 - y_i) \log (1 - p_i)]\)
\end{enumerate}

To aggregate the scores across the observations, we can use a simple average for both the Brier and log scores.

To see these scoring rules in action, let's fit fit the familiar logit model to data from \href{https://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2011.00522.x}{Krupnikov (2011)}. In this \emph{AJPS} article, she argues that late campaign negativity targeted toward a liked candidate demobilizes voters while other forms of negativity do not.

She concludes:

\begin{quote}
\ldots the substantive results reinforce the conclusion that it is late negativity that targets the individual's preferred candidate that leads to significant changes in the likelihood of turnout. Increases in negativity about the preferred candidate decrease turnout likelihood by as much as 6 percentage points; even more importantly, the decrease in turnout is statistically significant. In contrast, the substantive effects of negativity about the other candidate, as well as overall negativity, are statistically indistinguishable from 0.
\end{quote}

In the model below (from her Model 3 in Table 4 on p.~807), she is specifically interested in comparing the effects of \texttt{negaboutdislike} and \texttt{negaboutlike}. She shows that the estimated coefficient for \texttt{negaboutdislike} is not statistically significant, while the estimated coefficient for \texttt{negaboutlike} \emph{is} statistically significant.

First, let's reproduce her fitted model results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load data}
\NormalTok{krup\_raw }\OtherTok{\textless{}{-}}\NormalTok{ haven}\SpecialCharTok{::}\FunctionTok{read\_dta}\NormalTok{(}\StringTok{"data/krup.dta"}\NormalTok{) }

\CommentTok{\# model formula (model 3, tab. 4, p. 807, krupnikov 2011)}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ turnout }\SpecialCharTok{\textasciitilde{}} 
           \CommentTok{\# negativity}
\NormalTok{           negaboutdislike }\SpecialCharTok{+}\NormalTok{ negaboutlike }\SpecialCharTok{+}
           \CommentTok{\# resources}
\NormalTok{           income }\SpecialCharTok{+}\NormalTok{ education }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ unemployed }\SpecialCharTok{+}
           \CommentTok{\# evaluation of parties and candidates}
\NormalTok{           PIDStrength }\SpecialCharTok{+}\NormalTok{ AffectPID }\SpecialCharTok{+}\NormalTok{ care }\SpecialCharTok{+}\NormalTok{ AffectPRES }\SpecialCharTok{+}
           \CommentTok{\# social involvement}
\NormalTok{           lnYears }\SpecialCharTok{+}\NormalTok{ Church }\SpecialCharTok{+}\NormalTok{ homeowners }\SpecialCharTok{+}\NormalTok{ working }\SpecialCharTok{+} 
           \CommentTok{\# mobilization}
\NormalTok{           contacted }\SpecialCharTok{+} 
           \CommentTok{\# interest, exposure, and efficacy}
\NormalTok{           external }\SpecialCharTok{+}\NormalTok{ internal }\SpecialCharTok{+}\NormalTok{ interest }\SpecialCharTok{+}\NormalTok{ media\_index }\SpecialCharTok{+} 
           \CommentTok{\# other demographics}
\NormalTok{           married }\SpecialCharTok{+}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ southern }\SpecialCharTok{+}\NormalTok{ hispanic }\SpecialCharTok{+}\NormalTok{ gender }\SpecialCharTok{+} 
           \CommentTok{\# state conditions}
\NormalTok{           closeness }\SpecialCharTok{+}\NormalTok{ governors }\SpecialCharTok{+}\NormalTok{ primaries }\SpecialCharTok{+} 
           \CommentTok{\# volume and year controls}
\NormalTok{           volume2 }\SpecialCharTok{+}\NormalTok{ dummy1988 }\SpecialCharTok{+}\NormalTok{ dummy2000 }\SpecialCharTok{+}\NormalTok{ dummy1992 }\SpecialCharTok{+}\NormalTok{ dummy1996}

\CommentTok{\# drop rows with missing values from the data set}
\NormalTok{krup }\OtherTok{\textless{}{-}}\NormalTok{ krup\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_all\_vars}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ .)  }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}

\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ krup, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now let's compute the Brier scores for each observation and the aggregate to the data set by averaging.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# obtain prediction}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# compute brier scores}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ krup}\SpecialCharTok{$}\NormalTok{turnout                           }\CommentTok{\# create a vector to make code more readable}
\NormalTok{brier\_scores }\OtherTok{\textless{}{-}}\NormalTok{ (y }\SpecialCharTok{{-}}\NormalTok{ p)}\SpecialCharTok{\^{}}\DecValTok{2}                      \CommentTok{\# compute manually}
\NormalTok{brier\_scores\_alt }\OtherTok{\textless{}{-}}\NormalTok{ scoring}\SpecialCharTok{::}\FunctionTok{brierscore}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ p) }\CommentTok{\# compute with the scoring package}

\CommentTok{\# aggregate by averaging}
\NormalTok{bs }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(brier\_scores)}
\FunctionTok{print}\NormalTok{(bs, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.089
\end{verbatim}

Now let's do the same for the log scores.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute log scores}
\NormalTok{log\_scores }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(p) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p))}
\NormalTok{log\_scores\_alt }\OtherTok{\textless{}{-}}\NormalTok{ scoring}\SpecialCharTok{::}\FunctionTok{logscore}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ p)}

\CommentTok{\# aggregate by averaging}
\NormalTok{ls }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(log\_scores)}
\FunctionTok{print}\NormalTok{(ls, }\AttributeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3
\end{verbatim}

It's usually difficult to interpret or act upon the Brier and log scores for a single model. Instead, we typically use them to choose among a set of models.

As an simple example, I removed the two late-negativity variables from the model (Krupnikov's key explanatory variables)

Important: When using the Brier and log scores, \textbf{lower scores indicate a better fit.}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5789}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1974}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2237}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Model Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Avg. Log Score
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Avg. Brier Score
\end{minipage} \\
\midrule
\endhead
Full Model & 0.2984 & 0.0893 \\
Remove Negativity About Disliked Candidates & 0.2986 & 0.0893 \\
Remove Negativity About Liked Candidates & 0.2986 & 0.0893 \\
Remove Both Negativity Variables & 0.2986 & 0.0893 \\
\bottomrule
\end{longtable}

\includegraphics{05-01-model-fit-summaries_files/figure-latex/unnamed-chunk-5-1.pdf}

\hypertarget{cross-validation}{%
\subsection{Cross-Validation}\label{cross-validation}}

But here's the dirty little secret: you can \textbf{always} make your model better \emph{within your sample} by making the model more complex. As a simple illustration, I added a quadruple interactions between both forms of negativity and education, income, and gender.

\begin{verbatim}
turnout ~ negaboutdislike*education*income*gender + negaboutlike*education*income*gender + ...
\end{verbatim}

This model will better predictions than the baseline model.

\begin{longtable}[]{@{}lrr@{}}
\toprule
Model Name & Avg. Log Score & Avg. Brier Score \\
\midrule
\endhead
Full Model & 0.2983961 & 0.08928690 \\
Adding Wild Interactions & 0.2966916 & 0.08876646 \\
\bottomrule
\end{longtable}

Consider the two models fit trough the data below. A simple line (in green) fits the data quite nicely. However, a 10th-order polynomial (in orange) fits the observed data \emph{even better} (in fact, it has no error at all!).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{11}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{length.out =}\NormalTok{ n)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n) }\SpecialCharTok{+}\NormalTok{ x}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(y, x)}

\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{fit10 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x, }\DecValTok{10}\NormalTok{), }\AttributeTok{data =}\NormalTok{ data)}

\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(x, y)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{color =} \StringTok{"\#1b9e77"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}} \FunctionTok{poly}\NormalTok{(x, }\DecValTok{10}\NormalTok{), }\AttributeTok{n =} \DecValTok{1001}\NormalTok{, }\AttributeTok{color =} \StringTok{"\#d95f02"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{05-01-model-fit-summaries_files/figure-latex/unnamed-chunk-7-1.pdf}

But if we used the 10th-order polynomial for prediction, it would perform horribly. Why? Because it \emph{overfits} the data. That is, it explains both the \emph{systematic} and \emph{idiosyncratic} features of the observed data. Suppose we need to make a prediction for \(x = -0.95\). The complex model generates a prediction of about -14.

To avoid over-fitting the model, we can use two approaches.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  cross validation
\item
  information criteria
\end{enumerate}

Let's start with leave-one-out cross validation.

For each observation \(i\) in the data set:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Drop that observation \(i\).
\item
  Fit the model using the remaining data.
\item
  Predict the dropped observation.
\item
  Compute the score for that observation.
\end{enumerate}

Because the observation being predicted is left-out and \emph{not in the data set used to fit the model}, the model cannot ``cheat'' and fit the idiosyncratic variation in the left-out data point. In order to perform well, it must identify systematic variation in the \emph{other} data points and use that information to predict the left-out observation.

If your data set has \(n\) observations, then you must fit \(n\) models to perform leave-one-out cross validation. Let's estimate the time-cost for Krupnikov's model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit model and store time}
\NormalTok{time }\OtherTok{\textless{}{-}} \FunctionTok{system.time}\NormalTok{( }
\NormalTok{  fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ krup, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# multiply elapsed time times number of observations}
\FunctionTok{round}\NormalTok{(time[}\StringTok{"elapsed"}\NormalTok{]}\SpecialCharTok{*}\FunctionTok{nrow}\NormalTok{(krup)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{60}\NormalTok{), }\DecValTok{1}\NormalTok{)  }\CommentTok{\# convert to minutes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## elapsed 
##     6.6
\end{verbatim}

Each model takes about 0.05 seconds to fit. This seems fast, but you need to do it about 6,000 times, which takes about 300 seconds or five minutes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# note system time}
\NormalTok{start\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}

\CommentTok{\# perform cross validation}
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\CommentTok{\#for (i in 1:nrow(krup)) \{}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) \{}
  \ControlFlowTok{if}\NormalTok{ (i }\SpecialCharTok{\%\%} \DecValTok{100} \SpecialCharTok{==} \DecValTok{0}\NormalTok{) }\FunctionTok{print}\NormalTok{(i)}
  \CommentTok{\# create training data and test data, to make code readable}
\NormalTok{  training }\OtherTok{\textless{}{-}} \FunctionTok{slice}\NormalTok{(krup, }\SpecialCharTok{{-}}\NormalTok{i)}
\NormalTok{  test     }\OtherTok{\textless{}{-}} \FunctionTok{slice}\NormalTok{(krup,  i)}
  \CommentTok{\# fit model}
\NormalTok{  fit\_i }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ training, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
  \CommentTok{\# compute scores for test data (compute scores later)}
\NormalTok{  y\_i }\OtherTok{\textless{}{-}}\NormalTok{ test}\SpecialCharTok{$}\NormalTok{turnout}
\NormalTok{  p\_i }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit\_i, }\AttributeTok{newdata =}\NormalTok{ test, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
  \CommentTok{\# store results}
\NormalTok{  results\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{case\_id =}\NormalTok{ i,}
                              \AttributeTok{y =}\NormalTok{ y\_i,}
                              \AttributeTok{p =}\NormalTok{ p\_i)}
\NormalTok{\}}

\CommentTok{\# note system time}
\NormalTok{end\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{diff\_time }\OtherTok{\textless{}{-}} \FunctionTok{difftime}\NormalTok{(end\_time, start\_time, }\AttributeTok{units =} \StringTok{"mins"}\NormalTok{)}

\CommentTok{\# combine results and compute scores}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(results\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_score =} \SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(p) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p)),}
         \AttributeTok{brier\_score =}\NormalTok{ (y }\SpecialCharTok{{-}}\NormalTok{ p)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\CommentTok{\# average scores}
\FunctionTok{print}\NormalTok{(}\FunctionTok{mean}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{log\_score), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.411
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{mean}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{brier\_score), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.13
\end{verbatim}

This code took 0) minutes to run. This isn't always practical, especially for large data sets. (It is embarrassingly parallel, though, so it's possible to dramatically shrink this time using parallel computing.)

For large data sets, rather than drop each observation individually, we can divide the data into \(k\) equally-sized (or as close to equal as possible) groups. The we repeat the same process but drop and predict each \emph{group} rather than the individual data points. This is called \textbf{\(k\)-fold cross validation}. If \(k = n\), then we just have leave-one-out cross-validation.

The code below uses \(k = 10\) and finds the average log and Brier scores for out-of-sample prediction using \(k\)-fold cross-validation for the scobit data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cross validation groups}
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{10}  
\NormalTok{group }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{k, }\AttributeTok{length.out =} \FunctionTok{nrow}\NormalTok{(krup)))}

\CommentTok{\# perform cross validation}
\NormalTok{results\_list }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{k) \{}
  \CommentTok{\# create training data and test data, to make code readable}
\NormalTok{  training }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(krup, group }\SpecialCharTok{!=}\NormalTok{ i)}
\NormalTok{  test     }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(krup, group }\SpecialCharTok{==}\NormalTok{ i)}
  \CommentTok{\# fit model}
\NormalTok{  fit\_i }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ training, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
  \CommentTok{\# compute scores for test data (compute scores later)}
\NormalTok{  y\_i }\OtherTok{\textless{}{-}}\NormalTok{ test}\SpecialCharTok{$}\NormalTok{turnout}
\NormalTok{  p\_i }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit\_i, }\AttributeTok{newdata =}\NormalTok{ test, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
  \CommentTok{\# store results}
\NormalTok{  results\_list[[i]] }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{group =}\NormalTok{ i,}
                              \AttributeTok{y =}\NormalTok{ y\_i,}
                              \AttributeTok{p =}\NormalTok{ p\_i)}
\NormalTok{\}}

\CommentTok{\# combine results and compute scores}
\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(results\_list) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_score =} \SpecialCharTok{{-}}\NormalTok{(y}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(p) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ y)}\SpecialCharTok{*}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p)),}
         \AttributeTok{brier\_score =}\NormalTok{ (y }\SpecialCharTok{{-}}\NormalTok{ p)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\CommentTok{\# average scores}
\FunctionTok{print}\NormalTok{(}\FunctionTok{mean}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{log\_score), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.303
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{mean}\NormalTok{(results}\SpecialCharTok{$}\NormalTok{brier\_score), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0908
\end{verbatim}

This result took just a second or two.

To illustrate how we can use \(k\)-fold cross-validation to evaluate models, I use \(k\)-fold cross validation to compute the average scores for all seven models (simpler and more complex) models discussed above.

The results are \emph{really} close and can depend on the random assignment to the \(k\) groups, so we want a large \(k\) (or use leave-one-out cross validation). For the results below, I use 100-fold cross-validation.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5789}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1974}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2237}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Model Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Avg. Log Score
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Avg. Brier Score
\end{minipage} \\
\midrule
\endhead
Adding Wild Interactions & 0.2937 & 0.0855 \\
Full Model & 0.2901 & 0.0849 \\
Remove Both Negativity Variables & 0.2909 & 0.0851 \\
Remove Negativity About Disliked Candidates & 0.2905 & 0.0849 \\
Remove Negativity About Liked Candidates & 0.2909 & 0.0851 \\
\bottomrule
\end{longtable}

\includegraphics{05-01-model-fit-summaries_files/figure-latex/unnamed-chunk-11-1.pdf}

\hypertarget{information-criteria}{%
\subsection{Information Criteria}\label{information-criteria}}

As an alternative to cross-validation, we can use information criteria for a similar purpose without needing to refit the model many times.

Information criteria have the following general structure:

\[
-2 \log L(\hat{\theta}) + [\text{constant}\times k ]
\]

Here, \(\log L(\hat{\theta})\) is the value achieved when we maximized the log-likelihood function (not the \(\hat{\theta}\), but the value of \(\log L\) itself), \(k\) is the number of parameters, and \(\text{constant}\) is a constant term that varies across information criteria.

The two most common information criteria are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Akaike Information Criterion (AIC)} \(= -2 \log L(\hat{\theta}) + [2 \times k]\)
\item
  \textbf{Bayesian Information Criterion (AIC)} \(= -2 \log L(\hat{\theta}) + [\log(n) \times k]\)
\end{enumerate}

The AIC and BIC have a deep and detailed theoretical development--the choice of constant is not at all arbitrary. It doesn't seem helpful to reproduce the theory here, but instead mention a few practical points.

\begin{itemize}
\tightlist
\item
  The \emph{magnitude} of the IC is generally not of interest. Instead, we focus on the \emph{difference} in the IC between models.
\item
  Both the the AIC and the BIC work to identify the ``best'' model, but in two difference senses:

  \begin{itemize}
  \tightlist
  \item
    The AIC roughly compares the observed and predictive distributions are tries to identify the best match.
  \item
    The BIC roughly identifies the model with the highest posterior probability---the most likely model to have generated the data.
  \end{itemize}
\item
  Both AIC and BIC penalize adding parameters. That is, in order to improve the IC, a more complex model must improve the fit enough to offset the additional penalty. That said, the BIC imposes a larger penalty for \(n \geq 8\).
\end{itemize}

ADD SOME DISCUSSION ABOUT THE MAGNITUDE OF THE DIFFERENCE OF AIC AND BIC.

ADD RAFTERY's TABLE HERE.

To compute the AIC and BIC, we have the easy-to-use \texttt{AIC()} and \texttt{BIC()} functions.

We can use those to compare models with and without the \texttt{negaboutlike} variable, for example. The AIC \emph{slightly} prefers including the variable, but the BIC prefers the model without Krupnikov's key explanatory variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# aic and bic for full model}
\FunctionTok{AIC}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3774.467
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3996.707
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compare models}
\NormalTok{fit0 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{negaboutlike)}
\NormalTok{fit1 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{negaboutdislike)}
\NormalTok{fit2 }\OtherTok{\textless{}{-}} \FunctionTok{update}\NormalTok{(fit, . }\SpecialCharTok{\textasciitilde{}}\NormalTok{ . }\SpecialCharTok{{-}}\NormalTok{negaboutdislike }\SpecialCharTok{{-}}\NormalTok{ negaboutlike)}


\FunctionTok{AIC}\NormalTok{(fit, fit0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      df      AIC
## fit  33 3774.467
## fit0 32 3775.490
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit, fit0)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      df      BIC
## fit  33 3996.707
## fit0 32 3990.996
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# compute model weights}
\CommentTok{\# note: in krupnikov\textquotesingle{}s theory, fit1 should be best (fit includes unnecessary variable)}
\FunctionTok{AIC}\NormalTok{(fit, fit0, fit1, fit2) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ AIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(AIC),}
         \AttributeTok{akiaike\_weights =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      df      AIC  diff_min akiaike_weights
## fit  33 3774.467 0.8580177       0.2448183
## fit0 32 3775.490 1.8814836       0.1467578
## fit1 32 3774.571 0.9617180       0.2324479
## fit2 31 3773.609 0.0000000       0.3759760
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit, fit0, fit1, fit2) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ BIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(BIC),}
         \AttributeTok{post\_prob =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      df      BIC  diff_min    post_prob
## fit  33 3996.707 14.327138 0.0007477026
## fit0 32 3990.996  8.616044 0.0129980202
## fit1 32 3990.077  7.696278 0.0205874133
## fit2 31 3982.380  0.000000 0.9656668640
\end{verbatim}

\includegraphics{05-01-model-fit-summaries_files/figure-latex/unnamed-chunk-13-1.pdf}

\includegraphics{05-01-model-fit-summaries_files/figure-latex/unnamed-chunk-14-1.pdf}

\hypertarget{models-of-binary-outcomes}{%
\section{Models of Binary Outcomes}\label{models-of-binary-outcomes}}

Here, I describe a diverse collection of models of binary outcomes.

\hypertarget{logit}{%
\subsection{Logit}\label{logit}}

\[
y_i \sim \text{Bernoulli}(\pi_i), \text{ where } \pi_i = \text{logit}^{-1}(X_i\beta) \text{ and } \text{logit}^{-1}(x) = \frac{e^x}{1 + e^x} 
\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8217}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Quantity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
Distribution & \(y_i \sim \text{Bernoulli}(\pi_i)\) \\
Inverse Link Function & \(\pi_i = \text{logit}^{-1}(X_i\beta) = \frac{e^{X_i\beta}}{1 + e^{X_i\beta}}\) \\
Fit Syntax & \texttt{glm(formula,\ data,\ family\ =\ binomial()} \\
Expected Value & \(\hat{E}(\tilde{y} \mid X_s) = \hat{\pi}_s = \text{logit}^{-1}(X_s\hat{\beta})\) \\
Manual Computation of EV & \texttt{pi\_s\ =\ plogis(X\_s\ \%*\%\ beta\_hat)} \\
\texttt{predict()} Computation of EV & \texttt{pi\_s\ =\ predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "response")} \\
Marginal Effect & \(\frac{ \partial \hat{E}(\tilde{y} \mid X_s)}{\partial x_j} = \hat{\beta}_j\hat{\pi}_s(1 - \hat{\pi}_s)\) \\
Computation of Marginal Effect & \texttt{coef(fit)*p\_s*(1\ -\ p\_s)} \\
\bottomrule
\end{longtable}

Note: the marginal effect above assumes that \(x_j\) is included in the model \emph{linearly} with no polynomial or interactions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load nagler\textquotesingle{}s scobit data}
\NormalTok{scobit }\OtherTok{\textless{}{-}}\NormalTok{ haven}\SpecialCharTok{::}\FunctionTok{read\_dta}\NormalTok{(}\StringTok{"data/scobit.dta"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(newvote }\SpecialCharTok{!=} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)  }\CommentTok{\# weird {-}1s in data; unsure if sufficient}

\CommentTok{\# simplest model specification; no polynomials or interactions}
\NormalTok{f }\OtherTok{\textless{}{-}}\NormalTok{ newvote }\SpecialCharTok{\textasciitilde{}}\NormalTok{ neweduc }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov}

\CommentTok{\# fit model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Now let's compute quantities of interest. Let's first compute the expected value or ``predicted probability'' as \texttt{closing} varies from zero to 50 with all other variables at their medians.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let education vary across all possible values; else median}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)}
\NormalTok{)}

\CommentTok{\# expected value or "predicted probability" }
\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# marginal effect of closing on expected value}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{marginal\_effect }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit)[}\StringTok{"neweduc"}\NormalTok{]}\SpecialCharTok{*}\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value)}
\end{Highlighting}
\end{Shaded}

Now let's make a plot of the expected values and marginal effects.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot }
\NormalTok{gg1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(s, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ neweduc, }\AttributeTok{y =}\NormalTok{ expected\_value)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{()}
\NormalTok{gg2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(s, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ neweduc, }\AttributeTok{y =}\NormalTok{ marginal\_effect)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_bw}\NormalTok{()}

\CommentTok{\# combine plots and print}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{gg1 }\SpecialCharTok{+}\NormalTok{ gg2}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-4-1.pdf}

For this model, let's briefly consider the \emph{other} predictors in the model. In particular, let's reproduce the figure above, but for every combination of the other covariates that appeared in the data set. (It turns out the \emph{every} combination was overwhelming, so I used a sample of 100 other scenarios from the exhaustive list).

You can see that as the \emph{other} covariates change, the expected value changes. But more importantly, the maginal effect of education changes as the other covariates change. That's why it's \emph{critical} to develop an appropriate scenario that specifies values for \emph{all} the covariates in the model.

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-5-1.pdf}
Lastly, before we proceed to the next model, I'm going to save the expected values and marginal effects that I computed for the logit model to compare to the other models we discuss.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s\_logit }\OtherTok{\textless{}{-}}\NormalTok{ s     }\CommentTok{\# estimates of ev and me from logit model}
\NormalTok{fit\_logit }\OtherTok{\textless{}{-}}\NormalTok{ fit }\CommentTok{\# logit model fit}
\end{Highlighting}
\end{Shaded}

\hypertarget{probit}{%
\subsection{Probit}\label{probit}}

The probit model replaced the CDF of the standard logistic distribution with the CDF of the standard normal distribution. Rather than the ``inverse logit'' inverse link function, we use the CDF of the standard normal distrubtion.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8217}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Quantity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
Distribution & \(y_i \sim \text{Bernoulli}(\pi_i)\) \\
Inverse Link Function & \(\pi_i = \Phi(X_i\beta)\), where \(\Phi()\) is the PDF of the standard normal distributio \\
Fit Syntax & \texttt{glm(formula,\ data,\ family\ =\ binomial(link\ =\ "probit")} \\
Expected Value & \(\hat{E}(\tilde{y} \mid X_s) = \hat{\pi}_s = \Phi(X_s\hat{\beta})\) \\
Manual Computation of EV & \texttt{pi\_s\ =\ pnorm(X\_s\ \%*\%\ beta\_hat)} \\
\texttt{predict()} Computation of EV & \texttt{pi\_s\ =\ predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "response")} \\
Marginal Effect & \(\frac{ \partial \hat{E}(\tilde{y} \mid X_s)}{\partial x_j} = \phi(X\hat{\beta}) \hat{\beta}_j\), where \(\phi()\) is the PDF of the standard normal distribution \\
Computation of Marginal Effect & \texttt{dnorm(predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "link"))*coef(fit){[}j{]}} \\
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now let's compute quantities of interest. Let's first compute the expected value or ``predicted probability'' as \texttt{neweduc} varies from 1 to 8 (all possible values) with all other variables at their medians.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let education vary across all possible values; else median}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)}
\NormalTok{)}

\CommentTok{\# expected value or "predicted probability" }
\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# marginal effect of closing on expected value}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{linpred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{)}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{marginal\_effect }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(fit)[}\StringTok{"neweduc"}\NormalTok{]}\SpecialCharTok{*}\FunctionTok{dnorm}\NormalTok{(s}\SpecialCharTok{$}\NormalTok{linpred)}
\end{Highlighting}
\end{Shaded}

Now let's make a plot of the expected values and marginal effects. The grey lines show the orginal logit estimates.

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-9-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =} \FunctionTok{min}\NormalTok{(AIC) }\SpecialCharTok{{-}}\NormalTok{ AIC, }
         \AttributeTok{rel\_lik =} \FunctionTok{exp}\NormalTok{(diff\_min}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df      AIC  diff_min      rel_lik
## fit        6 113111.1 -182.1109 2.851781e-40
## fit_logit  6 112929.0    0.0000 1.000000e+00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit, fit\_logit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df      BIC
## fit        6 113168.2
## fit_logit  6 112986.1
\end{verbatim}

\hypertarget{cloglog}{%
\subsection{Cloglog}\label{cloglog}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8217}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Quantity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
Distribution & \(y_i \sim \text{Bernoulli}(\pi_i)\) distribution \\
Inverse Link Function & \(\pi_i = 1 - e^{-e^{X_i\beta}}\) \\
Fit Syntax & \texttt{glm(formula,\ data,\ family\ =\ binomial(link\ =\ "cloglog")} \\
Expected Value & \(\hat{E}(\tilde{y} \mid X_s) = \hat{\pi}_s = 1 - e^{-e^{X_s\beta}}\) \\
Manual Computation of EV & \texttt{pi\_s\ =\ 1\ -\ exp(-exp(X\_s\ \%*\%\ beta\_hat))} \\
\texttt{predict()} Computation of EV & \texttt{pi\_s\ =\ predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "response")} \\
Marginal Effect & \(\frac{ \partial \hat{E}(\tilde{y} \mid X_s)}{\partial x_j} = e^{-e^{X\hat{\beta}}}e^{X\hat{\beta}}\hat{\beta}_j\) \\
Computation of Marginal Effect & \texttt{exp(-exp(linpred))*exp(linpred)*coef(fit){[}j{]}}, where \texttt{linpred\ \textless{}-\ predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "link")} \\
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"cloglog"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now let's compute quantities of interest. Let's first compute the expected value or ``predicted probability'' as \texttt{neweduc} varies from 1 to 8 (all possible values) with all other variables at their medians.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let education vary across all possible values; else median}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)}
\NormalTok{)}

\CommentTok{\# expected value or "predicted probability" }
\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# marginal effect of closing on expected value}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{linpred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{)}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{marginal\_effect }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{exp}\NormalTok{(s}\SpecialCharTok{$}\NormalTok{linpred))}\SpecialCharTok{*}\FunctionTok{exp}\NormalTok{(s}\SpecialCharTok{$}\NormalTok{linpred)}\SpecialCharTok{*}\FunctionTok{coef}\NormalTok{(fit)[}\StringTok{"neweduc"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Now let's make a plot of the expected values and marginal effects. The grey lines show the orginial logit estimates.

Importantly, the cloglog model is not symmetric (like logit and probit) around 0.5.

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-13-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ AIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(AIC),}
         \AttributeTok{akiaike\_weights =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df    AIC diff_min akiaike_weights
## fit        6 113664 734.9586   2.545478e-160
## fit_logit  6 112929   0.0000    1.000000e+00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ BIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(BIC),}
         \AttributeTok{post\_prob =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df      BIC diff_min     post_prob
## fit        6 113721.0 734.9586 2.545478e-160
## fit_logit  6 112986.1   0.0000  1.000000e+00
\end{verbatim}

\hypertarget{cauchit}{%
\subsection{Cauchit}\label{cauchit}}

Like the probit uses the normal CDF as the inverse link function, the cauchit uses the Cauchy CDF. Why logit and probit are similar, there is a meaningful difference between the logit and cauchit. Because the Cauchy has heavier tails that the logistic and normal distributions, it behaves more linearly

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8217}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Quantity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
Distribution & \(y_i \sim \text{Bernoulli}(\pi_i)\) distribution \\
Inverse Link Function & \(\pi_i = \frac{1}{\pi}\text{arctan} (X_i\beta) + \frac{1}{2} = F_{Cauchy}(X_i\beta)\). \\
Fit Syntax & \texttt{glm(formula,\ data,\ family\ =\ binomial(link\ =\ "cauchit")} \\
Expected Value & \(\hat{E}(\tilde{y} \mid X_s) = \hat{\pi}_s = \frac{1}{\pi}\text{arctan} (X_s\beta) + \frac{1}{2}\) \\
Manual Computation of EV & \texttt{pi\_s\ =\ (1/pi)*atan(X\_s\ \%*\%\ beta\_hat)\ +\ 0.5} (Make sure you haven't redefined \texttt{pi}!) \\
\texttt{predict()} Computation of EV & \texttt{pi\_s\ =\ predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "response")} \\
Marginal Effect & \(\frac{ \partial \hat{E}(\tilde{y} \mid X_s)}{\partial x_j} = \pi\left[ 1 + (X_s\hat{\beta})^2 \right] \hat{\beta}_j = f_{\text{Cauchy}}(X_s\hat{\beta})\hat{\beta}_j\) \\
Computation of Marginal Effect & \texttt{dcauchy(linpred)*(coef(fit){[}j{]}}, where \texttt{linpred\ \textless{}-\ predict(fit,\ newdata\ =\ X\_s,\ type\ =\ "link")} \\
\bottomrule
\end{longtable}

To illustrate the tail behavior of the logit and cauchit model, I simulated a data set using the true cauchit model, and fitted the logit and cauchit models.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The logit curve \emph{smoothly} transitions from flat to steep to flat again.
\item
  The cauchit model seems to have three distinct sections a roughly linear portion below \(pi = 0.15\), another roughly linear portion between between 0.25 and 0.75, and another roughly linear portion above about 0.85. Between these roughly linear portions, the curve changes slope changes dramatically.
\end{enumerate}

\begin{verbatim}
## Rows: 10,000
## Columns: 3
## $ x     <dbl> -8.000000, -7.998400, -7.996800, -7.995200, -7.993599, -7.991999~
## $ pr    <dbl> 0.009101752, 0.009110178, 0.009118611, 0.009127051, 0.009135500,~
## $ model <chr> "logit (best approximation)", "logit (best approximation)", "log~
\end{verbatim}

\begin{verbatim}
## Rows: 10,000
## Columns: 3
## $ x     <dbl> -8.000000, -7.998400, -7.996800, -7.995200, -7.993599, -7.991999~
## $ pr    <dbl> 0.03900972, 0.03901743, 0.03902513, 0.03903284, 0.03904055, 0.03~
## $ model <chr> "cauchy (true)", "cauchy (true)", "cauchy (true)", "cauchy (true~
\end{verbatim}

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-15-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(f, }\AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"cauchit"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now let's compute quantities of interest. Let's first compute the expected value or ``predicted probability'' as \texttt{neweduc} varies from 1 to 8 (all possible values) with all other variables at their medians.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let education vary across all possible values; else median}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)}
\NormalTok{)}

\CommentTok{\# expected value or "predicted probability" }
\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# marginal effect of closing on expected value}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{linpred }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"link"}\NormalTok{)}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{marginal\_effect }\OtherTok{\textless{}{-}} \FunctionTok{dcauchy}\NormalTok{(s}\SpecialCharTok{$}\NormalTok{linpred)}\SpecialCharTok{*}\FunctionTok{coef}\NormalTok{(fit)[}\StringTok{"neweduc"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Now let's make a plot of the expected values and marginal effects. The grey lines show the orginial logit estimates.

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-18-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ AIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(AIC),}
         \AttributeTok{akiaike\_weights =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df      AIC diff_min akiaike_weights
## fit        6 112650.1   0.0000    1.000000e+00
## fit_logit  6 112929.0 278.9223    2.708833e-61
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ BIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(BIC),}
         \AttributeTok{post\_prob =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df      BIC diff_min    post_prob
## fit        6 112707.1   0.0000 1.000000e+00
## fit_logit  6 112986.1 278.9223 2.708833e-61
\end{verbatim}

\hypertarget{scobit}{%
\subsection{Scobit}\label{scobit}}

Nagler (1994) argues that we should consider relaxing the assumption that the marginal effect \(\frac{ \partial \hat{E}(\tilde{y} \mid X_s)}{\partial x_j}\) is maximized at \(\hat{E}(\tilde{y} \mid X_s) = 0.5\) and instead estimate a more general function that allows the point where the marginal effect is maximized to be \emph{estimated} from the data.

While the logit model uses the inverse link function \(\pi_i = \frac{1}{1 + e^{-X_i\beta}\right]^\alpha}\), the scobit model uses the inverse link function \(\pi_i = \frac{1}{\left[ 1 + e^{-X_i\beta}\right]^\alpha}\).

You can read more about Nagler's Scobit model in is 1994 \emph{AJPS} \href{https://www.jstor.org/stable/2111343}{article}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1783}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8217}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Quantity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule
\endhead
Distribution & \(y_i \sim \text{Bernoulli}(\pi_i)\) distribution \\
Inverse Link Function & \(\pi_i = \frac{1}{\left[ 1 + e^{-X_i\beta}\right]^\alpha}\) \\
Fit Syntax & None. I do not recommend ML. This model is difficult, so I recommend MCMC and weakly informative priors. \\
Expected Value & \(\hat{E}(\tilde{y} \mid X_s) = \frac{1}{\left[ 1 + e^{-X_s\hat{\beta}} \right]^\hat{\alpha}}\) \\
Manual Computation of EV & \texttt{pi\_s\ =\ 1/((1\ +\ exp(-X\_s\ \%*\%\ beta\_hat))\^{}alpha\_hat)} \\
\texttt{predict()} Computation of EV & None; use generated quantities block in Stan or manual post-processing of posterior simulations. \\
Marginal Effect & \(\frac{ \partial \hat{E}(\tilde{y} \mid X_s)}{\partial x_j} = \hat{\alpha} [1 + e^{-X\hat{\beta}}][e^{-X\hat{\beta}}]\hat{\beta}_j\) \\
Computation of Marginal Effect & Use generated quantities block in Stan or manual post-processing of posterior simulations. \\
\bottomrule
\end{longtable}

These models are notoriously difficult to estimate. Fortunately, we've got very good MCMC algorithms to deal with difficult likelihoods. If you \emph{must} fit this model, you should fit it with Stan and be extra careful.

I've tried fitting this lots of different ways and not had much success. I suggest using Stan's sampling algorithm (or perhaps its optimization routine) and using a fairly strong prior around \(\alpha\) to keep it near 1.

\begin{verbatim}
#{stan, output.var="scobit_model", cache = TRUE}
data {
  int<lower=0> N;   // number of data items
  int<lower=0> K;   // number of predictors
  matrix[N, K] X;   // predictor matrix
  int y[N];      // outcome vector
}
parameters {
  vector[K] beta;       // coefficients for X
  real<lower=0> alpha;  // nagler's shape parameter
}
transformed parameters {
  vector[N] pi;
  vector[N] linpred;
  for (i in 1:N) {
    linpred[i] = X[i, ]*beta;
    //pi[i] = exp(linpred[i])/(1 + exp(linpred[i]));
    //pi[i] = 1/(1 + exp(-linpred[i])); 
    pi[i] = 1/pow((1 + exp(-(X[i, ]*beta))), alpha);
  }
}
model {
  alpha ~ gamma(6, 5);  // alpha is "near" one; mode of gamma is (a - 1)/b; sd is sqrt(a)/b
  //beta ~ cauchy(0, 10);     // weakly informative prior to rule out very large and small values
  y ~ bernoulli(pi);  // likelihood
}
\end{verbatim}

\begin{verbatim}
#{r cache=TRUE, results="hide"}
# obtain the model matrix X
set.seed(1234)
scobit_small = sample_n(scobit, 1000)  # small sample to save time
mf <- model.frame(f, data = scobit_small)  # model frame
X <- model.matrix(f, mf)         # model matrix X

# obtain the outcome variable y
y <- model.response(mf)

# fit stan model
stan_data <- list(y = y, X = X, N = nrow(X), K = ncol(X))
stan_opt <- rstan::optimizing(scobit_model, data = stan_data, 
                              algorithm = "Newton")
stan_samp <- rstan::sampling(scobit_model, data = stan_data,
                            pars = c("beta", "alpha"),
                            cores = 3,
                            chains = 3,
                            iter = 2000,
                            warmup = 1000)
\end{verbatim}

\begin{verbatim}
#{r}
# ML results from stan optimization of scobit
print(stan_opt$par[1:7])

# ML results from glm() logit 
coef(glm(f, data = scobit_small, family = binomial))

# summary of stan mcmc samples
print(stan_samp)
\end{verbatim}

We will leave the analysis of this model to later in the semester when we have more experience with Stan.

\hypertarget{heteroskedastic-probit}{%
\subsection{Heteroskedastic Probit}\label{heteroskedastic-probit}}

We can develop the probit model using a ``latent variable'' or ``latent utility'' approach.

Suppose an unobserved (or ``latent'') outcome variable \(y^*\), so that \(y^*_i \sim N(\mu_i, \sigma = 1)\), where \(\mu_i = X_i\beta\). Then suppose that if \(y^*_i \geq 0\), we observe \(y_i = 1\). If \(y^*_i < 0\), we observe \(y_i = 0\).

A little simulation might help.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{b0 }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{b1 }\OtherTok{\textless{}{-}} \FloatTok{1.5}
\NormalTok{mu }\OtherTok{\textless{}{-}}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{x}

\NormalTok{y\_star }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, mu, }\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(y\_star }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(x, y\_star, y)}

\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y\_star, }\AttributeTok{color =} \FunctionTok{factor}\NormalTok{(y))) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{slope =}\NormalTok{ b1, }\AttributeTok{intercept =}\NormalTok{ b0, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-20-1.pdf}

An implication of this framework is that \(\Pr(y_i = 1 \mid X_i) = \Pr(y^*_i \geq 1 \mid X_i) = \Phi(X_i\beta)\), which gives us exactly the probit model. Long (1997) develops this further in Section 3.2.

But we can use this framework to generalize the usual probit model into a \emph{heteroskedastic probit model}. Rather than fix the error of the latent erors to \(\sigma = 1\), we could allow them to vary with a set of explanatory variables so that \(\sigma = e^{Z\gamma}\). (We use \(X\beta\) for the linear predictor that belongs to the mean and \(Z\gamma\) for the linear predictor that belongs to \(\sigma\).)

The \(Z\) variables have an interesting relationship to \(\Pr(y_i = 1 \mid X_i)\). If a variable drives the linear predictor \(Z\gamma\) (and therefore the error variance \(\sigma^2\)) higher, it pushes \(\Pr(y_i = 1 \mid X_i)\) toward 0.5. That is, if the probability of an event is near one, increasing the latent variance \emph{decreases} the chance of an event. But if the probability of an event is near zero, increasing the latent variance \emph{increases} the chance of an event. Taken to an extreme, this can produce a non-monotic effect of a variable included linearly in both sets of explanatory variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{500}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(n, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{b0 }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{b1 }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{mu }\OtherTok{\textless{}{-}}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{x}

\NormalTok{g0 }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{g1 }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(g0 }\SpecialCharTok{+}\NormalTok{ g1}\SpecialCharTok{*}\NormalTok{x)}

\NormalTok{y\_star }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n, mu, sigma)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(y\_star }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(x, y\_star, y)}

\NormalTok{gg1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y\_star, }\AttributeTok{color =} \FunctionTok{factor}\NormalTok{(y))) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey50"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{slope =}\NormalTok{ b1, }\AttributeTok{intercept =}\NormalTok{ b0, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }

\NormalTok{pr }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.01}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pr =} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{pnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{mean =}\NormalTok{ b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{x, }\AttributeTok{sd =} \FunctionTok{exp}\NormalTok{(g0 }\SpecialCharTok{+}\NormalTok{ g1}\SpecialCharTok{*}\NormalTok{x)))}
\NormalTok{gg2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(pr, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ pr)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{()}

\NormalTok{gg1 }\SpecialCharTok{+}\NormalTok{ gg2}
\end{Highlighting}
\end{Shaded}

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-21-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(glmx)}

\CommentTok{\# fit model}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{hetglm}\NormalTok{(newvote }\SpecialCharTok{\textasciitilde{}}\NormalTok{ neweduc }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov }\SpecialCharTok{|}\NormalTok{ neweduc }\SpecialCharTok{+}\NormalTok{ closing }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ south }\SpecialCharTok{+}\NormalTok{ gov, }
              \AttributeTok{data =}\NormalTok{ scobit, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now let's compute quantities of interest. Let's first compute the expected value or ``predicted probability'' as \texttt{neweduc} varies from 1 to 8 (all possible values) with all other variables at their medians.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# let education vary across all possible values; else median}
\NormalTok{s }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{neweduc =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{,}
  \AttributeTok{closing =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{closing),}
  \AttributeTok{age =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{age),}
  \AttributeTok{south =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{south),}
  \AttributeTok{gov =} \FunctionTok{median}\NormalTok{(scobit}\SpecialCharTok{$}\NormalTok{gov)}
\NormalTok{)}

\CommentTok{\# expected value or "predicted probability" }
\NormalTok{s}\SpecialCharTok{$}\NormalTok{expected\_value }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# marginal effect of closing on expected value}
\CommentTok{\#s$linpred \textless{}{-} predict(fit, newdata = s, type = "link")}
\CommentTok{\#s$sigma \textless{}{-} predict(fit, newdata = s, type = "scale")}
\CommentTok{\#s$marginal\_effect \textless{}{-} dnorm(0, mean = s$linpred, sd = s$sigma)*coef(fit)["neweduc"]}

\CommentTok{\# marginal effect of closing on expected value, *numerically*}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FloatTok{0.00001}
\NormalTok{s\_prime }\OtherTok{\textless{}{-}}\NormalTok{ s }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{neweduc =}\NormalTok{ neweduc }\SpecialCharTok{+}\NormalTok{ delta)}
\NormalTok{s\_prime}\SpecialCharTok{$}\NormalTok{expected\_value }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(fit, }\AttributeTok{newdata =}\NormalTok{ s\_prime, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{s}\SpecialCharTok{$}\NormalTok{marginal\_effect }\OtherTok{\textless{}{-}}\NormalTok{ (s\_prime}\SpecialCharTok{$}\NormalTok{expected\_value }\SpecialCharTok{{-}}\NormalTok{ s}\SpecialCharTok{$}\NormalTok{expected\_value)}\SpecialCharTok{/}\NormalTok{delta}
\end{Highlighting}
\end{Shaded}

Now let's make a plot of the expected values and marginal effects. The grey lines show the orginial logit estimates.

Importantly, the cloglog model is not symmetric (like logit and probit) around 0.5.

\includegraphics{05-02-models-of-binary-outcomes_files/figure-latex/unnamed-chunk-24-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{AIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ AIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(AIC),}
         \AttributeTok{akiaike\_weights =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df    AIC diff_min akiaike_weights
## fit       11 111876    0.000    1.000000e+00
## fit_logit  6 112929 1053.012   2.194214e-229
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{BIC}\NormalTok{(fit, fit\_logit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff\_min =}\NormalTok{ BIC }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(BIC),}
         \AttributeTok{post\_prob =} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{0.5}\SpecialCharTok{*}\NormalTok{diff\_min)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           df      BIC diff_min     post_prob
## fit       11 111980.6    0.000  1.000000e+00
## fit_logit  6 112986.1 1005.464 4.637491e-219
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"abhlth"}\NormalTok{, }\StringTok{"abpoor"}\NormalTok{, }\StringTok{"absingle"}\NormalTok{, }\StringTok{"abnomore"}\NormalTok{, }\StringTok{"abdefect"}\NormalTok{, }\StringTok{"abany"}\NormalTok{, }\StringTok{"abrape"}\NormalTok{, }\StringTok{"sexlty"}\NormalTok{,}
   \StringTok{"relgty"}\NormalTok{, }\StringTok{"sprolfe"}\NormalTok{, }\StringTok{"sprochce"}\NormalTok{, }\StringTok{"abstrgth"}\NormalTok{, }\StringTok{"abimp"}\NormalTok{, }\StringTok{"abinfo"}\NormalTok{, }\StringTok{"abfirm"}\NormalTok{, }
   \StringTok{"abcare"}\NormalTok{, }\StringTok{"abproct"}\NormalTok{, }\StringTok{"abconct"}\NormalTok{, }\StringTok{"choice"}\NormalTok{, }\StringTok{"mhealth"}\NormalTok{, }\StringTok{"chealth"}\NormalTok{, }\StringTok{"prolife"}\NormalTok{, }
   \StringTok{"fechld"}\NormalTok{, }\StringTok{"fefam"}\NormalTok{, }\StringTok{"fehelp"}\NormalTok{, }\StringTok{"fepresch"}\NormalTok{, }\StringTok{"pill"}\NormalTok{, }\StringTok{"teenpill"}\NormalTok{, }\StringTok{"pillok"}\NormalTok{, }
   \StringTok{"sexeduc"}\NormalTok{, }\StringTok{"divlaw"}\NormalTok{, }\StringTok{"premarsx"}\NormalTok{, }\StringTok{"teensex"}\NormalTok{, }\StringTok{"xmarsex"}\NormalTok{, }\StringTok{"homosex"}\NormalTok{, }\StringTok{"black"}\NormalTok{,  }
   \StringTok{"male"}\NormalTok{, }\StringTok{"prot"}\NormalTok{, }\StringTok{"cath"}\NormalTok{, }\StringTok{"jew"}\NormalTok{, }\StringTok{"reliten"}\NormalTok{, }\StringTok{"attend"}\NormalTok{, }\StringTok{"prayer"}\NormalTok{,}
   \StringTok{"eraread"}\NormalTok{, }\StringTok{"erameans"}\NormalTok{, }\StringTok{"era"}\NormalTok{, }\StringTok{"eratell"}\NormalTok{)}
\NormalTok{ab }\OtherTok{\textless{}{-}} \FunctionTok{read.fwf}\NormalTok{(}\StringTok{"data/ABGSS.DAT"}\NormalTok{, }
               \AttributeTok{widths =} \FunctionTok{list}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{17}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{15}\NormalTok{, }\DecValTok{13}\NormalTok{)),}
               \AttributeTok{col.names =}\NormalTok{ col\_names) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\AttributeTok{.cols =} \FunctionTok{everything}\NormalTok{(), }\SpecialCharTok{\textasciitilde{}} \FunctionTok{na\_if}\NormalTok{(.,  }\DecValTok{99999}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,860
## Columns: 47
## $ abhlth   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, NA, 1, 1, 1, 1, NA,~
## $ abpoor   <dbl> 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, NA, ~
## $ absingle <dbl> 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, NA, 1, 0, 1, 1, 1, 1, NA,~
## $ abnomore <dbl> 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, NA, ~
## $ abdefect <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, NA, ~
## $ abany    <dbl> 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, NA, ~
## $ abrape   <dbl> 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, NA, ~
## $ sexlty   <dbl> 2.11836, 1.96270, 1.94409, 1.87156, NA, 2.58640, 2.45286, 1.7~
## $ relgty   <dbl> 0.48873, 0.42645, 0.23033, 0.20373, 0.94325, 0.48873, 0.23033~
## $ sprolfe  <dbl> 1.2457, 1.2457, 1.2457, 1.2457, 0.0000, 2.4949, 1.8703, 1.870~
## $ sprochce <dbl> 0.5049, 0.2989, 0.5282, 0.6965, 0.0000, 0.6965, 0.3366, 0.330~
## $ abstrgth <dbl> -0.09128, 0.47005, 1.30473, 1.08312, NA, 1.05940, 0.77455, 0.~
## $ abimp    <dbl> 0.00, 0.67, 0.67, 1.00, NA, 0.67, 0.33, 0.33, 0.00, 0.33, 0.6~
## $ abinfo   <dbl> 0.00, 0.00, 1.00, 0.33, NA, 0.67, 0.67, 0.33, 1.00, 1.00, 1.0~
## $ abfirm   <dbl> 0.33, 0.33, 0.00, 0.00, NA, 0.00, 0.00, 0.00, 0.00, 0.00, 0.0~
## $ abcare   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ abproct  <dbl> 3, 2, 2, 3, 0, 3, 2, 2, 3, 2, 1, 3, 0, 1, 0, 3, 1, 2, 0, 0, 2~
## $ abconct  <dbl> 1, 1, 1, 1, 0, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2~
## $ choice   <dbl> 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1~
## $ mhealth  <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0~
## $ chealth  <dbl> 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ prolife  <dbl> 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1~
## $ fechld   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ fefam    <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ fehelp   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ fepresch <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ pill     <dbl> 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1, 0,~
## $ teenpill <dbl> 1, 1, 1, 1, NA, 1, 1, 1, 0, 1, NA, 1, 1, 1, 1, 1, 1, 1, NA, 0~
## $ pillok   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ sexeduc  <dbl> 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1~
## $ divlaw   <dbl> 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.5, 1~
## $ premarsx <dbl> 1.00, 1.00, 1.00, 0.67, 0.00, 1.00, 1.00, 0.67, 0.33, NA, 0.0~
## $ teensex  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~
## $ xmarsex  <dbl> 0.33, 0.00, 0.00, 0.33, 0.00, 1.00, 0.33, 0.00, 0.00, 0.33, 0~
## $ homosex  <dbl> 0.00, 0.00, 0.67, 0.67, 0.00, 1.00, 1.00, 0.00, 0.00, 0.00, 0~
## $ black    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ male     <dbl> 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0~
## $ prot     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0~
## $ cath     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1~
## $ jew      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~
## $ reliten  <dbl> 0.5, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0~
## $ attend   <dbl> 0.250, 0.500, 0.250, 0.250, 0.500, 0.250, 0.250, 0.875, 0.875~
## $ prayer   <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0~
## $ eraread  <dbl> 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
## $ erameans <dbl> 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, ~
## $ era      <dbl> 1.00, 0.33, 0.67, 0.67, NA, 0.67, 0.67, 0.00, 0.67, 0.67, 0.6~
## $ eratell  <dbl> NA, NA, NA, NA, 0.67, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_probit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(abhlth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ cath }\SpecialCharTok{+}\NormalTok{ reliten }\SpecialCharTok{+}\NormalTok{ attend }\SpecialCharTok{+}\NormalTok{ erameans }\SpecialCharTok{+}\NormalTok{ era, }
                  \AttributeTok{data =}\NormalTok{ ab, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{))}

\FunctionTok{library}\NormalTok{(glmx)}
\NormalTok{fit\_hetprobit }\OtherTok{\textless{}{-}} \FunctionTok{hetglm}\NormalTok{(abhlth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ cath }\SpecialCharTok{+}\NormalTok{ reliten }\SpecialCharTok{+}\NormalTok{ attend }\SpecialCharTok{+}\NormalTok{ erameans }\SpecialCharTok{+}\NormalTok{ era }\SpecialCharTok{|} 
\NormalTok{                       abproct}\SpecialCharTok{*}\NormalTok{abconct }\SpecialCharTok{+}\NormalTok{ abimp }\SpecialCharTok{+}\NormalTok{ abinfo }\SpecialCharTok{+}\NormalTok{ abfirm, }
                  \AttributeTok{data =}\NormalTok{ ab, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"probit"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} N;     }\CommentTok{// number of data items in X and Z}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} K\_x;   }\CommentTok{// number of predictors in X}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} K\_z;   }\CommentTok{// number of predictors in Z}
  \DataTypeTok{matrix}\NormalTok{[N, K\_x] X;   }\CommentTok{// predictor matrix Z}
  \DataTypeTok{matrix}\NormalTok{[N, K\_z] Z;   }\CommentTok{// predictor matrix Z}
  \DataTypeTok{int}\NormalTok{ y[N];           }\CommentTok{// outcome vector}
\NormalTok{\}}
\KeywordTok{parameters}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[K\_x] beta;       }\CommentTok{// coefficients for X}
  \DataTypeTok{vector}\NormalTok{[K\_z] gamma;       }\CommentTok{// coefficients for Z}
\NormalTok{\}}
\KeywordTok{transformed parameters}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[N] pi;}
  \DataTypeTok{vector}\NormalTok{[N] Xbeta;}
  \DataTypeTok{vector}\NormalTok{[N] Zgamma;}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N) \{}
\NormalTok{    Xbeta[i] = X[i, ]*beta;}
\NormalTok{    Zgamma[i] = Z[i, ]*gamma;}
\NormalTok{    pi[i] = Phi(Xbeta[i]/exp(Zgamma[i]));}
\NormalTok{  \}}
\NormalTok{\}}
\KeywordTok{model}\NormalTok{ \{}
\NormalTok{  y \textasciitilde{} bernoulli(pi);  }\CommentTok{// likelihood}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# listwise delete missing values from all variables of interest}
\NormalTok{ab\_stan }\OtherTok{\textless{}{-}}\NormalTok{ ab }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(abhlth, black, male, cath, reliten, attend, erameans, era, }
\NormalTok{         abproct, abconct, abimp, abinfo, abfirm) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{()}

\CommentTok{\# create X}
\NormalTok{fx }\OtherTok{\textless{}{-}}\NormalTok{ abhlth }\SpecialCharTok{\textasciitilde{}}\NormalTok{ black }\SpecialCharTok{+}\NormalTok{ male }\SpecialCharTok{+}\NormalTok{ cath }\SpecialCharTok{+}\NormalTok{ reliten }\SpecialCharTok{+}\NormalTok{ attend }\SpecialCharTok{+}\NormalTok{ erameans }\SpecialCharTok{+}\NormalTok{ era}
\NormalTok{mfx }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(fx, }\AttributeTok{data =}\NormalTok{ ab\_stan)  }\CommentTok{\# model frame}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(fx, mfx)         }\CommentTok{\# model matrix X}

\CommentTok{\# obtain the outcome variable y}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{model.response}\NormalTok{(mfx)}

\CommentTok{\# create X}
\NormalTok{fz }\OtherTok{\textless{}{-}} \ErrorTok{\textasciitilde{}}\NormalTok{ abproct}\SpecialCharTok{*}\NormalTok{abconct }\SpecialCharTok{+}\NormalTok{ abimp }\SpecialCharTok{+}\NormalTok{ abinfo }\SpecialCharTok{+}\NormalTok{ abfirm }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{mfz }\OtherTok{\textless{}{-}} \FunctionTok{model.frame}\NormalTok{(fz, }\AttributeTok{data =}\NormalTok{ ab\_stan)  }\CommentTok{\# model frame}
\NormalTok{Z }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(fz, mfz)         }\CommentTok{\# model matrix X}

\CommentTok{\# fit stan model}
\FunctionTok{library}\NormalTok{(rstan); }\FunctionTok{options}\NormalTok{(}\AttributeTok{mc.cores =}\NormalTok{ parallel}\SpecialCharTok{::}\FunctionTok{detectCores}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: StanHeaders
\end{verbatim}

\begin{verbatim}
## rstan (Version 2.21.3, GitRev: 2e1f913d3ca3)
\end{verbatim}

\begin{verbatim}
## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'rstan'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:tidyr':
## 
##     extract
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stan\_data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{X =}\NormalTok{ X, }\AttributeTok{Z =}\NormalTok{ Z,}
                  \AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(X), }\AttributeTok{K\_x =} \FunctionTok{ncol}\NormalTok{(X), }\AttributeTok{K\_z =} \FunctionTok{ncol}\NormalTok{(Z))}
\CommentTok{\# stan\_opt \textless{}{-} optimizing(hetprob\_model, data = stan\_data)  \# keeps crashing}
\NormalTok{stan\_samp }\OtherTok{\textless{}{-}}\NormalTok{ rstan}\SpecialCharTok{::}\FunctionTok{sampling}\NormalTok{(hetprob\_model, }\AttributeTok{data =}\NormalTok{ stan\_data,}
                            \AttributeTok{pars =} \FunctionTok{c}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"gamma"}\NormalTok{),}
                            \AttributeTok{cores =} \DecValTok{4}\NormalTok{,}
                            \AttributeTok{chains =} \DecValTok{4}\NormalTok{,}
                            \AttributeTok{iter =} \DecValTok{5000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}


\end{document}
