<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Wald Confidence Intervals | Statistical Modeling: A Tools Approach</title>
  <meta name="description" content="Lecture notes covering the key tools of regression modeling in political science." />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Wald Confidence Intervals | Statistical Modeling: A Tools Approach" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes covering the key tools of regression modeling in political science." />
  <meta name="github-repo" content="pos5747/adv-methods-notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Wald Confidence Intervals | Statistical Modeling: A Tools Approach" />
  
  <meta name="twitter:description" content="Lecture notes covering the key tools of regression modeling in political science." />
  

<meta name="author" content="Carlisle Rainey" />


<meta name="date" content="2022-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-nonparametric-bootstrap.html"/>
<link rel="next" href="evaluating-confidence-intervals.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>
<script>
    $(document).ready(function () {
        process_solutions();
    });
    function process_solutions() {
        $("div.section[id^='solution']").each(function(i) {
        var soln_wrapper_id = "cvxr_ex_" + i;
        var solution_id = $(this).attr('id');
        var button = $("<button onclick=\"toggle_solution('" + soln_wrapper_id + "')\">Show/Hide</button>");
        var new_div = $("<div id='" + soln_wrapper_id + "' class='solution' style='display: none;'></div>");
        var h = $(this).first();
        var others = $(this).children().slice(1);
        $(others).each(function() {
            $(this).appendTo($(new_div));
        });
        $(button).insertAfter($(h));
        $(new_div).insertAfter($(button));
        })
    }
    function toggle_solution(el_id) {
      $("#" + el_id).toggle();
    } 
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Methods Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Week 1: Maximum Likelihood<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="class-agenda.html"><a href="class-agenda.html"><i class="fa fa-check"></i><b>1.1</b> Class agenda<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html"><i class="fa fa-check"></i><b>1.2</b> Maximum Likelihood<span></span></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#example-bernoulli-distribution"><i class="fa fa-check"></i><b>1.2.1</b> Example: Bernoulli Distribution<span></span></a></li>
<li class="chapter" data-level="1.2.2" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#example-poisson-distribution"><i class="fa fa-check"></i><b>1.2.2</b> Example: Poisson Distribution<span></span></a></li>
<li class="chapter" data-level="1.2.3" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#remarks"><i class="fa fa-check"></i><b>1.2.3</b> Remarks<span></span></a></li>
<li class="chapter" data-level="1.2.4" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#example-beta-distribution"><i class="fa fa-check"></i><b>1.2.4</b> Example: Beta Distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-invariance-property.html"><a href="the-invariance-property.html"><i class="fa fa-check"></i><b>1.3</b> The Invariance Property<span></span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-invariance-property.html"><a href="the-invariance-property.html#example-bernoulli-odds"><i class="fa fa-check"></i><b>1.3.1</b> Example: Bernoulli Odds<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="the-invariance-property.html"><a href="the-invariance-property.html#example-poisson-sd"><i class="fa fa-check"></i><b>1.3.2</b> Example: Poisson SD<span></span></a></li>
<li class="chapter" data-level="1.3.3" data-path="the-invariance-property.html"><a href="the-invariance-property.html#example-beta-mean-and-variance"><i class="fa fa-check"></i><b>1.3.3</b> Example: Beta Mean and Variance<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html"><i class="fa fa-check"></i><b>1.4</b> The Parametric Bootstrap<span></span></a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html#example-toothpaste-cap-problm"><i class="fa fa-check"></i><b>1.4.1</b> Example: Toothpaste Cap Problm<span></span></a></li>
<li class="chapter" data-level="1.4.2" data-path="the-parametric-bootstrap.html"><a href="the-parametric-bootstrap.html#example-beta-distribution-1"><i class="fa fa-check"></i><b>1.4.2</b> Example: Beta Distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="sampling-distribution.html"><a href="sampling-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Sampling Distribution<span></span></a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="sampling-distribution.html"><a href="sampling-distribution.html#example-the-toothpaste-cap-problem"><i class="fa fa-check"></i><b>1.5.1</b> Example: The Toothpaste Cap Problem<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bias.html"><a href="bias.html"><i class="fa fa-check"></i><b>1.6</b> Bias<span></span></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="bias.html"><a href="bias.html#example-bernoulli-distribution-1"><i class="fa fa-check"></i><b>1.6.1</b> Example: Bernoulli Distribution<span></span></a></li>
<li class="chapter" data-level="1.6.2" data-path="bias.html"><a href="bias.html#example-poisson-distribution-1"><i class="fa fa-check"></i><b>1.6.2</b> Example: Poisson Distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="consistency.html"><a href="consistency.html"><i class="fa fa-check"></i><b>1.7</b> Consistency<span></span></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="consistency.html"><a href="consistency.html#example-illustrative"><i class="fa fa-check"></i><b>1.7.1</b> Example: Illustrative<span></span></a></li>
<li class="chapter" data-level="1.7.2" data-path="consistency.html"><a href="consistency.html#example-bernoulli-odds-1"><i class="fa fa-check"></i><b>1.7.2</b> Example: Bernoulli Odds<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="predictive-distribution.html"><a href="predictive-distribution.html"><i class="fa fa-check"></i><b>1.8</b> Predictive Distribution<span></span></a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="predictive-distribution.html"><a href="predictive-distribution.html#example-poisson-distribution-2"><i class="fa fa-check"></i><b>1.8.1</b> Example: Poisson Distribution<span></span></a></li>
<li class="chapter" data-level="1.8.2" data-path="predictive-distribution.html"><a href="predictive-distribution.html#example-beta-distribution-2"><i class="fa fa-check"></i><b>1.8.2</b> Example: Beta Distribution<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="week-2-bayesian-inference.html"><a href="week-2-bayesian-inference.html"><i class="fa fa-check"></i><b>2</b> Week 2: Bayesian Inference<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>2.1</b> Bayesian Inference<span></span></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#mechanics"><i class="fa fa-check"></i><b>2.1.1</b> Mechanics<span></span></a></li>
<li class="chapter" data-level="2.1.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-summaries"><i class="fa fa-check"></i><b>2.1.2</b> Posterior Summaries<span></span></a></li>
<li class="chapter" data-level="2.1.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-simulation"><i class="fa fa-check"></i><b>2.1.3</b> Posterior Simulation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="example-bernoulli.html"><a href="example-bernoulli.html"><i class="fa fa-check"></i><b>2.2</b> Example: Bernoulli<span></span></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="example-bernoulli.html"><a href="example-bernoulli.html#likelihood"><i class="fa fa-check"></i><b>2.2.1</b> The Likelihood<span></span></a></li>
<li class="chapter" data-level="2.2.2" data-path="example-bernoulli.html"><a href="example-bernoulli.html#the-prior"><i class="fa fa-check"></i><b>2.2.2</b> The Prior<span></span></a></li>
<li class="chapter" data-level="2.2.3" data-path="example-bernoulli.html"><a href="example-bernoulli.html#the-posterior"><i class="fa fa-check"></i><b>2.2.3</b> The Posterior<span></span></a></li>
<li class="chapter" data-level="2.2.4" data-path="example-bernoulli.html"><a href="example-bernoulli.html#point-estimates"><i class="fa fa-check"></i><b>2.2.4</b> Point Estimates<span></span></a></li>
<li class="chapter" data-level="2.2.5" data-path="example-bernoulli.html"><a href="example-bernoulli.html#credible-interval"><i class="fa fa-check"></i><b>2.2.5</b> Credible Interval<span></span></a></li>
<li class="chapter" data-level="2.2.6" data-path="example-bernoulli.html"><a href="example-bernoulli.html#simulation"><i class="fa fa-check"></i><b>2.2.6</b> Simulation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="example-poisson-distribution-3.html"><a href="example-poisson-distribution-3.html"><i class="fa fa-check"></i><b>2.3</b> Example: Poisson Distribution<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="remarks-1.html"><a href="remarks-1.html"><i class="fa fa-check"></i><b>2.4</b> Remarks<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="week-3-adding-predictors.html"><a href="week-3-adding-predictors.html"><i class="fa fa-check"></i><b>3</b> Week 3: Adding Predictors<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="review-the-normal-model.html"><a href="review-the-normal-model.html"><i class="fa fa-check"></i><b>3.1</b> Review: The Normal Model<span></span></a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="review-the-normal-model.html"><a href="review-the-normal-model.html#distribution"><i class="fa fa-check"></i><b>3.1.1</b> Distribution<span></span></a></li>
<li class="chapter" data-level="3.1.2" data-path="review-the-normal-model.html"><a href="review-the-normal-model.html#linear-predictor"><i class="fa fa-check"></i><b>3.1.2</b> Linear Predictor<span></span></a></li>
<li class="chapter" data-level="3.1.3" data-path="review-the-normal-model.html"><a href="review-the-normal-model.html#fitting-the-normal-linear-model"><i class="fa fa-check"></i><b>3.1.3</b> Fitting the Normal-Linear Model<span></span></a></li>
<li class="chapter" data-level="3.1.4" data-path="review-the-normal-model.html"><a href="review-the-normal-model.html#applied-example"><i class="fa fa-check"></i><b>3.1.4</b> Applied Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="bernoulli-model.html"><a href="bernoulli-model.html"><i class="fa fa-check"></i><b>3.2</b> Bernoulli Model<span></span></a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bernoulli-model.html"><a href="bernoulli-model.html#the-linear-probability-model"><i class="fa fa-check"></i><b>3.2.1</b> The Linear Probability Model<span></span></a></li>
<li class="chapter" data-level="3.2.2" data-path="bernoulli-model.html"><a href="bernoulli-model.html#the-logit-model"><i class="fa fa-check"></i><b>3.2.2</b> The Logit Model<span></span></a></li>
<li class="chapter" data-level="3.2.3" data-path="bernoulli-model.html"><a href="bernoulli-model.html#fitting-a-logit-model"><i class="fa fa-check"></i><b>3.2.3</b> Fitting a Logit Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>3.3</b> Poisson Model<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="poisson-model.html"><a href="poisson-model.html#predictive-distribution-1"><i class="fa fa-check"></i><b>3.3.1</b> Predictive Distribution<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="poisson-model.html"><a href="poisson-model.html#posterior-predictive-distribution"><i class="fa fa-check"></i><b>3.3.2</b> Posterior Predictive Distribution<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="posterior-predictive-distribution-1.html"><a href="posterior-predictive-distribution-1.html"><i class="fa fa-check"></i><b>3.4</b> [Posterior] Predictive Distribution<span></span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="posterior-predictive-distribution-1.html"><a href="posterior-predictive-distribution-1.html#for-the-logit-model"><i class="fa fa-check"></i><b>3.4.1</b> … for the logit model<span></span></a></li>
<li class="chapter" data-level="3.4.2" data-path="posterior-predictive-distribution-1.html"><a href="posterior-predictive-distribution-1.html#for-the-poisson-model"><i class="fa fa-check"></i><b>3.4.2</b> … for the Poisson model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="quantities-of-interest.html"><a href="quantities-of-interest.html"><i class="fa fa-check"></i><b>3.5</b> Quantities of Interest<span></span></a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="quantities-of-interest.html"><a href="quantities-of-interest.html#expected-value"><i class="fa fa-check"></i><b>3.5.1</b> Expected Value<span></span></a></li>
<li class="chapter" data-level="3.5.2" data-path="quantities-of-interest.html"><a href="quantities-of-interest.html#first-difference"><i class="fa fa-check"></i><b>3.5.2</b> First Difference<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="week-4-confidence-intervals.html"><a href="week-4-confidence-intervals.html"><i class="fa fa-check"></i><b>4</b> Week 4: Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-parametric-bootstrap-1.html"><a href="the-parametric-bootstrap-1.html"><i class="fa fa-check"></i><b>4.1</b> The Parametric Bootstrap<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="the-nonparametric-bootstrap.html"><a href="the-nonparametric-bootstrap.html"><i class="fa fa-check"></i><b>4.2</b> The <em>Non</em>parametric Bootstrap<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="the-nonparametric-bootstrap.html"><a href="the-nonparametric-bootstrap.html#example-coefficients-from-the-civilian-casualties-model"><i class="fa fa-check"></i><b>4.2.1</b> Example: Coefficients from the Civilian Casualties Model<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="the-nonparametric-bootstrap.html"><a href="the-nonparametric-bootstrap.html#example-first-difference-from-the-civilian-casualties-model"><i class="fa fa-check"></i><b>4.2.2</b> Example: First Difference from the Civilian Casualties Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="wald-confidence-intervals.html"><a href="wald-confidence-intervals.html"><i class="fa fa-check"></i><b>4.3</b> Wald Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="wald-confidence-intervals.html"><a href="wald-confidence-intervals.html#curvature-in-a-single-dimmension"><i class="fa fa-check"></i><b>4.3.1</b> Curvature in a Single Dimmension<span></span></a></li>
<li class="chapter" data-level="4.3.2" data-path="wald-confidence-intervals.html"><a href="wald-confidence-intervals.html#curvature-in-a-multiple-dimmensions"><i class="fa fa-check"></i><b>4.3.2</b> Curvature in a Multiple Dimmensions<span></span></a></li>
<li class="chapter" data-level="4.3.3" data-path="wald-confidence-intervals.html"><a href="wald-confidence-intervals.html#from-curvature-to-confidence-intervals"><i class="fa fa-check"></i><b>4.3.3</b> From Curvature to Confidence Intervals<span></span></a></li>
<li class="chapter" data-level="4.3.4" data-path="wald-confidence-intervals.html"><a href="wald-confidence-intervals.html#final-notes"><i class="fa fa-check"></i><b>4.3.4</b> Final Notes<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="evaluating-confidence-intervals.html"><a href="evaluating-confidence-intervals.html"><i class="fa fa-check"></i><b>4.4</b> Evaluating Confidence Intervals<span></span></a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="evaluating-confidence-intervals.html"><a href="evaluating-confidence-intervals.html#coverage"><i class="fa fa-check"></i><b>4.4.1</b> Coverage<span></span></a></li>
<li class="chapter" data-level="4.4.2" data-path="evaluating-confidence-intervals.html"><a href="evaluating-confidence-intervals.html#monte-carlo-simulation-to-assess-coverage"><i class="fa fa-check"></i><b>4.4.2</b> Monte Carlo Simulation to Assess Coverage<span></span></a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Modeling: A Tools Approach</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="wald-confidence-intervals" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Wald Confidence Intervals<a href="wald-confidence-intervals.html#wald-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can easily use the log-likelihood function to obtain point estimates. It turns out, though, that this same log-likelihood function contains information that helps use estimate the <em>precision</em> of those estimates as well.</p>
<p>As an example, consider the following two log-likelihood functions:</p>
<p><img src="04-02-wald-cis_files/figure-html/unnamed-chunk-2-1.png" width="480" />
Which of these two log-likelihood functions do you think provides a more <em>precise</em> estimate?</p>
<p><em>Note: These likelihoods are from a normal model with unknown mean. I simulated 100 observations for <span class="math inline">\(y_1\)</span> and 500 observations for <span class="math inline">\(y_2\)</span>. (I centered the data so the sample means both occurred exactly at two.</em></p>
<p><strong>Key Idea</strong>: We can use the curvature around the maximum likelihood estimate to get a sense of the uncertainty.</p>
<p>What quantity tells us about the amount of curvature at the maximum? The second derivative. As the second derivative goes down, the curvature goes up.
As the curvature goes up, the uncertainty goes down.</p>
<div id="curvature-in-a-single-dimmension" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Curvature in a Single Dimmension<a href="wald-confidence-intervals.html#curvature-in-a-single-dimmension" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To develop our intuition about “curvature” and confidence intervals, I analyze the <em>Stylized Normal Model</em> (<span class="math inline">\(\sigma = 1\)</span>). Here, we model the data as a normal distribution with <span class="math inline">\(\mu\)</span> unknown (and to be estimated), but <span class="math inline">\(\sigma = 1\)</span> (known; not estimated). That is, <span class="math inline">\(y \sim N(\mu, 1)\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\log \mathcal{L}(\mu) &amp;= -\frac{n}{2\pi} - \frac{1}{2}\sum_{i = 1}^n (y_i - \mu)^2\\
\dfrac{\partial \log \mathcal{L}(\mu)}{\partial \mu} &amp;= \sum_{i = 1}^n y_i - \mu n\\
\dfrac{\partial^2 \log \mathcal{L}(\mu)}{\partial^2 \mu} &amp;=  - n
\end{aligned}
\]</span></p>
<p>Facts:</p>
<ul>
<li>As <span class="math inline">\(n\)</span> increases, <span class="math inline">\(\frac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu}\)</span> becomes more negative.</li>
<li>As <span class="math inline">\(\frac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu}\)</span> gets more negative, the curvature increases.</li>
<li>As the curvature increases, the uncertainty decreases.</li>
</ul>
<p>Wouldn’t it be really nice if we could use <span class="math inline">\(\frac{\partial^2 \log \mathcal{L}(\mu)}{\partial^2 \mu}\)</span> to estimate the standard error?</p>
<p>It turns out that this quantity is a direct, almost magically intuitive estimator of the standard error.</p>
<p>In the single parameter case, we have the following approximation.</p>
<p><span class="math display">\[
\widehat{\text{Var}}(\hat{\theta}) \approx \left[\left. - \frac{\partial^2 \log \mathcal{L}(\theta)}{\partial^2 \theta}\right| _{\theta = \hat{\theta}}\right] ^{-1}
\]</span></p>
<p>I should be careful here. This is an asymptotic result. As the sample size grows, the variance of <span class="math inline">\(\hat{\beta}\)</span> gets closer and closer to <span class="math inline">\(\left[\left. - \frac{\partial^2 \log \mathcal{L}(\theta)}{\partial^2 \theta}\right| _{\theta = \hat{\theta}}\right] ^{-1}\)</span>. I’m interpreting this large sample result as a small sample approximation.</p>
<p>This mean that we find the second derivative, evaluate it at the maximum (<span class="math inline">\(\theta = \hat{\theta}\)</span>), and find the inverse (<span class="math inline">\(-1\)</span>). That’s an estimate of the variance. To convert it to a standard error, just take the square root.</p>
<p><span class="math display">\[
\widehat{\text{SE}}(\hat{\theta}) \approx \sqrt{\left[\left. - \frac{\partial^2 \log \mathcal{L}(\theta)}{\partial^2 \theta}\right| _{\theta = \hat{\theta}}\right] ^{-1}}
\]</span>
If we continue the stylized normal example, we have the following.</p>
<p><span class="math display">\[
\begin{equation*}
\dfrac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu} =  - n
~{\color{purple}{\Longrightarrow}}~
\left[\left. - \frac{\partial^2 \log \mathcal{L}(\mu | y)}{\partial^2 \mu}\right| _{\mu = \hat{\mu}}\right] ^{-1}
= \dfrac{1}{n}
\approx \widehat{\text{Var}}(\hat{\mu})
\end{equation*}
\]</span></p>
<p>And then</p>
<p><span class="math display">\[
\begin{equation*}
\widehat{\text{SE}}(\hat{\mu}) \approx \sqrt{\dfrac{1}{n}}
\end{equation*}
\]</span>
Does this answer make sense? What is the standard error of the mean from Methods II? Hint: It’s <span class="math inline">\(\text{SE}[\text{avg}(y)] \approx \sqrt{\dfrac{\text{population SD}}{n}}\)</span>. In this case, the “population SD” is <span class="math inline">\(\sigma = 1\)</span>, as assumed by the <em>stylized</em> normal model.</p>
</div>
<div id="curvature-in-a-multiple-dimmensions" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Curvature in a Multiple Dimmensions<a href="wald-confidence-intervals.html#curvature-in-a-multiple-dimmensions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To add multiple dimensions, let’s consider the beta model from Week 1. Here, we assume that <span class="math inline">\(y \sim \text{Beta}(\alpha, \beta)\)</span>, and our goal is to estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. The key is that we have <em>multiple</em> (i.e., two) parameters to estimate.</p>
<p>It’s a bit trickier to think about curvature in multiple dimensions.</p>
<p>Here’s what the log-likelihood function might look like for a give data set.</p>
<p><img src="04-02-wald-cis_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>To make more sense, of this 3D plot, let’s look at the contour plot.</p>
<p><img src="04-02-wald-cis_files/figure-html/unnamed-chunk-4-1.png" width="336" /></p>
<p>The curvature around the maximum <em>vertically</em> tells use the variance in <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><img src="04-02-wald-cis_files/figure-html/unnamed-chunk-5-1.png" width="336" /></p>
<p>The curvature around the maximum <em>horizontally</em> tells use the variance in <span class="math inline">\(\hat{\alpha}\)</span>.</p>
<p><img src="04-02-wald-cis_files/figure-html/unnamed-chunk-6-1.png" width="336" /></p>
<p>But there’s a third direction that’s relevant here: the curvature <em>diagonally</em>. The diagonal curvature tells us the covariance of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>. That is, if we <em>over</em>-estimate <span class="math inline">\(\alpha\)</span>, how much do we tend to over- (or under-)estimate <span class="math inline">\(\beta\)</span>?</p>
<p><img src="04-02-wald-cis_files/figure-html/unnamed-chunk-7-1.png" width="336" /></p>
<p>Rather than a single variance, we get a variance <strong>matrix</strong> (sometimes called the “covariance matrix” or the “variance-covariance matrix”.</p>
<p><span class="math display">\[
\begin{equation*}
\widehat{\text{Var}}(\hat{\theta})= \widehat{\text{Cov}}(\hat{\theta}) \approx \left. \left[
\displaystyle \begin{matrix}
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1} &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2}\\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1} &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}
\end{equation*}
\]</span>
The elements along the diagonal (in red) are the variances for each parameter, so the square root of the diagonal gives you the standard errors. This is exactly what we’d expect.</p>
<p><span class="math display">\[
\begin{equation*}
\widehat{\text{Var}}(\hat{\theta}) \approx \left. \left[
\displaystyle \begin{matrix}
\color{red}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1}} &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2}\\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1} &amp; \color{red}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2}}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}
\end{equation*}
\]</span></p>
<p>The off-diagonal elements (in blue) are the covariances–they’ll be really important to us later, but we don’t have a direct use for them at the moment.</p>
<p><span class="math display">\[
\begin{equation*}
\widehat{\text{Var}}(\hat{\theta}) \approx \left. \left[
\displaystyle \begin{matrix}
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1} &amp; \color{blue}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2}}\\
\color{blue}{- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1}} &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}
\end{equation*}
\]</span></p>
<p>But what about more than two parameters? It’s exactly what you’d expect. We call this matrix of second-derivatives the “information matrix” <span class="math inline">\(\mathcal{I}(\theta)\)</span>. When evaluated at the ML estimate, we call it the “observed information matrix” <span class="math inline">\(\mathcal{I}(\hat{\theta})\)</span>.</p>
<p><span class="math display">\[
\begin{equation*}
\begin{aligned}
\widehat{\text{Var}}(\hat{\theta}) &amp;\approx \left. \left[
\displaystyle \begin{matrix}
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_1} &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_2} &amp; \ldots &amp;- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_1 \partial \theta_k}\\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_1} &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_2} &amp; \ldots &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_2 \partial \theta_k}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
- \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_k \partial \theta_1}     &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial \theta_k \partial \theta_2} &amp; \ldots &amp; - \frac{\partial^2 \log \mathcal{L}(\theta| y)}{\partial^2 \theta_k}\\
\end{matrix}\right]^{-1} \right|_{\theta = \hat{\theta}}\\
&amp; \approx \mathcal{I}(\theta)^{-1}|_{\theta = \hat{\theta}}\\
&amp;\approx \mathcal{I}(\hat{\theta})^{-1}
\end{aligned}
\end{equation*}
\]</span></p>
</div>
<div id="from-curvature-to-confidence-intervals" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> From Curvature to Confidence Intervals<a href="wald-confidence-intervals.html#from-curvature-to-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To convert this variance estimate into a confidence interval, we need the following large sample result. It turns out that, as the sample size grows large, the ML estimate converges to a normally distributed random variable with mean <span class="math inline">\(theta_{true}\)</span> and variance <span class="math inline">\(\mathcal{I}(\theta_{true})^{-1}\)</span>..</p>
<p><strong>key fact</strong>: <span class="math inline">\(\hat{\theta} \overset{a}{\sim} N\left[ \theta_{true}, \mathcal{I}(\theta_{true})^{-1}\right]\)</span></p>
<p>In practice, we’ll take this to mean it’s <em>approximately</em> normal.</p>
<p><span class="math display">\[
\begin{align*}
90\%~\text{C.I.}  &amp;\approx \hat{\theta} \pm 1.64\dfrac{1}{\sqrt{\mathcal{I}(\hat{\theta})}}\\
95\%~\text{C.I.}  &amp;\approx \hat{\theta} \pm 1.96\dfrac{1}{\sqrt{\mathcal{I}(\hat{\theta})}}
\end{align*}
\]</span></p>
<p>To work with these intervals, then, we just need the variance matrix <span class="math inline">\(\widehat{\text{Var}}(\hat{\theta}) = \mathcal{I}(\hat{\theta})^{-1}\)</span>. Much like we can access the ML estimates of the model coefficients <span class="math inline">\(\hat{\beta}\)</span> with <code>coef()</code>, we can access <span class="math inline">\(\widehat{\text{Var}}(\hat{\theta})\)</span>.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="wald-confidence-intervals.html#cb230-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load hks data</span></span>
<span id="cb230-2"><a href="wald-confidence-intervals.html#cb230-2" aria-hidden="true" tabindex="-1"></a>hks <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/hks.csv&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb230-3"><a href="wald-confidence-intervals.html#cb230-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span>
<span id="cb230-4"><a href="wald-confidence-intervals.html#cb230-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-5"><a href="wald-confidence-intervals.html#cb230-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fit poisson regression model</span></span>
<span id="cb230-6"><a href="wald-confidence-intervals.html#cb230-6" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> osvAll <span class="sc">~</span> troopLag <span class="sc">+</span> policeLag <span class="sc">+</span> militaryobserversLag <span class="sc">+</span> </span>
<span id="cb230-7"><a href="wald-confidence-intervals.html#cb230-7" aria-hidden="true" tabindex="-1"></a>  brv_AllLag <span class="sc">+</span> osvAllLagDum <span class="sc">+</span> incomp <span class="sc">+</span> epduration <span class="sc">+</span> </span>
<span id="cb230-8"><a href="wald-confidence-intervals.html#cb230-8" aria-hidden="true" tabindex="-1"></a>  lntpop</span>
<span id="cb230-9"><a href="wald-confidence-intervals.html#cb230-9" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(f, <span class="at">data =</span> hks, <span class="at">family =</span> poisson)</span>
<span id="cb230-10"><a href="wald-confidence-intervals.html#cb230-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-11"><a href="wald-confidence-intervals.html#cb230-11" aria-hidden="true" tabindex="-1"></a><span class="co"># compute 90% confidence intervals</span></span>
<span id="cb230-12"><a href="wald-confidence-intervals.html#cb230-12" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)</span>
<span id="cb230-13"><a href="wald-confidence-intervals.html#cb230-13" aria-hidden="true" tabindex="-1"></a>var_hat <span class="ot">&lt;-</span> <span class="fu">vcov</span>(fit)</span>
<span id="cb230-14"><a href="wald-confidence-intervals.html#cb230-14" aria-hidden="true" tabindex="-1"></a>se_hat <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(var_hat))  <span class="co"># keep only the diagonal elements</span></span>
<span id="cb230-15"><a href="wald-confidence-intervals.html#cb230-15" aria-hidden="true" tabindex="-1"></a>ci_lwr <span class="ot">&lt;-</span> beta_hat <span class="sc">-</span> <span class="fl">1.64</span><span class="sc">*</span>se_hat</span>
<span id="cb230-16"><a href="wald-confidence-intervals.html#cb230-16" aria-hidden="true" tabindex="-1"></a>ci_upr <span class="ot">&lt;-</span> beta_hat <span class="sc">+</span> <span class="fl">1.64</span><span class="sc">*</span>se_hat</span>
<span id="cb230-17"><a href="wald-confidence-intervals.html#cb230-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-18"><a href="wald-confidence-intervals.html#cb230-18" aria-hidden="true" tabindex="-1"></a><span class="co"># make a nice table</span></span>
<span id="cb230-19"><a href="wald-confidence-intervals.html#cb230-19" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(<span class="st">`</span><span class="at">Variable</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">names</span>(beta_hat),</span>
<span id="cb230-20"><a href="wald-confidence-intervals.html#cb230-20" aria-hidden="true" tabindex="-1"></a>       <span class="st">`</span><span class="at">Coefficient Estimate</span><span class="st">`</span> <span class="ot">=</span> scales<span class="sc">::</span><span class="fu">number</span>(beta_hat, <span class="fl">0.001</span>),</span>
<span id="cb230-21"><a href="wald-confidence-intervals.html#cb230-21" aria-hidden="true" tabindex="-1"></a>       <span class="st">`</span><span class="at">90% Confidence Interval</span><span class="st">`</span> <span class="ot">=</span> <span class="fu">paste0</span>(<span class="st">&quot;[&quot;</span>, scales<span class="sc">::</span><span class="fu">number</span>(ci_lwr, <span class="fl">0.001</span>), <span class="st">&quot;,&quot;</span>, scales<span class="sc">::</span><span class="fu">number</span>(ci_upr, <span class="fl">0.001</span>), <span class="st">&quot;]&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb230-22"><a href="wald-confidence-intervals.html#cb230-22" aria-hidden="true" tabindex="-1"></a>  kableExtra<span class="sc">::</span><span class="fu">kable</span>(<span class="at">format =</span> <span class="st">&quot;markdown&quot;</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Coefficient Estimate</th>
<th align="left">90% Confidence Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="left">-3.579</td>
<td align="left">[-3.649,-3.509]</td>
</tr>
<tr class="even">
<td align="left">troopLag</td>
<td align="left">-0.170</td>
<td align="left">[-0.173,-0.167]</td>
</tr>
<tr class="odd">
<td align="left">policeLag</td>
<td align="left">-3.272</td>
<td align="left">[-3.313,-3.232]</td>
</tr>
<tr class="even">
<td align="left">militaryobserversLag</td>
<td align="left">8.100</td>
<td align="left">[8.080,8.120]</td>
</tr>
<tr class="odd">
<td align="left">brv_AllLag</td>
<td align="left">0.001</td>
<td align="left">[0.001,0.001]</td>
</tr>
<tr class="even">
<td align="left">osvAllLagDum</td>
<td align="left">0.291</td>
<td align="left">[0.283,0.299]</td>
</tr>
<tr class="odd">
<td align="left">incomp</td>
<td align="left">3.486</td>
<td align="left">[3.457,3.516]</td>
</tr>
<tr class="even">
<td align="left">epduration</td>
<td align="left">-0.022</td>
<td align="left">[-0.022,-0.022]</td>
</tr>
<tr class="odd">
<td align="left">lntpop</td>
<td align="left">0.189</td>
<td align="left">[0.186,0.193]</td>
</tr>
</tbody>
</table>
<p>Compare these confidence intervals to the parametric and nonparametric intervals from the previous section.</p>
</div>
<div id="final-notes" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Final Notes<a href="wald-confidence-intervals.html#final-notes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>The Wald confidence interval does not easily extend to quantities of interest. It turns out that their is a way, called the “delta method.” It’s a bit tedious and not necessary since we have other methods. But I’ll mention it here: “the delta method.” King, Tomz, and Wittenberg (2001) give us an easy alternative to the delta method; we’ll see their method in a couple of weeks.</li>
<li>If you use <code>optim()</code> to find the ML estimates, then you can have it return the observed information matrix <span class="math inline">\(\mathcal{I}(\hat{\theta})\)</span> to you by supplying the argument <code>hession = TRUE</code> to <code>optim()</code>. <code>optim()</code> returns a list; the component named <code>"hessian"</code> is the Hessian matrix. You simple need to find the inverse of the negative of the Hessian to obtain the estimated variance matrix. Something like <code>est &lt;- optim(..., hessian = TRUE)</code> followed by <code>var_hat &lt;- solve(-est$hessian)</code>.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-nonparametric-bootstrap.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="evaluating-confidence-intervals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["adv-methods-notes.pdf", "adv-methods-notes.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
