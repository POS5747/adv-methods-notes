
# Week 5: Models of Binary Outcomes and Model Fit Summaries

This week, we have two goals.

1. Develop out measures of model fit beyond the predictive distribution, adding cross validation and information criteria to our tool kit.
1. Modify the usual logit model in subtle and not-so-subtle ways to expand our options for modeling binary data


```{r include=FALSE}
library(tidyverse)
```

## Measures of Model Fit 

### Scoring Binary Predictions

Suppose you have a binary outcome $y_i$ for $i = \{1, 2, ..., n\}$ and you develop a set of predictions for each outcome in the from of probabilities $p_i$ for $i = \{1, 2, ..., n\}$ and a competitor develops the set $q_i$ for $i = \{1, 2, ..., n\}$. Intuitively, if the $p_i$s are "closer" to the $y_i$s than the $q_i$s, then the $p_i$s are a better prediction. By extension, the model that produced the $p_i$s is a better model than the model that produced the $q_i$s.

But we need a formal rule for defining what we mean by "closer." There are two common scoring rules at the level of the individual predictions.

1. **Brier Score** The Brier score is squared error of the prediction $p_i$ and the outcome $y_i$, so that $\text{Brier Score_i} =  (p_i -y_i)^2$. This is analogous to linear regression, where we minimize the RMS of the residuals.
1. **Log Score** The log score is the logarithm of the probabilities assigned to the event that occurred. This can be awkward to interpret, since better predictions produce *less negative* values. Therefore, it's common to multiply log scores by $-1$. In practice, we tend to we can compute this as $\text{Log Score}_i =  - [y_i \log(p_i) + (1 - y_i) \log (1 - p_i)]$

To aggregate the scores across the observations, we can use a simple average for both the Brier and log scores.

To see these scoring rules in action, let's fit fit the familiar logit model to data from [Krupnikov (2011)](https://onlinelibrary.wiley.com/doi/10.1111/j.1540-5907.2011.00522.x). In this *AJPS* article, she argues that late campaign negativity targeted toward a liked candidate demobilizes voters while other forms of negativity do not. 

She concludes: 

> ...the substantive results reinforce the conclusion that it is late negativity that targets the individual's preferred candidate that leads to significant changes in the likelihood of turnout. Increases in negativity about the preferred candidate decrease turnout likelihood by as much as 6 percentage points; even more importantly, the decrease in turnout is statistically significant. In contrast, the substantive effects of negativity about the other candidate, as well as overall negativity, are statistically indistinguishable from 0.

In the model below (from her Model 3 in Table 4 on p. 807), she is specifically interested in comparing the effects of `negaboutdislike` and `negaboutlike`. She shows that the estimated coefficient for `negaboutdislike` is not statistically significant, while the estimated coefficient for `negaboutlike` *is* statistically significant.

First, let's reproduce her fitted model results.

```{r}
# load data
krup_raw <- haven::read_dta("data/krup.dta") 

# model formula (model 3, tab. 4, p. 807, krupnikov 2011)
f <- turnout ~ 
           # negativity
           negaboutdislike + negaboutlike +
           # resources
           income + education + age + unemployed +
           # evaluation of parties and candidates
           PIDStrength + AffectPID + care + AffectPRES +
           # social involvement
           lnYears + Church + homeowners + working + 
           # mobilization
           contacted + 
           # interest, exposure, and efficacy
           external + internal + interest + media_index + 
           # other demographics
           married + black + southern + hispanic + gender + 
           # state conditions
           closeness + governors + primaries + 
           # volume and year controls
           volume2 + dummy1988 + dummy2000 + dummy1992 + dummy1996

# drop rows with missing values from the data set
krup <- krup_raw %>%
  get_all_vars(f, data = .)  %>%
  na.omit()

fit <- glm(f, data = krup, family = "binomial")
```

Now let's compute the Brier scores for each observation and the aggregate to the data set by averaging.

```{r}
# obtain prediction
p <- predict(fit, type = "response")

# compute brier scores
y <- krup$turnout                           # create a vector to make code more readable
brier_scores <- (y - p)^2                      # compute manually
brier_scores_alt <- scoring::brierscore(y ~ p) # compute with the scoring package

# aggregate by averaging
bs <- mean(brier_scores)
print(bs, digits = 2)
```

Now let's do the same for the log scores.

```{r}
# compute log scores
log_scores <- -(y*log(p) + (1 - y)*log(1 - p))
log_scores_alt <- scoring::logscore(y ~ p)

# aggregate by averaging
ls <- mean(log_scores)
print(ls, digits = 2)
```

It's usually difficult to interpret or act upon the Brier and log scores for a single model. Instead, we typically use them to choose among a set of models. 

As an simple example, I removed the two late-negativity variables from the model (Krupnikov's key explanatory variables)

Important: When using the Brier and log scores, **lower scores indicate a better fit.**

```{r echo=FALSE, fig.height=3, fig.width=6, cache = TRUE}
f_list <- list()
f_name <- c()

# full model
f_list[[1]] <- f
f_name[1] <- "Full Model"

# model 2
f_list[[2]] <- update(f, . ~ . -negaboutdislike)
f_name[2] <- "Remove Negativity About Disliked Candidates"

# model 3
f_list[[3]] <- update(f, . ~ . -negaboutlike)
f_name[3] <- "Remove Negativity About Liked Candidates"

# model 4
f_list[[4]] <- update(f, . ~ . -negaboutdislike - negaboutlike)
f_name[4] <- "Remove Both Negativity Variables"

# fit each model and store results
results_list <- list()
for (i in 1:length(f_list)) {
  # fit model
  fit_i <- glm(f_list[[i]], data = krup, family = "binomial")
  # obtain prediction
  p_i <- predict(fit_i, type = "response")
  # compute log scores
  log_scores_i <- -(y*log(p_i) + (1 - y)*log(1 - p_i))
  ls_i <- mean(log_scores_i)
  # compute brier scores
  brier_scores_i <- (y - p_i)^2 
  bs_i <- mean(brier_scores_i)
  # store results
  results_list[[i]] <- tibble(model_name = f_name[i],
                              log_score = ls_i, 
                              brier_score = bs_i)
}
results <- bind_rows(results_list) 

# make table
results %>%
  rename(`Model Name` = model_name,
         `Avg. Log Score` = log_score,
         `Avg. Brier Score` = brier_score) %>%
  kableExtra::kable(format = "markdown", digits = 4)

# make plot
results %>%
  rename(`Log Score` = log_score,
         `Brier Score` = brier_score) %>%
  pivot_longer(cols = c(`Log Score`, `Brier Score`), 
               values_to = "avg_score", 
               names_to = "rule") %>%
  mutate(model_name = reorder(model_name, -avg_score)) %>%
  ggplot(aes(x = avg_score, y = model_name)) + 
  geom_point() + 
  facet_wrap(vars(rule), ncol = 1, scales = "free") + 
  labs(x = "Avg. Score",
       y = NULL)
```

### Cross-Validation

But here's the dirty little secret: you can **always** make your model better *within your sample* by making the model more complex. As a simple illustration, I added a quadruple interactions between both forms of negativity and education, income, and gender.

```
turnout ~ negaboutdislike*education*income*gender + negaboutlike*education*income*gender + ...
```

This model will better predictions than the baseline model.

```{r echo=FALSE, cache = TRUE}
f_list <- list()
f_name <- c()

# full model
f_list[[1]] <- f
f_name[1] <- "Full Model"

# model 2
f_list[[2]] <- update(f, . ~ . -negaboutdislike - negaboutlike + 
                        negaboutdislike*education*income*gender + negaboutlike*education*income*gender)
f_name[2] <- "Adding Wild Interactions"

# fit each model and store results
results_list <- list()
for (i in 1:length(f_list)) {
  # fit model
  fit_i <- glm(f_list[[i]], data =  krup, family = "binomial")
  # obtain prediction
  p_i <- predict(fit_i, type = "response")
  # compute log scores
  log_scores_i <- -(y*log(p_i) + (1 - y)*log(1 - p_i))
  ls_i <- mean(log_scores_i)
  # compute brier scores
  brier_scores_i <- (y - p_i)^2 
  bs_i <- mean(brier_scores_i)
  # store results
  results_list[[i]] <- tibble(model_name = f_name[i],
                              log_score = ls_i, 
                              brier_score = bs_i)
}
results <- bind_rows(results_list) 

# make table
results %>%
  rename(`Model Name` = model_name,
         `Avg. Log Score` = log_score,
         `Avg. Brier Score` = brier_score) %>%
  kableExtra::kable(format = "markdown", digits = 8)
```


Consider the two models fit trough the data below. A simple line (in green) fits the data quite nicely. However, a 10th-order polynomial (in orange) fits the observed data *even better* (in fact, it has no error at all!). 

```{r fig.height=3, fig.width=4}
set.seed(1234)
n <- 11
x <- seq(-1, 1, length.out = n)
y <- rnorm(n) + x
data <- tibble(y, x)

fit <- lm(y ~ x, data = data)
fit10 <- lm(y ~ poly(x, 10), data = data)

ggplot(data, aes(x, y)) + 
  geom_smooth(method = lm, se = FALSE, color = "#1b9e77") + 
  geom_smooth(method = lm, se = FALSE, formula = y ~ poly(x, 10), n = 1001, color = "#d95f02") + 
  geom_point()

```

But if we used the 10th-order polynomial for prediction, it would perform horribly. Why? Because it *overfits* the data. That is, it explains both the *systematic* and *idiosyncratic* features of the observed data. Suppose we need to make a prediction for $x = -0.95$. The complex model generates a prediction of about `r round(predict(fit10, newdata = data.frame(x = -0.95)))`. 

To avoid over-fitting the model, we can use two approaches.

1. cross validation
1. information criteria

Let's start with leave-one-out cross validation.

For each observation $i$ in the data set:

1. Drop that observation $i$. 
1. Fit the model using the remaining data.
1. Predict the dropped observation.
1. Compute the score for that observation.

Because the observation being predicted is left-out and *not in the data set used to fit the model*, the model cannot "cheat" and fit the idiosyncratic variation in the left-out data point. In order to perform well, it must identify systematic variation in the *other* data points and use that information to predict the left-out observation.

If your data set has $n$ observations, then you must fit $n$ models to perform leave-one-out cross validation. Let's estimate the time-cost for Krupnikov's model.

```{r}
# fit model and store time
time <- system.time( 
  fit <- glm(f, data = krup, family = "binomial")
  )

# multiply elapsed time times number of observations
round(time["elapsed"]*nrow(krup)/(60), 1)  # convert to minutes
```

Each model takes about 0.05 seconds to fit. This seems fast, but you need to do it about 6,000 times, which takes about 300 seconds or five minutes.

```{r cache=FALSE}
# note system time
start_time <- Sys.time()

# perform cross validation
results_list <- list()
#for (i in 1:nrow(krup)) {
for (i in 1:10) {
  if (i %% 100 == 0) print(i)
  # create training data and test data, to make code readable
  training <- slice(krup, -i)
  test     <- slice(krup,  i)
  # fit model
  fit_i <- glm(f, data = training, family = "binomial")
  # compute scores for test data (compute scores later)
  y_i <- test$turnout
  p_i <- predict(fit_i, newdata = test, type = "response")
  # store results
  results_list[[i]] <- tibble(case_id = i,
                              y = y_i,
                              p = p_i)
}

# note system time
end_time <- Sys.time()
diff_time <- difftime(end_time, start_time, units = "mins")

# combine results and compute scores
results <- bind_rows(results_list) %>%
  mutate(log_score = -(y*log(p) + (1 - y)*log(1 - p)),
         brier_score = (y - p)^2)

# average scores
print(mean(results$log_score), 3)
print(mean(results$brier_score), 3)
```


This code took `r round(as.numeric(diff_time), 1)`) minutes to run. This isn't always practical, especially for large data sets. (It is embarrassingly parallel, though, so it's possible to dramatically shrink this time using parallel computing.)

For large data sets, rather than drop each observation individually, we can divide the data into $k$ equally-sized (or as close to equal as possible) groups. The we repeat the same process but drop and predict each *group* rather than the individual data points. This is called **$k$-fold cross validation**. If $k = n$, then we just have leave-one-out cross-validation.

The code below uses $k = 10$ and finds the average log and Brier scores for out-of-sample prediction using $k$-fold cross-validation for the scobit data.

```{r}
# cross validation groups
k <- 10  
group <- sample(rep(1:k, length.out = nrow(krup)))

# perform cross validation
results_list <- list()
for (i in 1:k) {
  # create training data and test data, to make code readable
  training <- filter(krup, group != i)
  test     <- filter(krup, group == i)
  # fit model
  fit_i <- glm(f, data = training, family = "binomial")
  # compute scores for test data (compute scores later)
  y_i <- test$turnout
  p_i <- predict(fit_i, newdata = test, type = "response")
  # store results
  results_list[[i]] <- tibble(group = i,
                              y = y_i,
                              p = p_i)
}

# combine results and compute scores
results <- bind_rows(results_list) %>%
  mutate(log_score = -(y*log(p) + (1 - y)*log(1 - p)),
         brier_score = (y - p)^2)

# average scores
print(mean(results$log_score), 3)
print(mean(results$brier_score), 3)
```

This result took just a second or two.

To illustrate how we can use $k$-fold cross-validation to evaluate models, I use $k$-fold cross validation to compute the average scores for all seven models (simpler and more complex) models discussed above.

The results are *really* close and can depend on the random assignment to the $k$ groups, so we want a large $k$ (or use leave-one-out cross validation). For the results below, I use 100-fold cross-validation.

```{r echo=FALSE, fig.height=3, fig.width=6, cache = TRUE}
# set seed
set.seed(1234)

# cross validation groups
k <- 10
group <- sample(rep(1:k, length.out = nrow(krup)))

# define models
f_list <- list()
f_name <- c()
f_list[[1]] <- f
f_name[1] <- "Full Model"
f_list[[2]] <- update(f, . ~ . -negaboutdislike)
f_name[2] <- "Remove Negativity About Disliked Candidates"
f_list[[3]] <- update(f, . ~ . -negaboutlike)
f_name[3] <- "Remove Negativity About Liked Candidates"
f_list[[4]] <- update(f, . ~ . -negaboutdislike - negaboutlike)
f_name[4] <- "Remove Both Negativity Variables"
f_list[[5]] <- update(f, . ~ . -negaboutdislike - negaboutlike + 
                        negaboutdislike*education*income*gender + negaboutlike*education*income*gender)
f_name[5] <- "Adding Wild Interactions"


# fit each model and store results
model_results_list <- list()
for (i in 1:length(f_list)) {
  # perform cross validation
  cv_results_list <- list()
  for (j in 1:k) {
    # create training data and test data, to make code readable
    training <- filter(krup, group != j)
    test     <- filter(krup, group == j)
    # fit model
    fit_j <- glm(f_list[[i]], data = training, family = "binomial")
    # compute scores for test data (compute scores later)
    y_j <- test$turnout
    p_j <- predict(fit_j, newdata = test, type = "response")
    # store results
    cv_results_list[[i]] <- tibble(group = j,
                                   model_name = f_name[i],
                                   y = y_j,
                                   p = p_j)
  }
  # store results
  model_results_list[[i]] <- bind_rows(cv_results_list)
}
results <- bind_rows(model_results_list) %>%
  mutate(log_score = -(y*log(p) + (1 - y)*log(1 - p)),
         brier_score = (y - p)^2)

# make table
results %>%
  group_by(model_name) %>%
  summarize(`Avg. Log Score` = mean(log_score),
            `Avg. Brier Score` = mean(brier_score)) %>%
  rename(`Model Name` = model_name) %>%
  kableExtra::kable(format = "markdown", digits = 4)

# make plot
results %>%
  group_by(model_name) %>%
  summarize(`Log Score` = mean(log_score),
            `Brier Score` = mean(brier_score)) %>%
  pivot_longer(cols = c(`Log Score`, `Brier Score`), 
               values_to = "avg_score", 
               names_to = "rule") %>% 
  mutate(model_name = reorder(model_name, -avg_score)) %>%
  ggplot(aes(x = avg_score, y = model_name)) + 
  geom_point() + 
  facet_wrap(vars(rule), ncol = 1, scales = "free") + 
  labs(x = "Avg. Score",
       y = NULL)
```

### Information Criteria

As an alternative to cross-validation, we can use information criteria for a similar purpose without needing to refit the model many times. 

Information criteria have the following general structure:

$$
-2 \log L(\hat{\theta}) + [\text{constant}\times k ]
$$

Here, $\log L(\hat{\theta})$ is the value achieved when we maximized the log-likelihood function (not the $\hat{\theta}$, but the value of $\log L$ itself), $k$ is the number of parameters, and $\text{constant}$ is a constant term that varies across information criteria.

The two most common information criteria are:

1. **Akaike Information Criterion (AIC)** $= -2 \log L(\hat{\theta}) + [2 \times k]$
1. **Bayesian Information Criterion (AIC)** $= -2 \log L(\hat{\theta}) + [\log(n) \times k]$

The AIC and BIC have a deep and detailed theoretical development--the choice of constant is not at all arbitrary. It doesn't seem helpful to reproduce the theory here, but instead mention a few practical points.

- The *magnitude* of the IC is generally not of interest. Instead, we focus on the *difference* in the IC between models.
- Both the the AIC and the BIC work to identify the "best" model, but in two difference senses:
   - The AIC roughly compares the observed and predictive distributions are tries to identify the best match.
   - The BIC roughly identifies the model with the highest posterior probability---the most likely model to have generated the data.
- Both AIC and BIC penalize adding parameters. That is, in order to improve the IC, a more complex model must improve the fit enough to offset the additional penalty. That said, the BIC imposes a larger penalty for $n \geq 8$.

ADD SOME DISCUSSION ABOUT THE MAGNITUDE OF THE DIFFERENCE OF AIC AND BIC.

ADD RAFTERY's TABLE HERE.

To compute the AIC and BIC, we have the easy-to-use `AIC()` and `BIC()` functions. 

We can use those to compare models with and without the `negaboutlike` variable, for example. The AIC *slightly* prefers including the variable, but the BIC prefers the model without Krupnikov's key explanatory variable.


```{r}
# aic and bic for full model
AIC(fit)
BIC(fit)

# compare models
fit0 <- update(fit, . ~ . -negaboutlike)
fit1 <- update(fit, . ~ . -negaboutdislike)
fit2 <- update(fit, . ~ . -negaboutdislike - negaboutlike)


AIC(fit, fit0)
BIC(fit, fit0)

# compute model weights
# note: in krupnikov's theory, fit1 should be best (fit includes unnecessary variable)
AIC(fit, fit0, fit1, fit2) %>%
  mutate(diff_min = AIC - min(AIC),
         akiaike_weights = exp(-0.5*diff_min)/sum(exp(-0.5*diff_min)))
BIC(fit, fit0, fit1, fit2) %>%
  mutate(diff_min = BIC - min(BIC),
         post_prob = exp(-0.5*diff_min)/sum(exp(-0.5*diff_min)))
```

```{r echo=FALSE, fig.height=3, fig.width=6, cache = TRUE}
# fit each model and store results
fit_list <- list()
for (i in 1:length(f_list)) {
  # perform cross validation
    fit_list[[i]] <- glm(f_list[[i]], data = krup, family = "binomial")
}

bic <- BIC(fit_list[[1]], fit_list[[2]], fit_list[[3]], fit_list[[4]], fit_list[[5]]) %>% 
  mutate(model_name = f_name) %>%
  mutate(model_name = reorder(model_name, BIC))

ggplot(bic, aes(x = BIC, y = model_name)) + 
  geom_point()
```

```{r echo=FALSE, fig.height=3, fig.width=6, cache = TRUE}
aic <- AIC(fit_list[[1]], fit_list[[2]], fit_list[[3]], fit_list[[4]], fit_list[[5]]) %>%
  mutate(model_name = f_name) %>%
  mutate(model_name = reorder(model_name, AIC))

ggplot(aic, aes(x = AIC, y = model_name)) + 
  geom_point()
```